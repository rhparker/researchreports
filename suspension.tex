\documentclass[12pt]{article}
\usepackage[pdfborder={0 0 0.5 [3 2]}, plainpages=false]{hyperref}%
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}%
\usepackage[shortalphabetic]{amsrefs}%
\usepackage{amsmath}
\usepackage{enumerate}
% \usepackage{enumitem}
\usepackage{amssymb}                
\usepackage{amsmath}                
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage[table,xcdraw]{xcolor}
\usepackage{tikz}
\usepackage{float}
\usepackage{booktabs}
\usepackage{svg}
\usepackage{mathtools}
\usepackage{cool}
\usepackage{url}
\usepackage{graphicx,epsfig}
\usepackage{makecell}
\usepackage{array}

\def\noi{\noindent}
\def\T{{\mathbb T}}
\def\R{{\mathbb R}}
\def\N{{\mathbb N}}
\def\C{{\mathbb C}}
\def\Z{{\mathbb Z}}
\def\P{{\mathbb P}}
\def\E{{\mathbb E}}
\def\Q{\mathbb{Q}}
\def\ind{{\mathbb I}}

\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\ran}{range}

\graphicspath{ {suspension/} }

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{hypothesis}{Hypothesis}

\newtheorem{notation}{Notation}

\begin{document}

\section{Suspension Bridge Equation}

\subsection{Background}

We look at the suspension bridge equation from Chen97. The idea is to look at an equation which has a second order time derivative which also possesses multimodal solutions. This is suggested on p. 2095 of SanStrut and p. 351 of Chen97.\\

The equation we will look at here is

\begin{equation}\label{susp}
u_{tt} + u_{xxxx} + e^{u - 1} - 1 = 0
\end{equation}

Note that the equation used for most of the analysis in Chen97 is 

\begin{equation}\label{susp2}
u_{tt} + u_{xxxx} + u^+ - 1 = 0
\end{equation}

which has a cusp at $u = 0$. On p. 351 of Chen97, Chen and McKenna note that they have proved existence of traveling wave solutions only to \eqref{susp2}, not to \eqref{susp}, but there is strong numerical evidence that similar solutions exist for \eqref{susp}. This issue has since been resolved for traveling wave solutions and will be discussed below. \\

First, to make our lives easier, we make the change of variables $u - 1 \mapsto u$ so that pulse solutions will decay to a baseline of 0 instead of 1. This gives us

\begin{equation}\label{susp3}
u_{tt} + u_{xxxx} + e^{u} - 1 = 0
\end{equation}

Since we are interested in traveling wave solutions, we make the usual traveling wave ansatz. Letting $\xi = x - ct$, substituting this into \eqref{susp3}, and changing $\xi$ back to $x$, we have

\begin{equation}\label{susp3}
u_{tt} - 2 c u_{x t} + u_{xxxx} + c^2 u_{xx} + e^{u} - 1 = 0
\end{equation}

For an equilibrium solution (such as a homoclinic orbit), all time derivatives are zero, so any equilibrium solution must satisfy the ODE

\begin{equation}\label{eqODE}
u_{xxxx} + c^2 u_{xx} + e^{u} - 1 = 0
\end{equation}

which is (46) on p. 342 of Chen97, with $\tilde{f}(u) = e^u - 1$. Note that $u = 0$ is a solution to this. A homoclinic orbit, if it exists, connects this constant equilibrium state to itself. The existence of a homoclinic orbit equilibrium solution was shown in Vdb18 using a computer-assisted proof method. They proved (Theorem 1) that a symmetric homoclinic orbit exists for \eqref{eqODE} for all parameter values $c$ with $c^2 \in [0.5, 1.9]$.\\

Linearizing about this trivial solution, we obtain the ODE

\begin{equation}
v_{xxxx} + c^2 v_{xx} + 1 = 0
\end{equation}

which has four eigenvalues

\begin{equation}
\nu = \pm \sqrt{\frac{-c^2 \pm \sqrt{c^4 - 4}}{2} }
\end{equation}

Note that $\sqrt{c^4 - 4}$ is always less that $c^2$ is magnitude. If $|c| < \sqrt{2}$, we have a complex conjugate quartet $\nu = \pm \alpha \pm \beta i$. When $c^2 = 2$, these eigenvalues collide on the imaginary axis at $\pm i$, then for $c^2 > 2$ we have a quartet of purely imaginary eigenvalues. Thus a bifurcation occurs at $c^2 = 2$. We expect that multipulse solutions will only be possible for $c^2 < 2$.

\subsection{Eigenvalue Problem}

For linear stability analysis, we look at the PDE eigenvalue problem. To do this, assume we have found an equilibrium solution $u_*(x)$ of \eqref{eqODE}. We linearize around $u^*(x)$ by taking the standard linearization ansatz

\begin{equation}
u(x,t) = u_*(x) + \epsilon e^{\lambda t} v(x)
\end{equation}

Plugging this into \eqref{susp3} and keeping only terms of order $\epsilon$, we obtain the quadratic eigenvalue problem

\begin{equation}\label{evp}
[\lambda^2 - 2 c \partial_x \lambda + (\partial_x^4 + c^2 \partial_x^2 + e^{u_*})]v = 0
\end{equation}

This is in the general form of a quadratic eigenvalue problem 

\[
(A_2 \lambda^2 + A_1 \lambda + A_0)v = 0
\]

where

\begin{align}
A_0 &= \partial_x^4 + c^2 \partial_x^2 + e^{u_*} \\
A_1 &= -2 c \partial_x \\
A_2 &= I
\end{align}

We can also write this as

\begin{equation}\label{evp2}
[(\lambda - c \partial_x)^2 + \partial_x^4 + e^{u_*}]v = 0
\end{equation}

Next, we look at symmetry. For a given equilibrium solution $u_*$, suppose we have a solution $v(x)$ to \eqref{evp} with corresponding eigenvalue $\lambda$. By taking complex conjugates, it follows that $\overline{v}(x)$ is a solution with corresponding eigenvalue $\overline{\lambda}$. If we assume that $u^*(x)$ is an even function, we can replace $v(x)$ by $v(-x)$ and $\lambda$ by $-\lambda$ to obtain another solution to \eqref{evp}. Thus for the linearization about an even function, we have the typical 4-fold symmetry of eigenvalues.\\ 

To continue, we write the eigenvalue problem as a first order system. Letting $V = (v_1, v_2, v_3, v_4) = (v, v_x, v_{xx}, v_{xxx})$, \eqref{evp} becomes $V' = A(\lambda; u^*)V$, where

\begin{equation}\label{evpsystem}
A(\lambda; u_*) = \begin{pmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
-(\lambda^2 + e^{u_*}) & 2 c \lambda & -c^2 & 0 
\end{pmatrix}
\end{equation}

\subsubsection{Essential Spectrum}

Suppose there exists an exponentially localized solution $u_*(x)$ of \eqref{eqODE}. By the Weyl Essential Spectrum Theorem, the essential spectrum of $A(\lambda; u_*)$ is the same as that of the asymptotic matrix $A_\infty = A(\lambda; 0)$.

\begin{equation}\label{Ainf}
A_\infty(\lambda) = \begin{pmatrix}
0 & 1 & 0 & 0 \\ 
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
-(\lambda^2 + 1) & 2 c \lambda & -c^2 & 0 
\end{pmatrix}
\end{equation}

The characteristic polynomial of \eqref{Ainf} is

\begin{equation}\label{charpoly}
p(\nu) = \nu^4 + c^2 \nu^2 - 2 c \lambda \nu + (\lambda^2 + 1) 
\end{equation} 

The essential spectrum is the set of all $\lambda \in \C$ for which $A_\infty$ has a purely imaginary eigenvalue. To find this, we substitute $\nu = i r$ for $r \in \R$ into \eqref{charpoly}. Collecting terms in $\lambda$, we obtain the equation

\begin{equation}\label{fredholmborder}
\lambda^2 - 2 i c r \lambda - c^2 r^2  + r^4 + 1 = 0 
\end{equation} 

Solving for $\lambda$, this becomes

\begin{align*}
\lambda_\pm(r) &= i c r \pm \sqrt{-1 - r^4} \\
&= \left( c r \pm \sqrt{1 + r^4} \right) i
\end{align*}

since $-1 - r^4 \leq -1 < 0$. Thus the essential spectrum is purely imaginary. Since $\lambda_\pm(r) \rightarrow \pm \infty$ as $r \rightarrow \pm \infty$, the essential spectrum extends out to infinity along the imaginary axis. The only remaining question is whether the essential spectrum contains a gap around the origin, i.e. is bounded away from 0. Matlab suggests there is such a gap at the origin, so we will look at that here. This is a plot of $\lambda_\pm(r)$ for $c = 1$ which shows that gap.

\begin{figure}[H]
\centering
\includegraphics[width=10cm]{essspec1.eps}
\label{fig:essspec1}
\caption{Imaginary part of the purely imaginary essential spectrum for $c = 1$.}
\end{figure}

The equation for $\lambda(r)$ is of the form $\lambda_\pm(r) = k r \pm f(r^2)$. Since

\begin{align*}
\lambda_-(-r) = -kr - f((-r)^2) = -(kr + f(r^2)) = -\lambda_+(r)
\end{align*} 

and we take $r \in \R$, it suffices to look at $\lambda_+(r)$. Using Mathematica, we find

\begin{align*}
\lambda_+'(r) = c+\frac{2 r^3}{\sqrt{r^4+1}}
\end{align*}

Solving $\lambda_+'(r) = 0$ involves solving the 6th order polynomial

\[
4 r^6 - c^2 r^4 + c^2 = 0
\]

Mathematica will actually do it (!), but it is an epic mess. So we will look at it numerically using Matlab. The next plot shows the minimum value of $\lambda_+(r)$ as a function of $c$. Note that for $c = 0$ this minimum is clearly $0$, and for $c = \sqrt{2}$ it is not hard to show that this minimum is actually 0 (hence no essential spectrum gap). $c = \sqrt{2}$ is the bifurcation point mentioned above, so this is consistent.

\begin{figure}[H]
\centering
\includegraphics[width=10cm]{minlambdaplus.eps}
\label{fig:essspec1}
\caption{Imaginary part of the purely imaginary essential spectrum for $c = 1$.}
\end{figure}

Note that this minimum decreases strictly as $c$ increases, so the essential spectrum gap narrows (to 0) as $c$ increases from 0 to $\sqrt{2}$.\\

We could probably do better, i.e. approximate or get an exact formula (likely a nasty one) for this curve, but this is good enough for now.

\subsubsection{Point Spectrum}

Now that have looked at the essential spectrum, we can move on the point spectrum. At this point, there are only a few things we can say.\\

If $\lambda = 0$, the eigenvalue problem reduces to

\begin{equation*}
A_0 v = (\partial_x^4 + c^2 \partial_x^2 + e^{u_*})v = 0
\end{equation*}

Thus we are are interested in the kernel of $A_0$. If we take $v = \partial_x u_*$, then

\begin{align*}
A_0 \partial_x u_* &= 
(\partial_x^4 + c^2 \partial_x^2 + e^{u_*})\partial_x u_* \\
&= \partial_x[(u_*)_{xxxx} + c^2 (u_*)_{xx} + e^{u_*}] \\
&= \partial_x(1) \\
&= 0
\end{align*}

where we use the fact that $u_*$ is an equilibrium solution, thus must satisfy \eqref{eqODE}. Thus 0 is an eigenvalue of \eqref{evp} with corresponding eigenfunction $\partial_x u_*$.\\

Since the operator $A_0$ is self-adjoint, by the Fredholm alternative we cannot have a function $v$ such that $A_0 v = \partial_x u_*$. (Otherwise $\partial_x u_*$ would be perpendicular to the kernel of $A_0^* = A_0$, which is impossible.) In other words, we cannot have a Jordan chain ending in $\partial_x u^*$.\\

Thus it makes sense to take the following hypothesis, which states that there is nothing else in the kernel of $A_0$.

\begin{hypothesis}\label{kerA0hyp}
The kernel of $A_0$ is given by
\begin{equation}\label{kerA0}
\ker A_0 = \spn\{ \partial_x u_*\}
\end{equation}
\end{hypothesis}

Finally, we look at the derivative with respect to the speed $c$, since it will be important in what is to follow. Taking the derivative of \eqref{eqODE} with respect to $c$, we get

\begin{align*}
0 &= u_{xxxxc} + 2 c u_{xx} + c^2 u_{xxc} + e^{u} u_c \\
&= 2 c u_{xx} + (\partial_x^4 + c^2 \partial_x^2 + e^{u})u_c
\end{align*}

At an equilibrium solution $u_*$, we rearrange this to get

\begin{align*}
(\partial_x^4 + c^2 \partial_x^2 + e^{u^*})u^*_c &= -2 c u^*_{xx}
\end{align*}

This simplifies to 

\begin{equation}\label{uc}
A_0 \partial_c u^* = -2 c u^*_{xx}
\end{equation}

For $\lambda \neq 0$, there is nothing else we can say now regarding the point spectrum. To find these eigenvalues, we will have to use another technique, such as the Krein matrix or Lin's method.

\subsubsection{Numerics}

In this section, we compute the spectrum numerically with Matlab and the \texttt{quadeig} package. For $c = 1.2$, here are plots of the spectrum of the linearization of \eqref{evp} about the single pulse solution. Matlab shows an essential spectrum gap, as discussed above, and predicts that this gap is $[-0.2061, 0.2061]$. The eigenvalue at 0 is from translation invariance, as discussed above.

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{essspecboundsF256.eps}
\includegraphics[width=8cm]{essspecboundsFD256.eps}
\caption{Essential spectrum gap for $c = 1.2$. Fourier spectral methods, $N = 256$ (left), finite difference methods, $N = 256$ (right)}
\end{figure}

\subsubsection{\texorpdfstring{Spectrum of $A_0$}{}}

At this point, we pause to look at the spectrum of the operator $A_0$. Since $A_0$ is self-adjoint, its spectrum will be real. First, we look at the essential spectrum. Writing the eigenvalue problem as a first order system, the operator $A_0$ is given by

\begin{equation}\label{A0system}
A_0(\lambda; u_*) = \begin{pmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
\lambda - e^{u_*} & 0 & -c^2 & 0 
\end{pmatrix}
\end{equation}

where $u^*$ is an equilibrium solution which is exponentially localized. By the Weyl Essential Spectrum Theorem, the essential spectrum only depends on the asymptotic matrix $A_0(\lambda; 0)$.

\begin{equation}
A_0(\lambda; 0) = \begin{pmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
\lambda - 1 & 0 & -c^2 & 0 
\end{pmatrix}
\end{equation}

The characteristic polynomial of the asymptotic matrix is 

\[
p(\nu) = \nu^4 + c^2 \nu^2 - \lambda + 1
\]

For the essential spectrum, we take $\nu = i r$, where $r \in \R$. This becomes

\[
p(i r) = r^4 - c^2 r^2 - \lambda + 1 = 0
\]

Solving for $\lambda$ gives us

\[
\lambda(r) = r^4 - c^2 r^2 + 1 = r^2(r^2 - c^2) + 1
\]

This is a fourth-order polynomial with limits of $\infty$ at both ends, so all we need to do is find its minimum. Since $\lambda'(r) = 4 r^3 - 2 c^2 r = 2 r(2 r^2 - c^2)$, there will be a minimum when $r = c/\sqrt{2}$. At this minimum, $\lambda = 1 - c^4/4$. Thus, the essential spectrum of $A_0$ is given by

\begin{equation}\label{A0ess}
\sigma_{\text{ess}}(A_0) = [1 - c^4/4, \infty)
\end{equation}

For $c \in (0, \sqrt{2})$, which are the values of $c$ under consideration, the essential spectrum of $A_0$ is positive and bounded away from the origin.\\

We discussed the kernel of $A_0$ above. By Hypothesis \ref{kerA0hyp}, the kernel of $A_0$ is one-dimensional and is spanned by $\partial_x u_*$. Thus we have a single eigenvalue of $A_0$ at 0. \\

Any remaining eigenvalues will depend on the solution we are linearizing about. In what follows, will consider the linearization about a single-pulse homoclinic orbit $q(x)$. First, we compute the spectrum of $A_0$ numerically.

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{specA0.eps}
\caption{Spectrum of $A_0$. $c = 1.3$. Finite difference methods, $N = 513$.}
\end{figure}

We see that there is a single eigenvalue at 0, and that the essential spectrum is bounded away from 0 and is (approximately) given by \eqref{A0ess}. We also note the presence of a single negative eigenvalue.\\

In this case (as opposed to SanStrut), it will be difficult (if not impossible) to prove the existence of a unique negative eigenvalue. The plot above suggests this is the case. In addition, we have numerically $\langle A_0 q, q \rangle < 0$. Thus we make the following hypothesis.

\begin{hypothesis}\label{A0neg}
For $c \in (0, \sqrt{2})$, the operator $A_0$ has exactly one negative eigenvalue $\lambda_- < 0$ with corresponding eigenfunction $v_-(x)$.
\end{hypothesis}

\subsection{Stability of Primary Pulse}

Choose a speed $c$ with $c^2 \in [0.5, 1.9]$, so that we can apply the results of Vdb18 and conclude that a homoclinic orbit exists. To assess stability of this primary pulse, we follow Grill87.\\

Letting $v = u_t$, we can write \eqref{susp3} in the form 

\begin{equation}\label{2dsystem}
\frac{d \textbf{u} }{dt} = J E'(\textbf{u})
\end{equation}

where $\textbf{u} = \begin{pmatrix}u&v\end{pmatrix}^T$, $J$ is the standard symplectic matrix

\begin{equation*}
J = \begin{pmatrix}0 & 1 \\ -1 & 0 \end{pmatrix}
\end{equation*}

and $E(\textbf{u})$ is the energy

\begin{equation}\label{energy}
E(\textbf{u}) = \int_{-\infty}^\infty \left(\frac{1}{2} v^2 + \frac{1}{2}u_{xx}^2 + e^{u} - u \right)dx
\end{equation}

The energy $E$ is conserved (in $t$), since

\begin{align*}
\frac{d}{dt} E(\textbf{u}) &= \langle E'(\textbf{u}), \textbf{u}_t \rangle \\
&= \langle E'(\textbf{u}), J E'(\textbf{u}) \rangle \\
&= 0
\end{align*}

where the last equality holds because $J$ is skew-symmetric.\\

The equation \eqref{2dsystem} is invariant under the unitary translation group $T(s)$, defined by $T(s)\phi(\cdot) = \phi(\cdot - s)$. It is easy to see that $T^*(s) = [T(s)]^{-1} = T(-s)$. The infinitesimal generator of $T(s)$ is

\begin{equation}
T'(0) = \begin{pmatrix}\partial_x & 0 \\ 0 & \partial_x \end{pmatrix}
\end{equation}

The energy $E(\textbf{u})$ is also invariant under $T(s)$, i.e. $E(T(s) \textbf{u}) = E(\textbf{u})$ for all $s \in \R$. This is clear since \eqref{energy} does not involve $x$ directly, and we are integrating over $\R$. If we differentiate $E(T(s) \textbf{u}) = E(\textbf{u})$ with respect to $s$ at $s = 0$, we obtain the relation

\begin{equation}\label{IP1}
\langle E'(\textbf{u}), T'(0) \textbf{u} \rangle = 0
\end{equation}

As in Grill87, we define 

\begin{equation}\label{defB}
B = J^{-1} T'(0) = \begin{pmatrix}0 & -\partial_x \\ \partial_x & 0\end{pmatrix}
\end{equation}

Since $B$ is the product of two skew-symmetric operators, $B$ is self-adjoint. Using this, we define the momentum as

\begin{equation}\label{defQ}
Q(\textbf{u}) = \frac{1}{2}\langle B \textbf{u}, \textbf{u} \rangle
\end{equation}

It is not hard to show that $Q'(\textbf{u}) = B \textbf{u}$. Expanding \eqref{defQ} and integrating by parts, we get

\begin{align*}
Q(\textbf{u}) &= \frac{1}{2}\langle (-v_x, u_x), (u, v) \rangle \\
&= \frac{1}{2} \left( -\int_{-\infty}^\infty v_x u dx + \int_{-\infty}^\infty u_x v dx \right) \\
&= \int_{-\infty}^\infty u_x v dx
\end{align*}

The momentum $Q(\textbf{u})$ is also conserved in $t$, since

\begin{align*}
\frac{d}{dt} E(\textbf{u}) &= \langle Q'(\textbf{u}), \textbf{u}_t \rangle \\ 
&= \langle B \textbf{u}, J E'(\textbf{u}) \rangle \\
&= -\langle J B \textbf{u}, E'(\textbf{u}) \rangle \\
&= -\langle T'(0) \textbf{u}, E'(\textbf{u}) \rangle \\
&= 0
\end{align*}

where we used \eqref{defB} and \eqref{IP1}.\\

To show that the primary pulse is stable, we will use Theorem 3 in Grill87. To do this, we need to verify the assumptions of the theorem. Assumption 1 concerns existence of solutions to \eqref{2dsystem}. We will take this as a hypothesis (unless we can find a reference to this somewhere. Chen and McKenna do not look at this).

\begin{hypothesis}
For every initial condition $\textbf{u}_0$ there exists a solution $\textbf{u}(t)$ to \eqref{2dsystem} on the interval $I = [0, T]$, where $T$ only depends on $||\textbf{u}_0||$, such that $\textbf{u}(0) = \textbf{u}(0)$. 
\end{hypothesis}

Assumption 2 in Grill87 involves bound states, which are special solutions of the form

\begin{equation}\label{boundstate}
\textbf{u}(x, t) = T(ct)\boldsymbol\phi(x) = \boldsymbol\phi(x - ct)
\end{equation}

(We use the variable $c$ to represent speed instead of $\omega$). By p. 166 of Grill87, if $\boldsymbol\phi(x) $ satisfies the stationary equation

\begin{equation}\label{boundstateeq}
E'(\boldsymbol\phi) - c Q'(\boldsymbol\phi) = 0
\end{equation}

then $T(ct)\boldsymbol\phi$ is a bound state. Writing $\boldsymbol\phi = \begin{pmatrix}\phi&\psi\end{pmatrix}^T$, in our case this becomes the pair of equations

\begin{align*}
\phi_{xxxx} + c \psi_x + e^{\phi} - 1 &= 0 \\
\psi - c \phi_x &= 0
\end{align*} 

Substituting the second equation into the first gives us

\begin{equation}
\phi_{xxxx} + c^2 \psi_{xx} + e^{\phi} - 1 = 0
\end{equation}

which is the same as \eqref{eqODE}. Thus, by Vdb2018, a symmetric homoclinic orbit solution to \eqref{eqODE} exists for all $c \in [\sqrt{0.5}, \sqrt{1.9}]$. Let $q(x; c)$ be this solution, and let $\textbf{q}=\begin{pmatrix}q&cq_x\end{pmatrix}$. (We use the notation $q$ for homoclinic orbits.) By smooth dependence on parameters, since \eqref{eqODE} is smooth in $c$, the map $c \mapsto \textbf{q}(x; c)$ is smooth in $c$. Since the solution $\textbf{q}(x; c)$ is smooth, part (c) in Assumption 2 of Grill87 is satisfied, i.e. $\textbf{q}(x; c) \in D(T'(0)^3) \cap D(J I T'(0)^2)$. Finally, since none of the derivatives in $x$ of $\textbf{q}(x; c)$ are 0, $T'(0)q(x; c) \neq 0$. Thus Assumption 2 in Grill87 is satisfied.\\

Finally, as in Grill87, we define the scalar

\begin{equation}\label{defd}
d(c) = E(\textbf{q}(x; c)) - c Q(\textbf{q}(x; c))
\end{equation}

and the operator 

\begin{equation}\label{defHc}
H_c = E''(\textbf{q}(x; c)) - c Q''(\textbf{q}(x; c))
\end{equation}

For this particular problem, we have

\begin{equation}
H_c = \begin{pmatrix}
\partial_x^4 + e^q & c \partial_x \\
-c \partial_x & 1
\end{pmatrix}
\end{equation}

We will verify Assumption 3B instead of Assumption 3. (This is suggested in the errata at the end of Grill90). For $\textbf{u} = \begin{pmatrix}u & v\end{pmatrix}^T$, 

\begin{align*}
\langle H_c \textbf{u}, \textbf{u} \rangle
&= \langle \begin{pmatrix} (\partial_x^4 + e^q)u + c \partial_x v \\ -c \partial_x u + v  \end{pmatrix},
\begin{pmatrix} u \\ v \end{pmatrix} \rangle \\ 
&= \langle (\partial_x^4 + e^q)u + c \partial_x v, u \rangle + \langle -c \partial_x u + v, v\rangle \\
&= \langle (\partial_x^4 + e^q)u, u \rangle + c \langle \partial_x v, u \rangle + c \langle -\partial_x u, v \rangle + \langle v, v \rangle \\
&= \langle (\partial_x^4 + e^q)u, u \rangle + c^2 \langle \partial_x^2 u, u \rangle - 2 c \langle \partial_x u, v \rangle + \langle v, v \rangle - c^2 \langle \partial_x^2 u, u \rangle\\
&= \langle A_0 u, u \rangle + \langle v, v \rangle - 2 c \langle v, \partial_x u \rangle + c^2 \langle \partial_x, \partial_x u \rangle\\
&= \langle A_0 u, u \rangle + || v - c \partial_x u||^2
\end{align*}

By Hypothesis \ref{A0neg}, $A_0$ has exactly one negative eigenvalue $\lambda_- < 0$ with corresponding eigenfunction $v_-(x)$. Let $\textbf{u} = \begin{pmatrix} v_-(x), c \partial_x v_-(x) \end{pmatrix}^T$. Substituting this in, we obtain

\begin{align*}
\langle H_c \textbf{u}, \textbf{u} \rangle &=
\langle A_0 v_-, v_- \rangle + || c \partial_x v_- - c \partial_x v_-||^2 \\
&= \lambda_- ||v_-||^2 < 0
\end{align*}

which verifies part (i) of Assumption 3B.\\

Next, we find kernel of $H_c$ by solving $H_c \textbf{u} = 0$. This is equivalent to solving the system of equations

\begin{align*}
u_{xxxx} + e^q u + c v_x &= 0 \\
-c u_x + v &= 0
\end{align*}

Multiplying the second equation by $c$, differentiating with respect to $x$, and substituting it into the first equation, we get

\begin{equation}
(\partial_x^4 + c^2 \partial_x^2 + e^q)u = 0
\end{equation}

which, in the notation of the previous section is

\begin{equation}
A_0 u = 0
\end{equation}

By Hypothesis \ref{kerA0hyp}, the kernel of $A_0$ is spanned by $\partial_x q$, thus the kernel of $H_c$ is spanned by $\begin{pmatrix} \partial_x q, c \partial_x^2 q \end{pmatrix}$. Define the subspace $S$ by

\begin{equation}
S = \spn\{ \partial_x q, v_-\}
\end{equation}

i.e. $S$ is the span of all eigenfunction of $A_0$ which correspond to eigenvalues which are not positive. Then define the space

\begin{equation}
\tilde{S} = \left\{
\begin{pmatrix}u & v\end{pmatrix}^T : u \in S^\perp, v = 0
\right\}
\end{equation}

Then $S$ is a closed subspace. For any $\textbf{u} = \begin{pmatrix}u & 0\end{pmatrix}^T \in \tilde{S}$, since $A_0$ is positive definite on $S^\perp$, there exists a constant $\delta > 0$ such that $\langle A_0 u, u \rangle \geq \delta ||u||^2$. Thus we have

\begin{align*}
\langle H_c \textbf{u}, \textbf{u} \rangle 
&= \langle A_0 u, u \rangle + || -c \partial_x u||^2 \\
&\geq \delta ||u||^2\\
&\delta ||\textbf{u}||^2
\end{align*}

This verifies part (ii) of Assumption 3B. Part (iii) holds since $v_-$ and $q_x$ are linearly independent. Having verified Assumptions 1, 2, and 3B, we can invoke Theorem 3 of Grill87 to conclude that $q(x)$ is stable if and only if $d''(c) > 0$. Differentiating \eqref{defd}, we have

\begin{align*}
d'(c) &= E'(\textbf{q})\partial_c \textbf{q} - Q(\textbf{q}) - c Q'(\textbf{q}) \partial_c \textbf{q} \\
&= [E'(\textbf{q}) - c Q'(\textbf{q})]\partial_c \textbf{q} - Q(\textbf{q}) \\
&= - Q(\textbf{q}) \\
&= -\int_{-\infty}^\infty q_x c q_x dx \\
&= -c ||q_x||^2
\end{align*}

since $E'(\textbf{q}) - c Q'(\textbf{q}) = 0$ by \eqref{boundstateeq}, since $\textbf{q}$ is a bound state. Differentiating once more, we have

\begin{align*}
d''(c) &= -\frac{\partial}{\partial c} \left( c ||q_x||^2 \right)
\end{align*}

Thus we can state the following proposition.

\begin{proposition}\label{stabcrit}
Let $c \in [\sqrt{0.5}, \sqrt{1.9}]$, and let $q(x; c)$ be the symmetric homoclinic orbit (single pulse) solution to \eqref{eqODE} associated with this speed $c$. Then $q(x)$ is stable if and only if $d''(c) > 0$, where

\begin{equation}\label{dcc}
d''(c) = -\frac{\partial}{\partial c} \left( c ||q_x||^2 \right)
\end{equation}
\end{proposition}

In our case, numerics suggest that $d''(c) > 0$, so $q(x)$ is stable. Thus we make the following hypothesis.

\begin{hypothesis}\label{hypdccpos}
$d''(c) > 0$ for $c \in [\sqrt{0.5}, \sqrt{1.9}]$.
\end{hypothesis}


\subsection{Numerics, Construction of Solutions}

Stationary solutions to \eqref{eqODE} are hard to come by. Nonetheless, we have figured out a way to compute these numerically, although it is a bit dicey. Our method is a variant of the mountain pass and string methods, and essentially combines Chen97 and Chamard10.

\begin{enumerate}
	\item The energy functional $I \in C^1(H^2, \R)$ is given by
	\begin{equation}
	I(z) = \frac{1}{2} \int_\R (|z''|^2 - c^2 |z'|^2)dx 
	+ \int_\R (e^z - z - 1) dx
	\end{equation}
	where $z$ is a function of $x$. For the numerical method, we truncate the domain to $[-L, L]$ for $L$ large.
	\item To do the mountain pass method, we need to find
	\begin{enumerate}
		\item A function $e_1 \in H^2$ which is a local minimum of $I$. From Lemma 2.4 in Chen97, $e_1 = 0$ satisfies this.
		\item A function $e_2 \in H^2$ for which $I(e_2) < I(e^1) = I(0) = 0$. We can use Lemma 2.5 in Chen97 to find such an $e_2$. 
	\end{enumerate} 
	The solution we want, by the Mountain Pass Lemma, is the infimum of the maxima of $I$ along all paths joining $e_1$ and $e_2$.
	\item Choose a discretization scheme. Following Chen97, we use the finite difference method.
	\item We then use the String Method from Chamard10.
	\begin{enumerate}
		\item Join $e_1$ and $e_2$ by a piecewise linear path $U(t) = t e_1 + (1-t) e_2 = (1-t)e_2$. Choose a discrete set of values for $t$ which are evenly spaced, i.e. the $N+1$ values $t = [0, 1/N, 2/N, ..., 1]$ for some reasonably sized $N$. We ended up using $N = 10$, which worked fine.
		\item Reparameterize the path to get points more-or-less equidistantly distributed in the $H^1$ norm.
		\item Evaluate the steepest decent direction $v(t_i)$ for all points $U(t_i)$ along the path. The steepest descent direction is computed as in section 3 of Chen97.
		\item For constrained maximal step size $h_M$, perform gradient descent on all points on the path $U(t_i)$ in the direction of $v(t_i)$.
		\item Go back to the reparameterization step, and repeat all this a bunch of times. Theoretically there is a termination condition, but we don't actually need to worry about that since we will be using Matlab's \texttt{fsolve}.
	\end{enumerate}
	\item After doing this a bunch of times, take the element of the path for which $I(U(t_i))$ is maximum. Or take one close to the maximum. Use this as an initial guess for Matlab's \texttt{fsolve}. This should converge to the solution we are looking for.
\end{enumerate}

Here are plots of the stationary solution $u(x)$ found using this method for $c = 1.354$ and $c = 1.40$. Compare these to figure 3 on p. 347 of Chen97. Note that by the substitution we performed, the baseline in our plots is at 0 as opposed to 1 in Chen97.

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{single1354.eps}
\includegraphics[width=8cm]{single14.eps}
\label{fig:single1}
\caption{Stationary traveling wave solutions to \eqref{eqODE}. $c = 1.354$ (left) and $c = 1.40$ (right).}
\end{figure}

We also compute solutions for other values of $c$.

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{single11.eps}
\includegraphics[width=8cm]{single12.eps}
\label{fig:single2}
\caption{Stationary traveling wave solutions to \eqref{eqODE}. $c = 1.1$ (left) and $c = 1.2$ (right).}
\end{figure}

Let $\nu = \pm \alpha \pm i \beta$ be the eigenvalues of the linearization about the zero solution. Then we can show numerically that the tail decays exponentially with rate (approximately) $\alpha$, and the period of the tail oscillations is (approximately) $2 \pi / \beta$. This is what we expect. \\

As in the other cases, we expect that we can join the tails to form double pulses every quarter period, i.e. every $\pi / 2 \beta$. Using the same method as we always use, together with Matlab's \texttt{fsolve}, here are the first four double pulses for $c = 1.2$.

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{double12_12.eps}
\includegraphics[width=8cm]{double12_34.eps}
% \caption{Double pulse traveling wave solutions to \eqref{eqODE} for $c = 1.2$. Double pulses 1 and 2 (left). Double pulses 3 and 4 (right).
\end{figure}

We can obtain periodic versions of all of these by interpolating the finite difference solutions onto the appropriate periodic grid, writing the operators using Fourier spectral matrices, and using \texttt{fsolve}. This is not shown.\\

\subsection{Numerics, Eigenvalue Computation}

It turns out that Matlab has a built-in polynomial eigenvalue solver, \texttt{polyeig}. However, I decided to use the quadratic eigenvalue solver \texttt{quadeig} by Chris Munro, Sven Hammarling and Francoise Tisseu (2013) because they say it's better. It probably makes very little difference.\\

Since this is going to look a lot like things I have already done (i.e. KdV5), I will not show all the plots. First, we look at $c = 1.2$.\\

For the first double pulse, here is the spectrum (zoomed near the origin) and a plot of the eigenfunctions corresponding to the small interaction eigenvalues. Note there is a pair of real interaction eigenvalues.

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{spec12_double1.eps}
\includegraphics[width=8cm]{evecs12_double1.eps}
\caption{Linearization about Double Pulse 1. Spectrum (left) and interaction eigenfunctions (right). Finite difference methods, $N = 512$, $c = 1.2$.}
\end{figure}

Next we look at the second double pulse. Note that this time we have a pair of purely imaginary (to leading order, there is a small real part of order like 1e-10 just as with KdV5) eigenvalues. The plot below is only the real part of the eigenfunctions. It is easy to locate the interaction eigenvalues in this case, since we have bounds on the essential spectrum. The interaction eigenvalues lie inside the essential spectrum gap.

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{spec12_double2.eps}
\includegraphics[width=8cm]{evecs12_double2real.eps}
\caption{Linearization about Double Pulse 2. Spectrum (left) and real part of interaction eigenfunctions (right). Finite difference methods, $N = 512$, $c = 1.2$.}
\end{figure}

The same pattern repeats (alternating real and purely imaginary interaction eigenvalues) as we go to higher double pulses.\\

We can also construct multipulses. For example, here is a triple pulse for $c = 1.2$ using the same pulse distance as double pulse 2. We get the expected two pairs of purely imaginary eigenvalues.

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{triple12_2.eps}
\includegraphics[width=8cm]{spec12_triple2.eps}
\caption{Triple pulse 2,2 (left). Spectrum of linearization about triple pulse 2,2 (right). Finite difference methods, $N = 512$, $c = 1.2$.}
\end{figure}

For other values of $c$, the odd-numbered double pulses have the same eigenvalue pattern, so we will focus on the even-numbered double pulses. Let's look at $c = 1.354$ (this value was used in Chen97). For Double Pulse 2, we now have a quartet of interaction eigenvalues. Their imaginary part now lies \emph{outside} the essential spectrum gap. This suggests that a Krein collision has occurred, where the interaction eigenvalues collide with the smallest essential spectrum eigenvalue and then form a quartet.

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{spec1354_double2.eps}
\caption{Linearization about Double Pulse 2. Spectrum showing Krein quartet. Finite difference methods, $N = 512$, $c = 1.354$.}
\end{figure}


The next two plots show that the Krein collision occurs between $c = 1.322$ (before collision) and $c = 1.323$ (after collision).

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{spec1322_double2.eps}
\includegraphics[width=8cm]{spec1323_double2.eps}
\caption{Linearization about Double Pulse 2. Spectrum before Krein collision ($c = 1.322$, left) and after Krein collision ($c = 1.323$, right). Finite difference methods, $N = 512$}
\end{figure}

\subsection{Lin's Method (part 1)}

In this section, we look at the eigenvalues of $A_0$, i.e. look at the eigenvalue problem

\[
A_0 v = \lambda v
\]

We expect this to be similar to SanStrut, since it's more-or-less the same thing but with a different nonlinearity. Using Matlab's \texttt{eig}, we can show that for the operator $A_0$ linearized about an $n-pulse$

\begin{enumerate}
	\item The essential spectrum is $[a, \infty)$ for some $a > 0$
	\item $n$ small eigenvalues near 0, one of which is actually at 0 (1D kernel)
	\item There are $n$ negative eigenvalues clustered near the negative eigenvalue of $A_0$ for the single pulse.
\end{enumerate}

We can verify this analytically.\\

In system-form, the operator $A_0$ is given by

\begin{equation}
A_0(\lambda; u_*) = \begin{pmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
\lambda - e^{u_*} & 0 & -c^2 & 0 
\end{pmatrix}
\end{equation}

where $u^*$ is an equilibrium solution. The essential spectrum only depends on the asymptotic matrix, so since all solutions $u^*(x)$ we will care about are decay exponentially to 0 at both ends, we only have to look at the asymptotic matrix

\begin{equation}
A_0(0) = \begin{pmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
\lambda - 1 & 0 & -c^2 & 0 
\end{pmatrix}
\end{equation}

The characteristic polynomial of this is

\[
p(\nu) = c^2 \nu^2 - \lambda + \nu^4 + 1
\]

Plugging in $\nu = i r$, this becomes

\[
p(i r) = c^2 r^2 - \lambda + r^4 + 1 = 0
\]

Solving for $\lambda$ gives us

\[
\lambda = r^4 + c^2 r^2 + 1 = r^2(r^2 + c^2) + 1
\]

from which we see that the essential spectrum is $\lambda = [1, \infty)$.\\

Using Lin's method (from San98) we can compute the small eigenvalues to leading order. It is easy to see that $q'$ is an eigenfunction with eigenvalue 0. For convenience, assume we a have an $n$-pulse, but assume that all the pulse distances are the same, say $2L$. (Of course, mixed-distance multipulses exist, but we will ignore that for now.) The, by Theorem 2 in San98, there are $n$ small eigenvalues which are given (to leading order) by solving $\det(aA - M \lambda I) = 0$. $M$ is the Melnikov integral, which in this case is

\begin{equation}
M = \int_{-\infty}^\infty (q'(x))^2 dx
\end{equation}

$A$ is the matrix

\begin{equation}
A = \begin{pmatrix}
-1 & 1 \\
1  & -2 & 1 \\
   & 1  & -2 & 1 \\
   & & \vdots & \vdots \\
   & & & & & 1 & -1
\end{pmatrix}
\end{equation}

and $a = \langle \Psi(L), Q'(-L) \rangle$, as in San98. For a 2-pulse, this becomes

\begin{align*}
\det \begin{pmatrix}
-a - M \lambda & a \\
a & -a - M \lambda
\end{pmatrix} = 0
\end{align*}

which has solutions $\lambda = \{ 0, -2a/M \}$. This is easy to compute numerically, so let's see if this agrees with what we get if we use \texttt{eig} on $A_0$.

\begin{table}[H]
\begin{tabular}{llll}
Double Pulse & Eigenvalue from eig(A0) & Predicted eigenvalue (-2a/M) & Error  \\
1            & 0.1278                  & 0.1915                       & .5     \\
2            & -0.0278                 & -0.0254                      & 0.08   \\
3            & 0.0090                  & 0.0072                       & .20    \\
4            & -0.0024                 & -0.0025                      & 0.0417 \\
5            & 6.8497e-04              & 7.2475e-04                   & 0.0581
\end{tabular}
\caption{Calculated and predicted small eigenvalue of $A_0$ about Double Pulse 2. $c = 1.2$. Finite difference methods, $N = 1500$}.
\end{table}

Well, it's not great, but the error does decrease as $L$ increases, so it should be ok. For the triple pulse we have to solve

\begin{align*}
\det \begin{pmatrix}
-a - M \lambda & a & 0\\
a & -2a - M \lambda & a \\
0 & a & -a - M \lambda
\end{pmatrix} = 0
\end{align*}

which has solutions $\lambda = \{ 0, -a/M -3a/M \}$.\\

For $c = 1.2$ and triple pulse 2,2, we have small eigenvalues from \texttt{eig} of $\{ -0.0145, -0.0401 \}$. The formula above predicts eigenvalues of $( -0.0127, -0.0381 \}$. The errors here are $0.049$ and $0.124$, which are not terrible.\\

We don't really care about this, except to contrast to the quadratic eigenvalue problem.\\

Similarly, using a method similar to SanStrut, we can show that $A_0(q)$ has a single negative eigenvalue, and we can use Lin's method to show that $A_0(q_n)$ has a $n$ negative eigenvalues all of which are close to the single negative eigenvalue of $A_0(q)$.

\subsection{Krein Matrix}

\subsubsection{Background and Numerics}

Before we do that, we can look at the Krein matrix as means of computing eigenvalues in the case where they are (predicted to be) purely imaginary. The hope is that we can get theoretical results for the 2-pulse and nice numerics for the n-pulse.
\\

This is an application Todd Kapitula's unpublished paper Krein Matrix for Star-Even Operator Polynomials (2018). Here we look at the polynomial eigenvalue problem for the suspension bridge problem linearized about stationary solution $q$. This second degree eigenvalue problem is given by

\begin{equation}
p(\lambda)v = (A_0 + A_1 \lambda + A_2 \lambda^2 )v = 0
\end{equation}

where

\begin{align*}
A_0 &= \partial_x^4 + c^2 \partial_x^2 + e^{q} \\
A_1 &= - 2 c \partial_x \\
A_2 &= I 
\end{align*}

Note that $A_0$ and $A_2$ are Hermitian, and $A_1$ is skew-Hermitian.\\

The Krein matrix should work perfectly in this case, since we don't have to deal with invertibility of operators and since the essential spectrum is purely imaginary and bounded away from the origin.\\

We know from the discussion in the previous section what the spectrum/eigenvalues of $A_0$ looks like. Since $A_0$ is Hermitian, the spectrum of $A_0$ is real. We also note that if we take $\lambda = i z$, the quadratic operator $p(iz)$ is Hermitian.
\\

Let $S = \spn\{s_1, \dots, s_n \}$, where $s_i$ are the eigenfunctions of $A_0$ corresponding to the small eigenvalues $\{\nu_1, \dots, \nu_n$. (We found these in the previous section using Lin's method). WLOG, we take $\nu_1 = 0$ and $s_1 = q'$, since we know the kernel of $A_0$ is 1-dimensional, and we know what the kernel eigenfunction is. We also note that since $A_0$ is self-adjoint and the eigenvalues $\nu_i$ are distinct, the corresponding eigenfunctions $s_i$ are orthogonal. Since we can scale the eigenfunctions any way we like, in order to make some later expressions look ``nicer'', we will take

\begin{equation}\label{orthonormaleigs}
\langle s_i, s_j \rangle = ||q_x||^2 \delta_{ij}
\end{equation}

With this setup, we can easily compute the Krein matrix using the top of p.4 in Kap2018. As long as as choose $c$ such that the Krein collision has not yet occurred, we get very accurate numerical results from this method, or at least they compare extremely well to the quadratic eigenvalue solver we are using. 

\begin{table}[H]
\begin{tabular}{lll}
Pulse & From \texttt{quadeig} & imag part (from Krein Matrix) \\
Double Pulse 2    & $\pm 0.0622i$ & 0.0622 \\
Triple Pulse 2,2  & $\pm 0.0446i, \pm 0.0764i$ & 0.0446, 0.0764 \\
Double Pulse 2    & $\pm 0.0176i$ & 0.0176 \\
\end{tabular}
\caption{Eigenvalues from quadratic eigenvalue problem, computed with \texttt{quadeig} and from the Krein matrix. $c = 1.2$, finite difference method, $N = 512$. }
\end{table}

So, basically, the Krein matrix nails it every time.\\

\subsubsection{Approxmation for Small Eigenvalues}

Now that we know we can use the Krein matrix, we should be able to use the approximation in Section 5 of Kap2018 since these eigenvalues are small.\\

Since we are looking for purely imaginary eigenvalues, we take $\lambda = i z$. Then the operator $p(iz)$ is Hermitian, which implies that the Krein matrix is also Hermitian (p. 6 of Kap2018). Following Lemma 5.6 in Kap2018, for small $|z|$ the Krein matrix is the $n \times n$ matrix

\begin{equation}
K_S(z) = ||q_x||^2 \text{diag}(\nu_1, \dots, \nu_n) + \overline{z} K_1 - \overline{z}^2 \tilde{K}_2 + \mathcal{O}(|z|^3)
\end{equation}

where

\begin{equation}
(K_1)_{jk} = \langle s_j, i A_1 s_k \rangle
\end{equation}

and

\begin{equation}
(\tilde{K}_2)_{jk} = \langle s_j, (A_2 - A_1 P_{S^\perp} (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} A_1 ) s_k  \rangle
\end{equation}

Compared to Lemma 5.6, note that we have omitted the $z$ out front, i.e. we use the form of the Krein matrix from p.4 rather than the form in Theorem 3.1. Since we also scaled the eigenfunctions differently, we have an additional factor of $||q_x||^2$ in the first term. The key observation is that this is, to leading order, a matrix-valued quadratic polynomial in $z$.\\

Since the operator $A_1$ is skew-Hermitian, $i A_1$ is Hermitian, thus for $K_1$ we have

\begin{align*}
(K_1)_{jk} = \langle s_j, i A_1 s_k \rangle &= \langle i A_1 s_j, s_k \rangle \\
&= \overline{ \langle s_k, i A_1 s_j \rangle } \\
&= \overline{ (K_1)_{kj} }
\end{align*}

As expected, $K_1$ is a Hermitian matrix. In particular, this implies that the diagonal entries are real. Since $A_0$ and the eigenfunctions $s_i$ are real, $(K_1)_{ii} = \langle s_i, i A_1 s_i \rangle = \langle s_i, i A_1 s_i \rangle$ is purely imaginary, thus must be 0. We conclude that diagonal terms of $K_1$ are 0.\\

For the matrix $\tilde{K}_2$, we have 

\begin{align*}
(\tilde{K}_2)_{jk} &= \langle s_j, s_k \rangle - \langle s_j, A_1 P_{S^\perp} (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} A_1 s_k  \rangle \\
&= \langle s_j, s_k \rangle + \langle P_{S^\perp} A_1 s_j, (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} A_1 s_k  \rangle
\end{align*}

since $A_2 = I$. Since the eigenfunctions are orthogonal, by our chosen scaling we have, $\langle s_j, s_k \rangle = ||q_x||^2 \delta_{jk}$. Thus, $\tilde{K} = ||q_x||^2 I + K_2$, where

\begin{align*}
(K_2)_{jk} &= 
\langle P_{S^\perp} A_1 s_j, (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} A_1 s_k \rangle
\end{align*}

Since $A_0$ and its spectral projections are self-adjoint, and $A_0$ and the eigenfunctions are real, $K_2$ is a real, symmetric matrix. Thus the Krein matrix takes the form

\begin{equation}\label{Kreinform}
K_S(z) = ||q_x||^2 \text{diag}(\nu_1, \dots, \nu_n) + \overline{z} K_1 
- \overline{z}^2 ( ||q_x||^2 I + K_2) + \mathcal{O}(|z|^3)
\end{equation}

where $K_1$ and $K_2$ are defined above. For convenience, we let

\begin{align*}
a_{ij} &= (K_1)_{ij} \\
b_{ij} &= (K_2)_{ij}
\end{align*}

Before we continue, we will characterize the Krein matrix for the 1-pulse, since in that case will it will be a scalar.

% Lemma : Krein matrix, 1-pulse

\begin{lemma}\label{Krein1pulse}
For the linearization about the 1-pulse, the Krein matrix is the scalar

\begin{equation}
K(z) = d''(c) \overline{z}^2 + \mathcal{O}(z^3)
\end{equation}

where $d''(c)$ is the stability criterion from Grill87. The form of $d''(c)$ for this problem is given in \eqref{dcc}.

\begin{proof}

For the 1-pulse, there is exactly one eigenvalue at 0, for which the corresponding eigenfunction is $q_x$, i.e. $\nu_1 = 0$ and $s_1 = q_x$ (there is no scaling needed here). Let $S = \ker A_0$ = $\spn \{q_x\}$. In this case, the Krein matrix will be 1x1, i.e. a scalar. Since $K_1$ is 0 on the diagonal $K_1 = 0$. Thus we are left with

\begin{align*}
K(z) &= -\overline{z}^2 \left( ||q_x||^2 + \langle P_{S^\perp} A_1 q_x, (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} A_1 q_x \rangle \right) + \mathcal{O}(z^3) \\
&= -\overline{z}^2 \left( ||q_x||^2 + 4 c^2 \langle P_{S^\perp} q_{xx}, (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} q_{xx} \rangle \right) + \mathcal{O}(z^3)
\end{align*}

All that remains is to evaluate $P_{S^\perp} (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} q_{xx}$. Since $S$ is the span of a single odd function, and $q_{xx}$ is an even function, $P_{S^\perp} q_{xx} = q_{xx}$. Thus we only need to evaluate $(P_{S^\perp} A_0 P_{S^\perp})^{-1} q_{xx}$. Let $y = (P_{S^\perp} A_0 P_{S^\perp})^{-1} q_{xx}$. Then $(P_{S^\perp} A_0 P_{S^\perp})y = q_{xx}$. From \eqref{uc}, we have

\begin{equation*}\label{uc}
A_0 \left( -\frac{1}{2c} q_c \right) = q_{xx}
\end{equation*}

Thus $s = -(1/2c) q_c + k q_x$ for some scalar $k$. Substituting this into the equation for $K(z)$, and noting that the $k q_x$ term is wiped out by the projection $P_{S^\perp}$, we have

\[
K(z) = -\overline{z}^2 \left( ||q_x||^2 - 2c \langle q_{xx}, q_c \rangle \right) + \mathcal{O}(z^3)
\]

Integration by parts gives us

\begin{align*}
||q_x||^2 - 2c \langle q_{xx}, q_c \rangle &= \langle q_x, q_x \rangle + 2c \langle q_{x}, q_{xc} \rangle  \\
&= \langle q_x, q_x \rangle + c \frac{\partial}{\partial c}\langle q_x, q_x \rangle \\
&= ||q_x||^2 + c \frac{\partial}{\partial c}||q_x||^2 \\
&= \frac{\partial}{\partial c} \left( c||q_x||^2 \right)
\end{align*}

Thus the Krein matrix becomes 

\begin{align*}
K(z) &= -\frac{\partial}{\partial c} \left( c||q_x||^2 \right) \overline{z}^2 + \mathcal{O}(z^3) \\
&= d''(c) \overline{z}^2 + \mathcal{O}(z^3)
\end{align*}

\end{proof}
\end{lemma}

We will need the following result for exponentially localized functions, which we prove in the following lemma.

\begin{lemma}\label{exploc}
Let $q_+(x)$ and $q_-(x)$ be exponentially localized pulses which decay exponentially with rate $\alpha$ and whose peaks are separated by a distance $2 X$. Then 

\begin{enumerate}[(i)]
	\item $ || q_-(x) q_+(x)||_{L^\infty(\R)} \leq C e^{2 \alpha X}$ 
	\item $ | \langle q_-(x) q_+(x) \rangle | \leq C X e^{2 \alpha X}$ 
	\item For any $0 < \epsilon < \alpha$, $| \langle q_-(x) q_+(x) \rangle | \leq C e^{(2 \alpha - \epsilon) X}$ 
\end{enumerate}

\begin{proof}
Since we are working on $\R$ and the norms are translation invariant, we can WLOG take the peaks of $q_\pm(x)$ to be centered at $\pm X$, so that we have

\begin{align*}
|q_-(x)| \leq C e^{-\alpha|x + X|} \\
|q_+(x)| \leq C e^{-\alpha|x - X|} \\
\end{align*}

\begin{enumerate}[(i)]

\item For the $L^\infty$ norm, we split it up into four pieces and take the maximum over those pieces

\begin{align*}
|| q_-(x) & q_+(x)||_{L^\infty(\R)} \\
&= \max\{ || q_-(x) q_+(x)||_{L^\infty(-\infty, -X]} + || q_-(x) q_+(x)||_{L^\infty[-X, 0])} \\
&+ || q_-(x) q_+(x)||_{[0, X]} + || q_-(x) q_+(x)||_{L^\infty([X, \infty)} \}
\end{align*}

For $x \in (-\infty, -X]$ 

\begin{align*}
| q_-(x) q_+(x) | &\leq C e^{\alpha|x + X|} e^{-\alpha|x - X|} \\
&= C e^{\alpha(x + X)} e^{\alpha(x - X)} \\
&= C e^{2 \alpha x} \\
&\leq C e^{-2 \alpha X} 
\end{align*}

since we are on the interval $(-\infty, -X]$. For $x \in [-X,0]$, 

\begin{align*}
| q_-(x) q_+(x) | &\leq C e^{\alpha|x + X|} e^{-\alpha|x - X|} \\
&= C e^{-\alpha(x + X)} e^{\alpha(x - X)} \\
&= C e^{-2 \alpha X}  \\
\end{align*}

The other two pieces are similar, and give identical bounds. Since all four pieces have the same uniform bound, the result follows.

\item For the $L^2$ inner product, we again split the domain up into four pieces

\begin{align}\label{splitL2}
| \langle q_-(x) &q_+(x) \rangle | \\
&\leq \int_{-\infty}^{-X} |q_-(x) q_+(x)| dx + \int_{-X}^0 |q_-(x) q_+(x)|
+\int_0^X |q_-(x) q_+(x)| dx +\int_X^\infty |q_-(x) q_+(x)| dx \nonumber
\end{align}

For the first integral, we have

\begin{align*}
\int_{-\infty}^{-X} |q_-(x) q_+(x)| dx &\leq C \int_{-\infty}^{-X} e^{\alpha(x + X)} e^{\alpha(x - X)} dx \\
&= C \int_{-\infty}^{-X} e^{2 \alpha x} dx \\
&= C e^{-2 \alpha X}
\end{align*}

For the second integral, we have

\begin{align*}
\int_{-X}^0 |q_-(x) q_+(x)| dx &\leq C \int_{-X}^0 e^{-\alpha(x + X)} e^{\alpha(x - X)} dx \\
&= C \int_{-X}^0 e^{-2 \alpha X} dx \\
&= C X e^{-2 \alpha X}
\end{align*}

The other two integrals are similar. Combining these gives us the bound (ii). For the bound (iii), choose any $\epsilon$ with $0 < \epsilon < \alpha$. Then for the second integral in \eqref{splitL2}, we also have

\begin{align*}
\int_{-X}^0 |q_-(x) q_+(x)| dx &\leq C \int_{-X}^0 e^{-\alpha(x + X)} e^{\alpha(x - X)} dx \\
&= C \int_{-X}^0 e^{-(\alpha-\epsilon)(x + X)} e^{-\epsilon(x + X)}  e^{\alpha x} e^{-\alpha X} dx \\ 
&= C e^{-(2 \alpha - \epsilon) X} \int_{-X}^0 e^{-(\alpha-\epsilon)x} e^{-\epsilon(x + X)}  e^{\alpha x}  dx \\ 
&= C e^{-(2 \alpha - \epsilon) X} \int_{-X}^0 e^{\epsilon x} e^{-\epsilon(x + X)} dx \\ 
&\leq C e^{-(2 \alpha - \epsilon) X} \int_{-X}^0 e^{-\epsilon(x + X)} dx \\ 
&= C e^{-(2 \alpha - \epsilon) X} \frac{1}{\epsilon}\left( 1 - e^{-\epsilon X} \right) dx \\ 
&\leq C e^{-(2 \alpha - \epsilon) X} dx \\ 
\end{align*}

where we used the fact that $0 < e^{\epsilon x} \leq 1$ since $x \leq 0$. Although we can choose any $\epsilon$ we want, the constant $C$ increases as $\epsilon$ gets smaller. The third integral in \eqref{splitL2} is similar. Since this bound is weaker than the bound for the first and fourth integrals in $\eqref{splitL2}$, the result (iii) follows.

\end{enumerate}

\end{proof}
\end{lemma}

From Lin's method (San98), the $n-$pulse $q_n$ resembles, to leading order, $n$ copies of the primary pulse $q$. The $n$ peaks are separated by distances $2 X_1, \dots, 2 X_{n-1}$. We can write $q_n$ as 

\begin{align}\label{qn}
q_n = \sum_{i = 1}^{n} q^i + r
\end{align}

where each $q^i$ is a translate of the primary pulse $q$ by some distance $L_i$, i.e. 

\begin{equation}\label{qi}
q^i = q(\cdot - L_i)
\end{equation}

For $i, j = 1, \dots, n$, let $\rho_{ji}$ be the signed distance between peak $i$ and peak $j$, i.e. 

\begin{equation}\label{rhoji}
\rho_{ji} = L_j - L_i
\end{equation}

We note that for all $i, j$, $|\rho_{ji}| \geq 2 X_m$. We also get from Lin's method a uniform bound on the remainder term, i.e. $||r|| \leq C e^{-\alpha X_m}$, which also holds for derivatives with respect to $x$ (see, for example, Theorem 1 in San98). From the Lin's method construction of $q_n$, the translates $q^i$ and all their derivatives are exponentially separated, thus Lemma \ref{exploc} applies with $X = X_m$ for $q^i, q^j$ and any of their derivatives with $i \neq j$.\\

Similarly, using Lin's method, we can write the eigenfunctions of $A_0$ in terms of translates of the derivative of the primary pulse. Each eigenfunction is, to leading order, a linear combination of these translates, i.e. for $j = 1, \dots, n$

\begin{align}\label{sj}
s_j = \sum_{i = 1}^{n} d_{ji} q^i_x + w_j
\end{align}

for constants $d_{ji} \in \C$. Let $D$ be the matrix defined by $(D)_{ji} = d_{ji}$. Note that this is not the same matrix $D$ as in San98. For the remainder terms we have from Lin's method (see, for example, San98)

\begin{equation}\label{sjwbound}
||w_j|| \leq C e^{-2 \alpha X_m}
\end{equation}

which also holds for derivatives with respect to $x$. \\

In the following lemmas, we will characterize the matrices $K_1$ and $K_1$, with the goal of showing that the Krein matrix is diagonal to leading order. First we will show that the entries of the matrix $K_1$ are exponentially small.\\

% lemma : K1 is exponentially small

\begin{lemma}\label{K1small}
The matrix $K_1$ is exponentially small, i.e. 

\begin{equation}
K_1 = \mathcal{O}(e^{-(3 \alpha/2) X_m} )
\end{equation}

\begin{proof}

Let $a_{jk} = (K_1)_{jk}$. We know from above that $a_{jj} = 0$, so we only need to consider the case $j \neq k$. Since $a_{jk} = \langle s_j, i A_1 s_k\rangle = 2 c i \langle s_j, (s_k)_x \rangle$, all we need to show is that $\langle s_j, (s_k)_x \rangle = \mathcal{O}(e^{-\alpha X_m})$. Using the expression \eqref{sj} for the eigenfunctions, we have

\begin{align*}
\langle s_j &, (s_k)_x \rangle = 
\langle \left( \sum_{i = 1}^{n} d_{ji} q^i_x + w_j \right) \left( \sum_{i = 1}^{n} d_{ki} q^j_{xx} + (w_k)_x \right) \rangle \\
&= \sum_{i = 1}^{n} d_{ji} d_{ki} \langle q^i_x, q^i_{xx} \rangle 
+ \sum_{i \neq l} d_{ji} d_{kl} \langle q^i_x, q^l_{xx} \rangle 
+ \langle \sum_{i = 1}^{n} d_{ji} q^i_x + w_j, (w_k)_x \rangle 
+ \langle w_j, \sum_{i = 1}^{n} d_{ki} q^j_{xx} \rangle \\
&= \sum_{i = 1}^{n} d_{ji} d_{ki} \langle q^i_x, q^i_{xx} \rangle 
+ \sum_{i \neq l} d_{ji} d_{kl} \langle q^i_x, q^l_{xx} \rangle 
+ \langle s_j, (w_k)_x \rangle 
+ \sum_{i = 1}^{n} d_{ki} \langle w_j, q^j_{xx} \rangle
\end{align*}  

For the first term on the RHS, since the $L^2$ inner product on $\R$ is translation invariant and the $q^i$ are translates of $q$, we have

\[
\langle q^i_x, q^i_{xx} \rangle = \langle q_x, q_{xx} \rangle = 0
\] 

since we are taking the inner product of an even and an odd function. For the second term on the RHS, we use Lemma \ref{exploc} with $\epsilon = \alpha / 2$. Since $q^i_x$ and $q^l_{xx}$ are exponentially separated by at least $2 X_m$, $\langle q^i_x, q^l_{xx} \rangle = \mathcal{O}(e^{-(3 \alpha/2) X_m})$. For the third term, by Holder's inequality,

\[
|\langle s_j, (w_k)_x \rangle| \leq ||s_j||_{L^1} ||(w_k)_x||_{L^\infty} \leq C e^{-2 \alpha X_m}
\]

using the remainder bound \eqref{sjwbound} and the fact that the eigenfunctions are exponentially localized, thus integrable. The same bound holds for the final term, again by Holder's inequality. Since the bound for the second term in the RHS is the weakest, it dictates the overall bound.

\end{proof}
\end{lemma}

Next, we show that the rows of the matrix $D$ defined above are orthonormal to leading order.

% lemma : orthogonal D

\begin{lemma}\label{orthogonalD}
Let $D$ be the matrix defined above representing the coefficients of the translates of the derivative of the primary pulse. Then the rows of $D$ are orthonormal to leading order, i.e. 

\begin{equation}
\sum_{i = 1}^{n} d_{ji} d_{ki} = \delta_{ij} + \mathcal{O}(e^{-(3 \alpha/2) X_m})
\end{equation}

\begin{proof}
Since the eigenvalues $\nu_i$ are distinct, the eigenfunctions $s_i$ are orthogonal, and we have scaled them so that $||s_i|| = ||q_x||$. Thus, using the expression \eqref{sj} for the eigenfunctions, we have, similar to Lemma \ref{K1small}

\begin{align*}
||q_x||^2 \delta_{jk} &= \langle s_j, s_k \rangle \\
&= \langle \sum_{i = 1}^{n} d_{ji} q^i_x + w_j, \sum_{i = 1}^{n} d_{ki} q^i_x + w_k\rangle \\
&= \sum_{i = 1}^{n} d_{ji} d_{ki} \langle q^i_x, q^i_{x} \rangle 
+ \sum_{i \neq l} d_{ji} d_{kl} \langle q^i_x, q^l_{x} \rangle 
+ \langle s_j, w_k \rangle 
+ \sum_{i = 1}^{n} d_{ki} \langle w_j, q^j_{x} \rangle
\end{align*}

As in Lemma \ref{K1small}, the second term on the RHS is order $e^{-(3 \alpha/2) X_m}$ and the last two terms on the RHS are order $e^{-2 \alpha X_m}$. By translation invariance, we have for all $i$

\[
\langle q^i_x, q^i_{x} \rangle = \langle q_x, q_{x} \rangle = ||q_x||^2
\]

Thus we have
\[
||q_x||^2 \sum_{i = 1}^{n} d_{ji} d_{ki} = ||q_x||^2 \delta_{ij} + \mathcal{O}(e^{-(3 \alpha / 2) X_m})
\]

dividing by $||q_x||^2$ gives us
\[
\sum_{i = 1}^{n} d_{ji} d_{ki} = \delta_{ij} + \mathcal{O}(e^{-(3 \alpha / 2) X_m})
\]

\end{proof}
\end{lemma}

Before we can analyze matrix $K_2$, we need to deal with the term involving $(P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} A_1 s_k$ in $K_2$. We do that in a series of lemmas. First, we show that $(P_{S^\perp} A_0 P_{S^\perp})^{-1}$ is a bounded linear operator on $S^\perp$.

% lemma : PA_0 is invertible on Sperp

\begin{lemma}\label{PA0inv}
When restricted to $S^\perp$, $P_{S^\perp} A_0 |_{S^\perp}$ is an invertible linear operator, and its inverse $(P_{S^\perp} A_0 |_{S^\perp})^{-1}$ is a bounded linear operator. If $z \in S^\perp$ is smooth, then $y = (P_{S^\perp} A_0 |_{S^\perp})^{-1} z$ is smooth as well.

\begin{proof}
Consider the operator $P_{S^\perp} A_0$. Since $A_0$ is self-adjoint and the projection $P_{S^\perp}$ is self-adjoint, $P_{S^\perp} A_0$ is also self-adjoint, since

\[
(P_{S^\perp} A_0)^* = A_0^* P_{S^\perp}^* = A_0 P_{S^\perp} = P_{S^\perp} A_0
\]

where the last equality holds since operators commute with their own spectral projections. We showed above that the essential spectrum of $A_0$ is $\sigma_{\text{ess}} = [1, \infty)$. Thus for any $\lambda \notin [1, \infty)$, the operator $A_0 - \lambda I$ is Fredholm with index 0. In particular, $A_0$ and $(A_0 - \nu_i I)$ are Fredholm with index 0. Since $A_0$ is Fredholm, its range is closed. Thus by the closed range theorem (see Yosida, etc), 

\begin{equation}
\ran A_0 = (\ker A_0^*)^\perp = (\ker A_0)^\perp
\end{equation}

where the last equality holds since $A_0$ is self-adjoint. Similarly, since $\nu \in \R$ and $\nu < 1$, we have

\begin{equation}\label{KerRangeNu}
\ran (A_0 - \nu_i I) = (\ker (A_0 - \nu_i I))^\perp 
\end{equation}

Next, we look at the kernel of $P_{S^\perp} A_0 = A_0 P_{S^\perp}$. This clearly contains $S$ (the kernel of $P_{S^\perp}$), but we need to ensure that it contains nothing else. It contains the kernel of $A_0$, but $\ker A_0 = \spn\{ q_n' \}$, and $q_n' \in S$, so we have already taken care of that. Finally, it contains any function $y$ for which $(A_0 - \nu_i I) y = s_i$, since that will be wiped out by the projection $P_{S^\perp}$. But we cannot have a function $y$ with $(A_0 - \nu_i I) y = s_i$, since by \eqref{KerRangeNu}, we would have $s_i \perp \ker (A_0 - \nu_i I)$, which contains $s_i$. Thus we conclude that

\[
\ker P_{S^\perp} A_0 = S 
\]

Since the range of $A_0$ is closed and $P_{S^\perp}$ is bounded, the range of $P_{S^\perp} A_0$ is closed. Thus by the closed range theorem and the fact that $P_{S^\perp} A_0$ is self-adjoint,

\[
\ran P_{S^\perp} A_0 = (\ker (P_{S^\perp} A_0)^*)^\perp = (\ker (P_{S^\perp} A_0))^\perp = S^\perp
\]

Since $\dim \ker P_{S^\perp} A_0 = \text{codim } P_{S^\perp} A_0 = 2$, $P_{S^\perp} A_0$ is a Fredholm operator with index 0.\\

If we restrict the operator $P_{S^\perp} A_0$ to $S^\perp$ (which is well-defined since its range is $S^\perp$), we get rid of the 2-dimensional kernel, thus obtain an invertible operator. Thus for $z \in S^\perp$ we can solve the equation

\[
(P_{S^\perp} A_0) y = z
\]

uniquely for $y \in S^\perp$, i.e. we have for $z \in S^\perp$,

\begin{equation}
y = (P_{S^\perp} A_0 |_{S^\perp})^{-1} z
\end{equation}

Note that in Kap2018, this operator is written $(P_{S^\perp} A_0 P_{S^\perp})^{-1}$. Since the spectrum of $P_{S^\perp} A_0 |_{S^\perp}$ is bounded away from 0 (by what we discussed above about the spectrum of $A_0$), the spectrum of $(P_{S^\perp} A_0 |_{S^\perp})^{-1}$ is bounded by a constant, i.e. 

\[
\text{spec } P_{S^\perp} A_0 |_{S^\perp}  \subset [-\rho, \rho]
\]

for some $\rho > 0$. Since $P_{S^\perp} A_0$ is self-adjoint, we can use the resolvent bound for normal operators to conclude that $(P_{S^\perp} A_0 |_{S^\perp})^{-1}$ is a bounded linear operator with operator norm

\[
|| (P_{S^\perp} A_0|_{S^\perp})^{-1} || \leq \frac{1}{\rho}
\]

Since $A_0: H^4 \in L^2 \rightarrow L^2$, if $z$ is smooth then $y (P_{S^\perp} A_0 |_{S^\perp})^{-1} z$ must be smooth as well.

\end{proof}
\end{lemma}

Next, using the definition for $A_1$ and the expression \eqref{sj} for $s_k$, we need to evaluate

\begin{align*}
(P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} A_1 s_k 
&= -2c \sum_{i = 1}^{n} d_{ji} (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} q^i_{xx} + (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} (w_j)_x 
\end{align*}

Since $(P_{S^\perp} A_0 P_{S^\perp})^{-1}$ is a bounded linear operator and the remainder term $(w_j)_x = \mathcal{O}(e^{-\alpha X_m})$, the last term on the RHS is also $\mathcal{O}(e^{-\alpha X_m})$. Thus all we need to do is evaluate $(P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} q^i_{xx}$. We will do this using Lin's method. Before we can do that, we prove a useful lemma and corollary which gives a leading order expansion for $e^{q_n(x)}$.

% lemma : exponential separation 

\begin{lemma}\label{expsep}

For the $n-$pulse $q_n(x)$ and for all $i = 1, \dots, n$

\begin{equation}
\exp(q_n(x)) = \exp( q^i(x)) + \sum_{j \neq i} (\exp(q^j(x)) - 1) + \mathcal{O}(e^{-\alpha X_m}) 
\end{equation}

\begin{proof}

Recall from \eqref{qn} that 

\[
q_n = \sum_{i = 1}^{n} q^i + \mathcal{O}(e^{-\alpha X_m})
\]

Fix $i$, and let $s(x) = \sum_{j \neq i} q_j(x)$. Then we have

\[
q_n = q^i + s + \mathcal{O}(e^{-\alpha X_m})
\]

Using a Taylor expansion, 

\begin{align*}
\exp(q_n(x)) &= \exp( q^i(x) + s(x) + \mathcal{O}(e^{-\alpha X_m}) ) \\
&= \exp( q^i(x) + s(x) ) \exp( \mathcal{O}(e^{-\alpha X_m}) )\\
&= \exp( q^i(x) + s(x) ) (1 + \mathcal{O}(e^{-\alpha X_m})) \\
&= \exp( q^i(x) + s(x) ) + \mathcal{O}(e^{-\alpha X_m}) \\
&= \exp( q^i(x) )\exp(s(x)) + \mathcal{O}(e^{-\alpha X_m})
\end{align*}

where the penultimate line holds since $q$ is bounded. Looking at the first term on the RHS, we have

\begin{align*}
\exp( q^i(x))\exp(s(x)) &=
\sum_{m=0}^\infty \frac{q^i(x)^m}{m!}
\sum_{n=0}^\infty \frac{s(x)^n}{n!}\\
&= \sum_{m=0}^\infty \frac{q^i(x)^m}{m!} 
+ \sum_{n=0}^\infty\frac{s(x)^n}{n!} - 1 +
\sum_{m=1}^\infty \frac{q^i(x)^m}{m!}
\sum_{n=1}^\infty \frac{s(x)^n}{n!} \\
&= \exp( q^i(x)) + \exp(s(x)) - 1 +
q^i(x)s(x)\sum_{m=0}^\infty \frac{q^i(x)^m}{(m+1)!}
\sum_{n=0}^\infty \frac{s(x)^n}{(n+1)!}
\end{align*}

For the final term on the RHS, we have

\begin{align*}
\left| q^i(x)s(x)\sum_{m=0}^\infty \frac{q^i(x)^m}{(m+1)!} \sum_{n=0}^\infty \frac{s(x)^n}{(n+1)!} \right| 
&\leq \left| q^i(x)s(x) \right| \sum_{m=0}^\infty \frac{|q^i(x)|^m}{m!} 
\sum_{n=0}^\infty \frac{|s(x)|^n}{n!} \\
&\leq \left| q^i(x)s(x) \right| e^{|q^i(x)|}e^{|s(x)|} \\
&\leq C \left| q^i(x)s(x) \right|
\end{align*}

where in the last line we used the fact that $q$ is bounded. Since all $n$ peaks are exponentially separated, so are $q^i(x)$ and $s(x)$, thus from Lemma \ref{expsep}, $\left| q^i(x)s(x) \right| = \mathcal{O}(e^{-\alpha 2 X_m})$. Putting this all together, we have

\begin{equation}
\exp(q_n) = \exp( q^i(x)) + (\exp(s(x)) - 1) + \mathcal{O}(e^{-\alpha X_m}) 
\end{equation}

Repeat this $n - 2$ more times to get the result.

\end{proof}
\end{lemma}

With this taken care of, we will use Lin's method to solve $(P_{S^\perp} A_0 |_{S^\perp})^{-1} P_{S^\perp} q^i_{xx}$.

% lemma

\begin{lemma}\label{Linsolve}
For all $i = 1, \dots, n$,
 
\begin{equation}
(P_{S^\perp} A_0|_{S^\perp})^{-1} P_{S^\perp} q^i_{xx} = -\frac{1}{2c}P_{S^\perp} q^i_c
+ \tilde{w}_i
\end{equation}

where $\tilde{w}_i$ is smooth and 

\begin{equation}
||\tilde{w}_i|| = \mathcal{O}(e^{-2 \alpha X_m})
\end{equation}

\begin{proof}
For convenience, let $k = -1/2c$, so that $A_0 k q_c$ = $q_{xx}$. Since everything involved is translation invariant, we also have $A_0 k q^i_c$ = $q^i_{xx}$. Applying the projection $P_{S^\perp}$ on both sides and using the fact that $A_0$ commutes with its own spectral projections, we have

\begin{align*}
P_{S^\perp} A_0 k q^i_c &= P_{S^\perp} q^-_{xx} \\
A_0 (P_{S^\perp} k q^i_c) &= P_{S^\perp} q^i_{xx}
\end{align*}

Using this as motivation, we will look for a solution $y$ to 

\begin{equation}\label{Lineq1}
(P_{S^\perp} A_0) y = P_{S^\perp} q^i_{xx}
\end{equation}

of the form

\begin{equation}\label{yansatz}
y = P_{S^\perp} k q^i_c + \tilde{w}
\end{equation}

We will use Lin's method to show that $\tilde{w}$ is exponentially small. Using Lemma \ref{expsep}, for $i = 1, \dots, n$ we can write $A_0$ as 

\begin{equation}\label{A0expansion}
A_0 = \partial_x^4 + c^2 \partial_x^2 + e^{q^i(x)} + \sum_{j \neq i} (e^{q^j(x)} - 1) + \tilde{h}(x)
\end{equation}

where $||\tilde{h}|| = \mathcal{O}(e^{-\alpha X}$. Substituting in our ansatz \eqref{yansatz} for $y$ in \eqref{Lineq1}, we obtain

\begin{align*}
A_0 (P_{S^\perp} k q^i_c + \tilde{w}) &= P_{S^\perp} q^i_{xx} \\
\left(\partial_x^4 + c^2 \partial_x^2 + e^{q^i(x)} + \sum_{j \neq i} (e^{q^j(x)} - 1) + \tilde{h}(x) \right)P_{S^\perp} k q^i_c + A_0 \tilde{w} &= P_{S^\perp} q^i_{xx} \\
(\partial_x^4 + c^2 \partial_x^2 + e^{q^i(x)})P_{S^\perp} k q^i_c + \left( \sum_{j \neq i} (e^{q^j(x)} - 1)+ \tilde{h}(x) \right) P_{S^\perp}k q^i_c + A_0 \tilde{w} &= P_{S^\perp} q^i_{xx} \\
A_0 \tilde{w} + \underbrace{\left( \sum_{j \neq i} (e^{q^j(x)} - 1)\right) P_{S^\perp} k q^i_c}_{\mathcal{O}(e^{-\alpha X})} 
+ \underbrace{h(x) P_{S^\perp}k q^i_c}_{\mathcal{O}(e^{-\alpha X})} &= 0 \\
A_0 \tilde{w} + h(x) &= 0
\end{align*}

where we have collected all of the $\mathcal{O}(e^{-\alpha X})$ stuff and called it $h(x)$. Note that $A_0$ is still the linearization about the $n$-pulse, but we will separate that out in the next step using our expansion \eqref{A0expansion} for $A_0$.\\

Next, following San98, we write this piecewise as a first order system. For convenience of notation, we use the notation $W$ when we write $\tilde{w}$ in the first order system. First, as in San98, we choose the $2n$ intervals 

\begin{align*}
(-\infty, 0], [0, X_1], [-X_1, 0], [0, X_2], \dots, [-X_{n-1}, 0], [0, \infty)
\end{align*}

Then, using the expansions \eqref{A0expansion} for $A_0$, we obtain the first order system

\begin{align*}
(W_i^\pm)'(x) = A(q(x)) W_i^\pm(x) + G_i(x) W_i^\pm(x) + B H(x)
\end{align*}

where

\begin{align*}
A(q(x)) &= \begin{pmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
-e^{q(x)} & 0 & -c^2 & 0 
\end{pmatrix},
B = \begin{pmatrix}
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
1 & 0 & 0 & 0 
\end{pmatrix},
G_i(x) &= \begin{pmatrix}
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
\sum_{j \neq i} (1 - e^{q(x - \rho_{ji})}) & 0 & 0 & 0 
\end{pmatrix}
\end{align*}

The key here is that the expansions $A_0$ in \eqref{A0expansion} depend on $i$, so we have a different expansion on each piece. Now we use Lin's method, i.e. look to solve the equations

\begin{align}
(W_i^\pm)'(x) &= A(q(x)) W_i^\pm(x) + G_i(x) W_i^\pm(x)+ B H(x) \nonumber \\
W_1^+(X) - W_2^-(-X) &= 0 \label{Wsystem} \\
W_i^-(0) - W_i^+(0) \in \C \Psi(0) \nonumber 
\end{align}

Where $\Psi(x)$ is the unique bounded solution to the adjoint equation $W' = -A(q(x))^* W$. These equations, solved via Lin's method, will give us a unique solution $W_i^\pm$, but in general these will have jumps $n$ at $x = 0$ (in the piecewise coordinate system).\\

From the bound on $h$, we have $||H|| = \mathcal{O}(e^{-\alpha X_m)}$. On the $i^\pm$ pieces, we note that by construction $q(x - \rho_{ji}) = \mathcal{O}(e^{-\alpha X_m})$ on the interval $[-X_{i-1}, X_i]$. Thus, using a Taylor series, we have on that interval

\begin{align*}
1 - e^{q(x - \rho_{ji})} &= q(x - \rho_{ji}) + \mathcal{O}(q(x - \rho_{ji}))^2 = \mathcal{O}(e^{-\alpha X_m})
\end{align*}

We conclude that $||G|| = \mathcal{O}(e^{-\alpha X_m)}$. With these bounds, we are set up to use Lin's method. Following San98 (or another suitable reference), we obtain unique functions $W_i^\pm$ which solve \eqref{Wsystem}. In adapting San98, we note that our system is similar to (3.7) in San98, except the $d_i$ is missing (we take it to be 1), there is no $\lambda$ (we take it to be 1), $D = 0$, and the $\lambda B_i^\pm$ term is absent entirely. The jumps are given by Lemma 3.6 in San98.

\begin{equation}
\xi_i = \int_{-\infty}^\infty \langle \Psi(x), H(x) \rangle dx + R_i
\end{equation}

where the $R_i$ are remainder terms for which $R = \mathcal{O}(e^{-3 \alpha X_m})$. A uniform bound on $W_i^\pm$ comes from Lin's method and is given by 

\begin{equation}\label{Wbound}
||W|| \leq C e^{-2 \alpha X_m}
\end{equation}

Taking this out of the first order system form, we have constructed via Lin's method a piecewise solution $y_i^\pm = P_{S^\perp} k q^i+_c + \tilde{w}_i^\pm$ to the problem $A_0 y = P_{S^\perp} q^i_{xx}$. What we want is solution $y \in \ran P_{S^\perp}$ to $P_{S^\perp} A_0 y = P_{S^\perp} q^i_{xx}$, i.e. we want the operator $P_{S^\perp} A_0$ instead $A_0$. But we can easily get this by applying the projection $P_{S^\perp}$ to both sides.

\begin{align*}
P_{S^\perp} A_0 y_i^\pm &= P_{S^\perp} P_{S^\perp} q^i_{xx} \\
P_{S^\perp} P_{S^\perp} A_0 y_i^\pm &= P_{S^\perp} q^i_{xx} \\
P_{S^\perp} A_0 (P_{S^\perp} y_i^\pm) &= P_{S^\perp} q^i_{xx}
\end{align*}

where we used the fact that $P_{S^\perp}$ is a spectral projection and thus commutes with $A_0$. Thus $P_{S^\perp} y_i^\pm \in \ran P_{S^\perp}$ and $P_{S^\perp} A_0 (P_{S^\perp} y_i^\pm) = P_{S^\perp} q^i_{xx}$. Since everything above is in $S^\perp$, this is equivalent to

\begin{equation}
(P_{S^\perp} A_0)|_{S^\perp} (P_{S^\perp} y_i^\pm) = P_{S^\perp} q^i_{xx}
\end{equation}

We showed in Lemma \ref{PA0inv} that $(P_{S^\perp} A_0)|_{S^\perp}$ is an invertible operator on $S^\perp$. Thus $y = (P_{S^\perp} A_0)|_{S^\perp} P_{S^\perp} q^i_{xx}$ is the unique solution to the problem $P_{S^\perp} A_0 = P_{S^\perp} q^i_{xx}$. Since $P_{S^\perp} y_i^\pm$ is another such solution, they must be identical. In particular, from Lemma \ref{PA0inv}, since $P_{S^\perp} q^i_{xx}$ is smooth, $y$ must be smooth as well. Since $P_{S^\perp} k q^+_c$ is smooth and $y = P_{S^\perp} k q^i_c + P_{S^\perp} \tilde{w}_i^\pm$, $P_{S^\perp} \tilde{w}_i^\pm$ must be smooth. In particular, this implies that the jumps $\xi_i$ are all 0. Since projections are bounded with norm of 1, using the uniform bound \eqref{Wbound} on $||W||$ we have 

\[
P_{S^\perp} \tilde{w}_i^\pm = \mathcal{O}(e^{-2 \alpha X_m})
\]

Thus the unique solution in $S^\perp$ to $P_{S^\perp} A_0 y = P_{S^\perp} q^i_{xx}$.

\begin{align*}
y &= P_{S^\perp} k q^i_c + \tilde{w}_i
\end{align*}

where

\begin{align*}
||\tilde{w}_i|| = \mathcal{O}e^{-2 \alpha X_m}
\end{align*}

\end{proof}
\end{lemma}

% lemma : K_2 is diagonally dominant

\begin{lemma}\label{K2exp}

The matrix $K_2$ is given, to leading order, by

\begin{equation}
K_2 = -2c \langle q_{xx}, q_c \rangle I + \mathcal{O}(e^{-(3 \alpha/2) X_m})
\end{equation}

\begin{proof}

\begin{align*}
(K_2)_{jk} 
&= 4 c^2 \langle P_{S^\perp} (s_j)_{x}, (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} (s_k)_{x} \rangle \\
&= 4 c^2 \langle (s_j)_{x}, P_{S^\perp}(P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} (s_k)_{x} \rangle \\
&= 4 c^2 \langle \sum_{i = 1}^{n} d_{ji} q^i_{xx} + (w_j)_x, P_{S^\perp} (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} \left( \sum_{i = 1}^{n} d_{ki} q^i_{xx} + (w_k)_x \right) \rangle \\
&= -\frac{1}{2 c} 4 c^2 
\langle \sum_{i = 1}^{n} d_{ji} q^i_{xx} + (w_j)_x, \sum_{i = 1}^{n} d_{ki} q^i_c + \mathcal{O}(e^{-2 \alpha X_m}) \rangle \\
\end{align*}

where we used Lemma \ref{Linsolve}, the fact that $(P_{S^\perp} A_0 P_{S^\perp})^{-1}$ is a bounded linear operator from Lemma \ref{PA0inv}, and the bound on the remainder terms $w_i$ from Lemma \ref{Linsolve}. Simplifying this, we have

\begin{align*}
(K_2)_{jk} &= -2 c \left( 
\langle \sum_{i = 1}^{n} d_{ji} q^i_{xx} + (w_j)_x, \sum_{i = 1}^{n} d_{ki} q^i_c + \mathcal{O}(e^{-2 \alpha X_m}) \rangle \right) \\
&= -2 c \left( \sum_{i = 1}^{n} d_{ji} d_{ki} \langle q^i_{xx}, q^i_c \rangle
+ \sum_{i\neq l} d_{ji} d_{kl} \langle q^i_{xx}, q^j_c \rangle
+ \langle (s_j)_x,  \mathcal{O}(e^{-2 \alpha X_m}) \rangle 
+ \sum_{i=1}^n \langle (w_j)_x, d_{ki} q^i_c \rangle \right)
\end{align*}

Since $q^i_{xx}$ and $q^j_c$ are exponentially separated for $i \neq j$ by at least $X_m$, the second term on the RHS is $\mathcal{O}(e^{-(3 \alpha/2) X_m})$ as in Lemma \ref{K1small}. The last two terms on the RHS are $\mathcal{O}(e^{-\alpha X_m})$ as in Lemma \ref{K1small}. The first term on the RHS is

\begin{align*}
\sum_{i = 1}^{n} d_{ji} d_{ki} \langle q^i_{xx}, q^i_c \rangle &= \sum_{i = 1}^{n} d_{ji} d_{ki} \langle q_{xx}, q_c \rangle \\
&= \langle q_{xx}, q_c \rangle \sum_{i = 1}^{n} d_{ji} d_{ki} \\
&= \langle q_{xx}, q_c \rangle \delta_{jk} + \mathcal{O}(e^{-(3 \alpha/2) X_m})
\end{align*}

where the sum is given by Lemma \ref{orthogonalD}. Thus we have

\begin{align*}
K_2 = -2c \langle q_{xx}, q_c \rangle + \mathcal{O}(e^{-(3\alpha /2)X_m})
\end{align*}

\end{proof}
\end{lemma}

We combine these lemmas in the following theorem

\begin{theorem}
For the linearization of the suspension bridge equation about a multipulse equilibrium solution, the Krein matrix is given by

\begin{equation}\label{Kreinapprox}
K_S(z) = ||q_x||^2 \text{diag}(\nu_1, \dots, \nu_n)
 + d''(c) I \overline{z}^2 + \mathcal{O}(e^{-(3 \alpha/2) X_m}|z| + |z|^3)
\end{equation}

\begin{proof}

From \eqref{Kreinform}, Lemma \ref{K1small} and Lemma \ref{K2exp}, we have

\begin{align*}
K_S(z) &= ||q_x||^2 \text{diag}(\nu_1, \dots, \nu_n) + \overline{z} \mathcal{O} (e^{-(3 \alpha/2) X_m})
 - \overline{z}^2 ( (||q_x||^2 - 2c \langle q_{xx}, q_c \rangle)I + \mathcal{O}(e^{-(3 \alpha/2) X_m}) + \mathcal{O}(|z|^3) \\
 &= ||q_x||^2 \text{diag}(\nu_1, \dots, \nu_n) 
 - \overline{z}^2 ( ||q_x||^2 - 2c \langle q_{xx}, q_c \rangle) + \mathcal{O}(e^{-(3 \alpha/2) X_m}|z| + |z|^3)
\end{align*}

From Lemma \ref{Krein1pulse}, we have

\begin{align*}
-\left(||q_x||^2 - 2c \langle q_{xx}, q_c \rangle\right) 
&= -\frac{\partial}{\partial c} \left( c||q_x||^2 \right) \\
&= d''(c)
\end{align*}

where $d''(c)$ is defined in \eqref{dcc}. Combining these gives us \eqref{Kreinapprox}. 

\end{proof}
\end{theorem}

We can use this to compute the eigenvalues of the linearization of the suspension bridge equation about a multipulse equilibrium solution. To do that, we need to solve $\det K_S(z) = 0$ for $z$. Since we showed above that the Krein matrix is a perturbation of a diagonal matrix, for convenience we write

\[
K_S(z) = \text{diag}(k_1(z), \dots, k_n(z)) + \mathcal{O}(e^{-\alpha X_m}|z| + |z|^3)
\]

From (say) Ipsen and Rehman (2008), when we take the determinant, we should get something resembling

\[
\det K_S(z) = \prod_{i = 1}^n (k_i(z) + \tilde{r}_i(z))
\]

where $\tilde{r}_i(z) = \mathcal{O}(e^{-(3 \alpha/2) X_m}|z| + |z|^3)$ and is real since $K_S(z)$ is Hermitian. Thus we can find $z$ by solving

\[
||q_x||^2 \nu_i
 - \overline{z}^2 \frac{\partial}{\partial c} \left( c||q_x||^2 \right) + \tilde{r}_i(z) = 0
\]

Divide this by $-\frac{\partial}{\partial c} \left( c||q_x||^2 \right)$ and let

\[
b_i = \frac{||q_x||^2}{\frac{\partial}{\partial c} \left( c||q_x||^2 \right)} \nu_i
\]

Then (redefining $\tilde{r}_i(z)$), this becomes

\[
\overline{z}^2 - b_i + \tilde{r}_i(z) = 0
\]

Since we are taking $z$ real, this becomes

\[
z^2 - b_i + \tilde{r}_i(z) = 0
\]

Since $\tilde{r}_i(z) = \mathcal{O}(e^{-(3 \alpha/2) X_m}|z| + |z|^3)$, we can write this as

\begin{equation}\label{fz}
\tilde{f}_i(z) = (1 + \epsilon_2) z^2 + \epsilon_1 z - b_i + \tilde{r}_i(z) = 0
\end{equation}

where $\epsilon_1, \epsilon_2  = \mathcal{O}(e^{-(3 \alpha/2) X_m})$ and we have again redefined $\tilde{r}_i(z)$ so that $\tilde{r}_i(z) = \mathcal{O}(|z|^3)$. Dividing by $(1 + \epsilon_2) = \mathcal{O}(1)$, this becomes

\begin{equation}\label{fz}
f_i(z) = z^2 + \epsilon_1 z - \tilde{b}_i + \tilde{r}_i(z) = 0
\end{equation}

where we have redefined $\epsilon_1$ (but it is still $\mathcal{O}(e^{-(3 \alpha/2) X_m})$) and $\tilde{r}_i(z)$ appropriately, and $\tilde{b}_i = b_i / (1 + \epsilon_2)$. Let

\begin{equation}\label{gz}
g_i(z) = z^2 + \epsilon_1 z - \tilde{b}_i
\end{equation}

Then $f_i(z) = g_i(z) + \tilde{r}_i(z)$. Since we are taking $z$ small (the expansion used in Kap2018 is only valid for small $z$), the term $\tilde{r}_i(z)$ will be very small, so we should have approximately $f(z) = g(z)$. With this motivation, let's first look at the quadratic part $g(z)$, i.e. solve $g_i(z) = z^2 + \epsilon_1 z - \tilde{b}_i = 0$. The idea here is to make sure we have sufficient control over the remainder terms so that they do not interfere. This is easily solvable for $z$ to get

\begin{align*}
z &= \frac{1}{2}\left(-\epsilon_1 \pm \sqrt{\epsilon_1^2 + 4 \tilde{b}_i}\right) \\
&=  \pm \sqrt{\tilde{b}_i} \left( 1 + \frac{\epsilon_1^2}{4 \tilde{b}_i} \right)^{1/2} - \frac{1}{2} \epsilon_1
\end{align*}

Recall that $\epsilon_1  = \mathcal{O}(e^{-(3 \alpha/2) X_m})$. From Lin's method, the eigenvalues $\nu_i = \mathcal{O}(e^{-2 X_m})$, thus the $\tilde{b}_i$ are of the same order. This implies

\begin{equation}
\frac{\epsilon_1^2}{4 \tilde{b}_i} = \mathcal{O}(e^{-\alpha X_m})
\end{equation}

For sufficiently large $X_m$, $|\epsilon_1^2/4 \tilde{b}_i| < 1$. Thus we can use a Taylor expansion to get

\begin{align*}
z &= \pm \sqrt{\tilde{b}_i} \left( 1 + \frac{1}{2} \frac{\epsilon_1^2}{4 \tilde{b}_i} + \mathcal{O}\left( \frac{\epsilon_1^2}{4 \tilde{b}_i}\right)^2 \right) - \frac{1}{2} \epsilon_1 \\
&= \pm \sqrt{\tilde{b}_i} \left( 1 + \mathcal{O}(e^{-\alpha X_m}) \right) - \frac{1}{2} \epsilon_1 \\
&= \pm \sqrt{\tilde{b}_i} + \mathcal{O}(e^{-3 \alpha X_m}) - \frac{1}{2} \epsilon_1 \\
&= \pm \sqrt{\tilde{b}_i} + \mathcal{O}(e^{-(3 \alpha/2) X_m})
\end{align*}

since $\epsilon_1  = \mathcal{O}(e^{-(3 \alpha/2) X_m})$. Let $z_i^\pm$ be two roots of \eqref{gz}. Then $z_i^\pm = \mathcal{O}(e^{-\alpha X_m})$ and $\tilde{r}(z_i^\pm) = \mathcal{O}(e^{-3 \alpha X_m})$.\\

Now that we have taken care of this, we look to solve \eqref{fz}, i.e. $f_i(z) = 0$. Since we have estimates on the remainder term and know the order of $\tilde{b}_i$, it is clear that for $\tilde{b}_i$, \eqref{fz} has two roots near $z_i^\pm$, the roots of \eqref{gz}. \\

We can, of course, show this. There are many ways to do it, including using Newton's method with initial guess of $z_i^\pm$ and showing that it converges to a unique root using the Newton-Kantorovich theorem. In this case, we can show it directly with ordinary calculus, so we will just do that. We proceed in the following steps. We will look for the root of $f_i(z)$ near $z_i^+$. Recall that $z_i^+ > 0$. The other one is similar.

\begin{enumerate}

\item Evaluate the first two derivatives of $f_i$. 
\begin{align*}
f_i'(z) &= 2z + \epsilon_1 + \mathcal{O}(z^2) \\
f_i''(z) &= 2 + \mathcal{O}(z)
\end{align*}

\item Since $\epsilon_1 = \mathcal{O}(e^{-(3 \alpha/2) X_m}) = \mathcal{O}((z_i^+)^{3/2})$ and the remainder term in $f_i'(z_i^+)$ is $\mathcal{O}((z_i^+)^2)$, choose $z_i^+$ sufficiently small so that $f_i'(z_i^+) \geq z_i^+$.

\item Since $z_i^+ = \mathcal{O}(e^{-\alpha X_m})$, if necessary, increase $X_m$ so that $1 \leq f_i''(z) \leq 3$ on the interval $I = [3 z_i^+ /4, 5 z_i^+/4]$.

\item Since we have a bound on the second derivative, it follows that $f_i'(z) > z_i^+/4$ for $z \in I$. Thus $f$ is strictly increasing, and therefore invertible, on $I$. 

\item Finally, we need to show that $f(I)$ contains 0. To do this, we evaluate $f(z)$ at the endpoints of $I$.

\begin{align*}
f_i(3 z_i^+/4) &= f_i(z_i^+ - z_i^+/4) \\
&= ((z_i^+)^2 + \epsilon_1 z_i^+ - \tilde{b}_i) - (7/16)(z_i^+)^2 
+ \epsilon_1 z_i^+/4 + \mathcal{O}((z_i^+)^3) \\
&= - (7/16)(z_i^+)^2 + \mathcal{O}((z_i^+)^{5/2}) + \mathcal{O}((z_i^+)^3)
\end{align*}

Since the last two terms on the RHS are higher order than the first term, we can decrease $z_i^+$, if necessary, so that $f_i(3 z_i^+/4) < 0$. Similarly, we can get $f_i(5 z_i^+/4) > 0$. Since $f_i$ is continuous, by the IFT there exists $z^* \in I$ with $f_i(z^*) = 0$. This is the root we are looking for.

\item We can get a bound on $z^*$ by using the derivative of $(f_i)^{-1}$ on $I$. Since $f_i'(z) > z_i^+/4$ for $z \in I$, $|((f_i)^{-1})'(y)| < 4 / z_i^+$ on $f(I)$. In particular, this implies

\begin{align*}
|z^* - z_i^+| &\leq \frac{4}{z_i^+}|0 - f(z_i^+)| \\
&= \frac{4}{z_i^+}\mathcal{O}((z_i^+)^3) \\
&= \mathcal{O}((z_i^+)^2)
\end{align*}

\item Now that we have found the root $z^*$ that we want, we can put this all together to determine how far it is from $\sqrt{b_i}$. By the triangle inequality,

\begin{align*}
|z^* - \sqrt{b_i}| &\leq |z^* - z_i^+| + |z_i^+ - \sqrt{\tilde{b}_i}| + |\sqrt{b_i} - \sqrt{\tilde{b}_i} | \\
&= \mathcal{O}(e^{-(3 \alpha/2) X_m})
\end{align*}

\end{enumerate}

The same hold for the root $z_i^-$. We conclude that for an $n-pulse$, the Krein matrix has $n$ pairs of roots, which are given by

\begin{align*}
z_i &= \pm \sqrt{b_i} + \mathcal{O}(e^{-(3 \alpha/2) X_m})
\end{align*}

where 

\[
b_i = \frac{||q_x||^2}{\frac{\partial}{\partial c} \left( c||q_x||^2 \right)} \nu_i
\]

Since the denominator is negative by Hypothesis 1 and the numerator is always positive, $\sqrt{b_i}$ is real if $\nu_i < 0$. Recalling that we took $\lambda = i z$ in the initial setup, where $z$ is real, we conclude the following.\\

If $\nu_i < 0$ for $i = 2, \dots, n$ (recall that we always have $\nu_1 = 0$), the eigenvalues of the linearization about the $n-$pulse are:

\begin{enumerate}
	\item A double eigenvalue at 0
	\item $n-1$ pairs of purely imaginary eigenvalues $\pm \lambda_i$. To leading order, we have $\lambda_i = \sqrt{b_i}$. By Hamiltonian symmetry, we can conclude that these eigenvalues must be purely imaginary, since if the error term bumped us off the imaginary axis, we would have to have a quartet, and we don't have enough eigenvalues to do that.
\end{enumerate}

\subsection{Lin's Method (part 2)}

We will try the same thing as was done in San98. The main difference here is that we have a quadratic eigenvalue problem. The linearization about the rest state is hyperbolic, so that should present no problem. And we know that we have an eigenvalue at 0, so we should be able to perturb that eigenfunction to construct the interaction eigenfunctions. Numerical plots of the interaction eigenfunctions suggest that should be possible, and there is no reason to suspect this won't work. This is a sketch of what we need to do; if it looks promising, details can be filled it.

\begin{enumerate}

\item Let $q(x)$ be a single pulse equilibrium solution to \eqref{eqODE}. Suppose for now that it exists and decays exponentilly with rate $\alpha$. Suppose we have also constructed a double pulse $q_2(x)$, and it has the same decay properties. We have not proven we can do this, but we can likely follow SanStrut. The double pulse is a perturbation of the single pulse, and can be written piecewise on the appropriate intervals as

\[
q_2(x) = q(x) + r_i^\pm(x)
\]

where the remainder term $r_i^\pm$ is small and hopefully we can get estimates on it.

\item Write the EVP as 1st order system following San98. In other words, split off the $\lambda$ terms. Let $V = (v_1, v_2, v_3, v_4) = (v, v_x, v_{xx}, v_{xxx})$. Then we have

\begin{equation}\label{splitevp}
V' = A(q_2)V + \lambda B_1 V + \lambda^2 B_2 V
\end{equation}

where

\begin{equation}
A(q_2) = \begin{pmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
-e^{q_2} & 0 & -c^2 & 0 
\end{pmatrix}
\end{equation}

\begin{align*}
B_1 = \begin{pmatrix}
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 2 c & 0 & 0 
\end{pmatrix} &&
B_2 = \begin{pmatrix}
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
-1 & 0 & 0 & 0 
\end{pmatrix}
\end{align*}

\item The variational and adjoint variational equations we will use are

\begin{align}
V' &= A(q)V \\
W' &= -A(q)^* W
\end{align}

For now, we assume that $Q'$ is the unique bounded solution to the variational equation, and that therefore the adjoint variational equation has a unique bounded solution $\Psi$. Let $Z = \C \Psi(0)$.

\item To exploit all of this, we will write our eigenfunction piecewise as

\[
V_i^\pm = d_i Q_2' + W_i^\pm = d_i(Q' + (R_i^\pm)') + W_i^\pm
\]


Now we substitute this into \eqref{splitevp}. Using the fact that $(Q_2')' = A(q_2)Q_2'$, this becomes

\[
(W_i^\pm)' =A(q_2)W_i^\pm + \lambda B_1 W_i^\pm + \lambda^2 B_2 W_i^\pm 
+ \lambda B_1 d_i Q_2' + \lambda^2 B_2 d_i Q_2' 
\]

Let 

\begin{align*}
\tilde{H}_i^\pm &= Q_2' = Q' + (R_i^\pm)' \\
H &= Q'
\end{align*}

Then this becomes

\[
(W_i^\pm)' =A(q_2)W_i^\pm + \lambda B_1 W_i^\pm + \lambda^2 B_2 W_i^\pm 
+ \lambda B_1 d_i \tilde{H}_i^\pm + \lambda^2 B_2 d_i \tilde{H}_i^\pm 
\]

Finally, let $G_i^\pm = A(q_2) - A(q) = A(q + r_i^\pm) - A(q)$. Then this becomes

\[
(W_i^\pm)' =A(q)W_i^\pm + (G_i^\pm + \lambda B_1 + \lambda^2 B_2) W_i^\pm 
+ d_i( \lambda B_1 + \lambda^2 B_2 ) \tilde{H}_i^\pm 
\]

We don't have any bounds on anything yet, but we can show easily that

\begin{align*}
G_i^\pm(x) &= \mathcal{O}(e^{q(x) + r_i^\pm(x)} - e^{q(x)} )\\
&= \mathcal{O}(e^{q(x)}(e^{r_i^\pm(x)} - 1)) \\
&= \mathcal{O}(e^{r_i^\pm(x)} - 1)
\end{align*}

since $q(x)$ is bounded and is given. The idea is that $r_i^\pm$ is small, thus $G_i^\pm$ is small as well.\\

Thus we have the following system we need to solve.

\begin{enumerate}[(i)]
\item $(W_i^\pm)' A(q)W_i^\pm + (G_i^\pm + \lambda B_1 + \lambda^2 B_2) W_i^\pm + d_i( \lambda B_1 + \lambda^2 B_2 ) \tilde{H}_i^\pm$
\item $W_i^\pm(0) \in Z \oplus Y^+ \oplus Y^-$
\item $W_i^+(0) - W_i^-(0) \in Z$
\item $W_i^+(X_i) - W_{i+1}^-(-X_i) = D_i d$
\end{enumerate}

where

\begin{equation}
D_i d = (Q'(-X_i) + (R_{i+1}^-)(-X_i))d_{i+1}
- (Q'(X_i) + (R_i^+(X_i))d_i
\end{equation}

Compare this to (3.7) and (3.8) in San98. This is essentially the same except for the quadratic terms in $\lambda$.\\

\item Let $\Phi(y, x)$ be the evolution operator of the variational equation. Since the linearization about the zero solution is hyperbolic, the adjoint variational equation has an exponential dichotomy on $\R^\pm$, and we split the evolution operator up into pieces $\Phi_\pm^{s/u}$ on the appropriate pieces of the dichotomy. We have the usual estimates for these.\\

Next, we write this in integrated form. This will be the same as in San98, except we have the presence of the $\lambda^2$ terms.

\begin{align*}
W_i^-(x) = \Phi_-^s(&x, -X_{i-1})a_{i-1}^- + \Phi_-^u(x, 0)b_i^- \\
&+ \int_0^x \Phi_-^u(x, y)[(G_i^-(y) + \lambda B_1 + \lambda^2 B_2) W_i^-(y) + d_i (\lambda B_1 + \lambda^2 B_2) \tilde{H}_i^-(y) ] dy \\
&+ \int_{-X_{i-1}}^x \Phi_-^s(x, y)[(G_i^-(y) + \lambda B_1 + \lambda^2 B_2) W_i^-(y) + d_i (\lambda B_1 + \lambda^2 B_2) \tilde{H}_i^-(y) ] dy \\
W_i^+(x) = \Phi_+^u(&x, X_i)a_i^+ + \Phi_+^s(x, 0)b_i^+ \\
&+ \int_0^x \Phi_+^s(x, y)[(G_i^+(y) + \lambda B_1 + \lambda^2 B_2) W_i^+(y) + d_i (\lambda B_1 + \lambda^2 B_2) \tilde{H}_i^+(y) ] dy \\
&+ \int_{X_i}^x \Phi_+^u(x, y) [(G_i^+(y) + \lambda B_1 + \lambda^2 B_2) W_i^+(y) + d_i (\lambda B_1 + \lambda^2 B_2) \tilde{H}_i^+(y) ] dy
\end{align*}

Again, these are similar to (3.14) in San98, except for the presence of the $\lambda^2$ terms.\\

\item Before we keep going, by what we have learned so far, this will produce two Melkinov integrals, one for each of the $B_i$. These should be, based on what we have from before

\begin{align*}
M_1 &= \int_{-\infty}^\infty \langle \Psi(x), B_1 H(x) \rangle dx \\
M_2 &= \int_{-\infty}^\infty \langle \Psi(x), B_2 H(x) \rangle dx
\end{align*}

Since $H(x) = Q'(x)$, we know what that is. We need to figure out what $\Psi$ is. The variational problem, if we write it as a 4th order ODE, is

\[
(\partial_x^4 + c^2 \partial_x^2 + e^{q})]v = 0
\]

This has unique bounded solution $q'(x)$. Since only even derivatives are involved, this is self-adjoint, thus the adjoint variational problem, when written in this form, also has a unique solution $q'(x)$. We know from before that this implies that for the system-version $\Psi' = -A(q)^* \Psi$, the fourth component of $\Psi$ is $q'(x)$. From this we can determine the rest. Written out, the matrix equation $W' = -A(q)^* W$ is

\begin{equation}
\begin{pmatrix}w_1 \\ w_2 \\ w_3 \\ w_4 \end{pmatrix}' =
\begin{pmatrix}
0 & 0 & 0 & e^q \\
-1 & 0 & 0 & 0 \\
0 & -1 & 0 & c^2 \\
0 & 0 & -1 & 0 
\end{pmatrix}
\begin{pmatrix}w_1 \\ w_2 \\ w_3 \\ w_4 \end{pmatrix}
\end{equation}

This is equivalent to the system of four equations

\begin{align*}
w_1' &= e^q w_4 \\
w_2' &= -w_1 \\
w_3' &= -w_2 + c^2 w_4 \\
w_4' &= -w_3
\end{align*}

Rearranging to solve for $(w_1, w_2, w_3)$ in terms of $w_4$, we have

\begin{align*}
w_3 &= -w_4' \\
w_2 &= w_4'' + c^2 w_4 \\
w_1 &= -w_4''' - c^2 w_4'
\end{align*}

The final equation becomes $-w_4'''' - c^2 w_4'' = e^q w^4$, which is satisfied, as expected, by $w_4 = q'$. Thus we have

\begin{align*}
\Psi = \begin{pmatrix}
-q'''' - c^2 q''\\
q''' + c^2 q'\\
-q''\\
q'
\end{pmatrix}
\end{align*}

Using this, we can compute the Melnikov integrals.

\begin{align*}
M_1 &= \int_{-\infty}^\infty \langle \Psi(x), B_1 \tilde{H}(x) \rangle dx \\
&= \int_{-\infty}^\infty c q'(x) q''(x) dx \\
&= 0 \\
M_2 &= \int_{-\infty}^\infty \langle \Psi(x), B_2 \tilde{H}(x) \rangle dx \\
&= \int_{-\infty}^\infty  (q'(x))^2 dx \\
&\neq 0
\end{align*}

Since one of these is nonzero, we should be good. In fact, we need $M_1$ to be 0 to get the eigenvalues we expect.

\item If we work through all of this naively and try to guess what the result is, we do not get the right result. The naive guess for the double pulse is that, after all is said and done, we get $\lambda^2 = \sqrt{-2a / M}$, i.e. these eigenvalues are the square roots of the eigenvalues of $A_0$. From our numerics and Krein matrix calculations, we have shown this is not the case. Thus either we set the problem up incorrectly, or there are other terms contributing to the leading order estimate which we have not accounted for in the naive guess.


\end{enumerate}

\subsection{References}

Y.Chen and P.J.McKenna. Traveling Waves in a Nonlinearly Suspended Beam: Theoretical Results and Numerical Observations. Journal of Differential Equations Volume 136, Issue 2, 20 May 1997, Pages 325-355.\\

M.Grillakis, J.Shatah, and W.Strauss. Stability theory of solitary waves in the presence of symmetry, I. Journal of Functional Analysis Volume 74, Issue 1, 1987, Pages 160-197.\\

Ilse.Ipsen and R.Rehman. Perturbation Bounds for Determinants and Characteristic Polynomials. SIAM J. Matrix Anal. \& Appl., 30(2), 762–776.\\

Will add other references when I do the actual paper/writeup/thesis/whatever.


\end{document}