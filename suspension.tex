\documentclass[12pt]{article}
\usepackage[pdfborder={0 0 0.5 [3 2]}, plainpages=false]{hyperref}%
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}%
\usepackage[shortalphabetic]{amsrefs}%
\usepackage{amsmath}
\usepackage{enumerate}
% \usepackage{enumitem}
\usepackage{amssymb}                
\usepackage{amsmath}                
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage[table,xcdraw]{xcolor}
\usepackage{tikz}
\usepackage{float}
\usepackage{booktabs}
\usepackage{svg}
\usepackage{mathtools}
\usepackage{cool}
\usepackage{url}
\usepackage{graphicx,epsfig}
\usepackage{makecell}
\usepackage{array}

\def\noi{\noindent}
\def\T{{\mathbb T}}
\def\R{{\mathbb R}}
\def\N{{\mathbb N}}
\def\C{{\mathbb C}}
\def\Z{{\mathbb Z}}
\def\P{{\mathbb P}}
\def\E{{\mathbb E}}
\def\Q{\mathbb{Q}}
\def\ind{{\mathbb I}}

\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\ran}{range}

\graphicspath{ {suspension/} }

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{hypothesis}{Hypothesis}

\newtheorem{notation}{Notation}

\begin{document}

\section{Suspension Bridge Equation}

\subsection{Background}

We look at the suspended beam equation from Chen97. The idea is to look at an equation which has a second order time derivative which also possesses multimodal solutions. This is suggested on p. 2095 of SanStrut and p. 351 of Chen97.\\

The equation we will look at is

\begin{equation}\label{susp}
u_{tt} + u_{xxxx} + e^{u - 1} - 1 = 0
\end{equation}

Note that the equation used for most of the analysis in Chen97 is 

\begin{equation}\label{susp2}
u_{tt} + u_{xxxx} + u^+ - 1 = 0
\end{equation}

which has a cusp at $u = 0$. On p. 351 of Chen97, Chen and McKenna note that they have proved existence of solutions only to \eqref{susp2}, not to \eqref{susp}, but there is strong numerical evidence that similar solutions exist for \eqref{susp}. Fortunately, this has been addressed in the intervening years (and we will get to it in a bit). \\

We are interested in traveling wave solutions, so we take a modified form of the usual traveling wave ansatz. Letting $\xi = x - ct$, we take

\begin{equation}
u(x, t) = z(\xi, t) + 1 = z(x - ct, t) + 1
\end{equation}

Substituting this into \eqref{susp} gives us

\begin{equation*}
z_{tt} - c z_{\xi t} + c^2 z_{\xi \xi} + z_{\xi \xi \xi \xi} + e^{z} - 1 = 0
\end{equation*}

The advantage of adding 1 in our ansatz is that the background for $z$ is at $z = 0$, whereas for $u$ it is at $u = 1$. Since it's annoying to use $z$ and $\xi$, we rewrite this using $u$ and $x$ to get our traveling frame suspended beam equation

\begin{equation}\label{susp3}
u_{tt} - 2 c u_{x t} + u_{xxxx} + c^2 u_{xx} + e^{u} - 1 = 0
\end{equation}

For an equilibrium solution (such as a homoclinic orbit), all time derivatives are zero, so any equilibrium solution must satisfy the ODE

\begin{equation}\label{eqODE}
u_{xxxx} + c^2 u_{xx} + e^{u} - 1 = 0
\end{equation}

which is (46) on p. 342 of Chen97, with $\tilde{f}(u) = e^u - 1$.\\

Note that $u = 0$ is a solution to this. A homoclinic orbit, if it exists, connects this constant equilibrium state to itself. If we linearize about this trivial solution, we get the ODE

\begin{equation}
v_{xxxx} + c^2 v_{xx} + 1 = 0
\end{equation}

which has eigenvalues

\begin{equation}
\nu = \pm \sqrt{\frac{-c^2 \pm \sqrt{c^4 - 4}}{2} }
\end{equation}

Note that $\sqrt{c^4 - 4}$ is always less that $c^2$ is magnitude. If $|c| < \sqrt{2}$, we have a complex conjugate quartet $\nu = \pm \alpha \pm \beta i$. When $c^2 = \sqrt{2}$, these eigenvalues collide on the imaginary axis at $\pm i$, then for $c^2 > 2$ we have a quartet of purely imaginary eigenvalues. Thus a bifurcation occurs at $c^2 = 2$.\\

The existence of a homoclinic orbit equilibrium solution was shown by van den Berg et al (2018) using a computer-assisted proof method. They proved (Theorem 1) that a symmetric homoclinic orbit exists for \eqref{eqODE} for all parameter values $c^2 \in [0.5, 1.9]$. If we assume this homoclinic orbit is transversely constructed (or find that result in this paper), we can apply the results of SanStrut to get the existence of multi-pulse solutions.

\subsection{Eigenvalue Problem}

For linear stability analysis, we look at the PDE eigenvalue problem. To do this, assume we have found an equilibrium solution $u^*(x)$ of \eqref{eqODE}. Then we linearize around this by taking the standard linearization ansatz

\begin{equation}
u(x,t) = u_*(x) + \epsilon e^{\lambda t} v(x)
\end{equation}

Plugging this into \eqref{susp3} and keeping only terms up to order $\epsilon$, we obtain the quadratic eigenvalue problem

\begin{equation}\label{evp}
[\lambda^2 - 2 c \partial_x \lambda + (\partial_x^4 + c^2 \partial_x^2 + e^{u_*})]v = 0
\end{equation}

which is in the general form of a quadratic eigenvalue problem $(A_2 \lambda^2 + A_1 \lambda + A_0)v = 0$, where in this case $A_2 = I$. We can also write this as

\begin{equation}\label{evp2}
[(\lambda - c \partial_x)^2 + \partial_x^4 + e^{u_*}]v = 0
\end{equation}

Before we do that, we look at symmetry. For a given equilibrium solution $u^*$, suppose we have a solution $v(x)$ to \eqref{evp} with corresponding eigenvalue $\lambda$. By taking complex conjugates, it follows that $\overline{v}(x)$ is a solution with corresponding eigenvalue $\overline{\lambda}$. As long as $u^*(x)$ is an even function (which it typically is), if we replace $v(x)$ by $v(-x)$ and $\lambda$ by $-\lambda$, we also have a solution (this follows from the odd/even properties of the various operators involved). Thus we have the typical 4-fold symmetry of eigenvalues.\\ 

To continue, let's write the eigenvalue problem as a first order system. Letting $V = (v_1, v_2, v_3, v_4) = (v, v_x, v_{xx}, v_{xxx})$, we have $V' = A(\lambda; u^*)V$, where

\begin{equation}
A(\lambda; u_*) = \begin{pmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
-(\lambda^2 + e^{u_*}) & 2 c \lambda & -c^2 & 0 
\end{pmatrix}
\end{equation}

Assume we there exists an exponentially localized solution $u_0(x)$. Then, we can find the essential spectrum by looking at the asymptotic matrix

\begin{equation*}
A_\infty(\lambda) = \begin{pmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
-(\lambda^2 + 1) & 2 c \lambda & -c^2 & 0 
\end{pmatrix}
\end{equation*}

The characteristic polynomial of this is 

\begin{equation}\label{charpoly}
p(\nu) = \nu^4 + c^2 \nu^2 - 2 c \lambda \nu + (\lambda^2 + 1) 
\end{equation} 

For the essential spectrum, we want the values of $\lambda$ for which we have a purely imaginary eigenvalue, i.e. $\nu = i r$ for $r \in \R$. Plugging this in and collecting terms in $\lambda$, we get the equation

\begin{equation}\label{fredholmborder}
\lambda^2 - 2 i c r \lambda - c^2 r^2  + r^4 + 1 = 0 
\end{equation} 

Solving for $\lambda$, we have

\begin{align*}
\lambda_\pm(r) &= i c r \pm \sqrt{-1 - r^4} \\
&= \left( c r \pm \sqrt{1 + r^4} \right) i
\end{align*}

since $-1 - r^4 \leq -1 < 0$. Thus the essential spectrum is purely imaginary. Since $\lambda(r) \rightarrow \pm \infty$ as $r \rightarrow \pm \infty$, the essential spectrum extends out to infinity along the imaginary axis. The question is whether the essential spectrum is the entire imaginary axis or whether there is a gap around the origin. Matlab suggests a gap at the origin, so let's look at that. Here is a plot of $\lambda_\pm(r)$ for $c = 1$ which shows that gap.

\begin{figure}[H]
\centering
\includegraphics[width=10cm]{essspec1.eps}
\label{fig:essspec1}
\caption{Imaginary part of the purely imaginary essential spectrum for $c = 1$.}
\end{figure}

The equation for $\lambda(r)$ is of the form $\lambda_\pm(r) = k r \pm f(r^2)$. Since

\begin{align*}
\lambda_-(-r) = -kr - f((-r)^2) = -(kr + f(r^2)) = -\lambda_+(r)
\end{align*} 

and we take $r \in \R$, it suffices to look at $\lambda_+(r)$. Using Mathematica, we find

\begin{align*}
\lambda_+'(r) = c+\frac{2 r^3}{\sqrt{r^4+1}}
\end{align*}

Solving $\lambda_+'(r) = 0$ involves solving the 6th order polynomial

\[
4 r^6 - c^2 r^4 + c^2 = 0
\]

Mathematica will actually do it (!), but it is an epic mess. So we will look at it numerically using Matlab. The next plot shows the minimum value of $\lambda_+(r)$ as a function of $c$. Note that for $c = 0$ this minimum is clearly $0$, and for $c = \sqrt{2}$ it is not hard to show that this minimum is actually 0 (hence no essential spectrum gap.) $c = \sqrt{2}$ is the bifurcation point mentioned above, so this is consistent.

\begin{figure}[H]
\centering
\includegraphics[width=10cm]{minlambdaplus.eps}
\label{fig:essspec1}
\caption{Imaginary part of the purely imaginary essential spectrum for $c = 1$.}
\end{figure}

Note that this minimum decreases strictly as $c$ increases, so the essential spectrum gap narrows (to 0) as $c$ increases from 0 to $\sqrt{2}$.\\

We could probably do better, i.e. approximate or get an exact formula (likely a nasty one) for this curve, but this is good enough for now.\\

When we compute the spectrum numerically with Matlab (details on how we do this are below), we see this essential spectrum gap around the origin. For $c = 1.2$, Matlab predicts that the essential spectrum gap is $[-0.2061, 0.2061]$. Here are plots of the computed spectrum from Matlab of the linearization about the single pulse solution. These show this essential spectrum gap. The eigenvalues at 0 are from translation invariance, which are discussed below.

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{essspecboundsF256.eps}
\includegraphics[width=8cm]{essspecboundsFD256.eps}
\caption{Essential spectrum gap for $c = 1.2$. Fourier spectral methods, $N = 256$ (left), finite difference methods, $N = 256$ (right)}
\end{figure}

Now that have looked at the essential spectrum, we can move on the point spectrum. There are only a few things we can say at this point.\\

If $\lambda = 0$, the eigenvalue problem reduces to

\begin{equation*}
A_0 v = (\partial_x^4 + c^2 \partial_x^2 + e^{u_*})v = 0
\end{equation*}

If we take $v = \partial_x u_*$, then 
\begin{align*}
A_0 \partial_x u_* &= 
(\partial_x^4 + c^2 \partial_x^2 + e^{u_*})\partial_x u_* \\
&= \partial_x[(u_*)_{xxxx} + c^2 (u_*)_{xx} + e^{u_*}] \\
&= \partial_x(1) \\
&= 0
\end{align*}

where we use the fact that $u_*$ is an equilibrium solution, thus must satisfy \eqref{eqODE}. Thus 0 is an eigenvalue of \eqref{evp} with corresponding eigenfunction $\partial_x u_*$.\\

When we solve for the eigenvalues numerically (again, details below), we get eigenvalues (close to) 0, but both eigenfunctions look like the derivative of $u_*$. In this case, I don't think we can have a generalized kernel function since the operator $A_0$ is self-adjoint, so by the Fredholm alternative we cannot have a function $v$ such that $A_0 v = \partial_x u_*$. (Otherwise $\partial_x u_*$ would be perpendicular to the kernel of $A_0^* = A_0$, which is impossible.)\\

Best guess is that we get two eigenvalues at 0 from the quadratic solver because of how it works.\\

We also want to look at the derivative with respect to the speed $c$, since it often shows up in these types of problems. Taking the derivative of \eqref{eqODE} with respect to $c$, we get

\begin{align*}
0 &= u_{xxxxc} + 2 c u_{xx} + c^2 u_{xxc} + e^{u} u_c \\
&= 2 c u_{xx} + (\partial_x^4 + c^2 \partial_x^2 + e^{u})u_c
\end{align*}

At an equilibrium solution $u_*$, we rearrange this to get

\begin{align*}
(\partial_x^4 + c^2 \partial_x^2 + e^{u^*})u^*_c &= -2 c u^*_{xx}
\end{align*}

In other words, we have

\begin{equation}\label{uc}
A_0 \partial_c u^* = -2 c u^*_{xx}
\end{equation}

This will be useful later.

\subsection{Numerics, Construction of Solutions}

The big issue here is that stationary solutions to \eqref{eqODE} are hard to come by. Nonetheless, we have figured out a way to compute these numerically, although it is a bit dicey. Our method is a variant of the mountain pass and string methods, and essentially combines Chen97 and Chamard10.

\begin{enumerate}
	\item The energy functional $I \in C^1(H^2, \R)$ is given by
	\begin{equation}
	I(z) = \frac{1}{2} \int_\R (|z''|^2 - c^2 |z'|^2)dx 
	+ \int_\R (e^z - z - 1) dx
	\end{equation}
	where $z$ is a function of $x$. For the numerical method, we truncate the domain to $[-L, L]$ for $L$ large.
	\item To do the mountain pass method, we need to find
	\begin{enumerate}
		\item A function $e_1 \in H^2$ which is a local minimum of $I$. From Lemma 2.4 in Chen97, $e_1 = 0$ satisfies this.
		\item A function $e_2 \in H^2$ for which $I(e_2) < I(e^1) = I(0) = 0$. We can use Lemma 2.5 in Chen97 to find such an $e_2$. 
	\end{enumerate} 
	The solution we want, by the Mountain Pass Lemma, is the infimum of the maxima of $I$ along all paths joining $e_1$ and $e_2$.
	\item Choose a discretization scheme. Following Chen97, we use the finite difference method.
	\item We then use the String Method from Chamard10.
	\begin{enumerate}
		\item Join $e_1$ and $e_2$ by a piecewise linear path $U(t) = t e_1 + (1-t) e_2 = (1-t)e_2$. Choose a discrete set of values for $t$ which are evenly spaced, i.e. the $N+1$ values $t = [0, 1/N, 2/N, ..., 1]$ for some reasonably sized $N$. We ended up using $N = 10$, which worked fine.
		\item Reparameterize the path to get points more-or-less equidistantly distributed in the $H^1$ norm.
		\item Evaluate the steepest decent direction $v(t_i)$ for all points $U(t_i)$ along the path. The steepest descent direction is computed as in section 3 of Chen97.
		\item For constrained maximal step size $h_M$, perform gradient descent on all points on the path $U(t_i)$ in the direction of $v(t_i)$.
		\item Go back to the reparameterization step, and repeat all this a bunch of times. Theoretically there is a termination condition, but we don't actually need to worry about that since we will be using Matlab's \texttt{fsolve}.
	\end{enumerate}
	\item After doing this a bunch of times, take the element of the path for which $I(U(t_i))$ is maximum. Or take one close to the maximum. Use this as an initial guess for Matlab's \texttt{fsolve}. This should converge to the solution we are looking for.
\end{enumerate}

Here are plots of the stationary solution $u(x)$ found using this method for $c = 1.354$ and $c = 1.40$. Compare these to figure 3 on p. 347 of Chen97. Note that by the substitution we performed, the baseline in our plots is at 0 as opposed to 1 in Chen97.

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{single1354.eps}
\includegraphics[width=8cm]{single14.eps}
\label{fig:single1}
\caption{Stationary traveling wave solutions to \eqref{eqODE}. $c = 1.354$ (left) and $c = 1.40$ (right).}
\end{figure}

We also compute solutions for other values of $c$.

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{single11.eps}
\includegraphics[width=8cm]{single12.eps}
\label{fig:single2}
\caption{Stationary traveling wave solutions to \eqref{eqODE}. $c = 1.1$ (left) and $c = 1.2$ (right).}
\end{figure}

Let $\nu = \pm \alpha \pm i \beta$ be the eigenvalues of the linearization about the zero solution. Then we can show numerically that the tail decays exponentially with rate (approximately) $\alpha$, and the period of the tail oscillations is (approximately) $2 \pi / \beta$. This is what we expect. \\

As in the other cases, we expect that we can join the tails to form double pulses every quarter period, i.e. every $\pi / 2 \beta$. Using the same method as we always use, together with Matlab's \texttt{fsolve}, here are the first four double pulses for $c = 1.2$.

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{double12_12.eps}
\includegraphics[width=8cm]{double12_34.eps}
% \caption{Double pulse traveling wave solutions to \eqref{eqODE} for $c = 1.2$. Double pulses 1 and 2 (left). Double pulses 3 and 4 (right).
\end{figure}

We can obtain periodic versions of all of these by interpolating the finite difference solutions onto the appropriate periodic grid, writing the operators using Fourier spectral matrices, and using \texttt{fsolve}. This is not shown.\\

\subsection{Numerics, Eigenvalue Computation}

It turns out that Matlab has a built-in polynomial eigenvalue solver, \texttt{polyeig}. However, I decided to use the quadratic eigenvalue solver \texttt{quadeig} by Chris Munro, Sven Hammarling and Francoise Tisseu (2013) because they say it's better. It probably makes very little difference.\\

Since this is going to look a lot like things I have already done (i.e. KdV5), I will not show all the plots. First, we look at $c = 1.2$.\\

For the first double pulse, here is the spectrum (zoomed near the origin) and a plot of the eigenfunctions corresponding to the small interaction eigenvalues. Note there is a pair of real interaction eigenvalues.

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{spec12_double1.eps}
\includegraphics[width=8cm]{evecs12_double1.eps}
\caption{Linearization about Double Pulse 1. Spectrum (left) and interaction eigenfunctions (right). Finite difference methods, $N = 512$, $c = 1.2$.}
\end{figure}

Next we look at the second double pulse. Note that this time we have a pair of purely imaginary (to leading order, there is a small real part of order like 1e-10 just as with KdV5) eigenvalues. The plot below is only the real part of the eigenfunctions. It is easy to locate the interaction eigenvalues in this case, since we have bounds on the essential spectrum. The interaction eigenvalues lie inside the essential spectrum gap.

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{spec12_double2.eps}
\includegraphics[width=8cm]{evecs12_double2real.eps}
\caption{Linearization about Double Pulse 2. Spectrum (left) and real part of interaction eigenfunctions (right). Finite difference methods, $N = 512$, $c = 1.2$.}
\end{figure}

The same pattern repeats (alternating real and purely imaginary interaction eigenvalues) as we go to higher double pulses.\\

We can also construct multipulses. For example, here is a triple pulse for $c = 1.2$ using the same pulse distance as double pulse 2. We get the expected two pairs of purely imaginary eigenvalues.

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{triple12_2.eps}
\includegraphics[width=8cm]{spec12_triple2.eps}
\caption{Triple pulse 2,2 (left). Spectrum of linearization about triple pulse 2,2 (right). Finite difference methods, $N = 512$, $c = 1.2$.}
\end{figure}

For other values of $c$, the odd-numbered double pulses have the same eigenvalue pattern, so we will focus on the even-numbered double pulses. Let's look at $c = 1.354$ (this value was used in Chen97). For Double Pulse 2, we now have a quartet of interaction eigenvalues. Their imaginary part now lies \emph{outside} the essential spectrum gap. This suggests that a Krein collision has occurred, where the interaction eigenvalues collide with the smallest essential spectrum eigenvalue and then form a quartet.

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{spec1354_double2.eps}
\caption{Linearization about Double Pulse 2. Spectrum showing Krein quartet. Finite difference methods, $N = 512$, $c = 1.354$.}
\end{figure}


The next two plots show that the Krein collision occurs between $c = 1.322$ (before collision) and $c = 1.323$ (after collision).

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{spec1322_double2.eps}
\includegraphics[width=8cm]{spec1323_double2.eps}
\caption{Linearization about Double Pulse 2. Spectrum before Krein collision ($c = 1.322$, left) and after Krein collision ($c = 1.323$, right). Finite difference methods, $N = 512$}
\end{figure}

\subsection{Lin's Method (part 1)}

In this section, we look at the eigenvalues of $A_0$. We expect this to be similar to SanStrut, since it's more-or-less the same thing but with a different nonlinearity. Using Matlab's \texttt{eig}, we can show that for the operator $A_0$ linearized about an $n-pulse$

\begin{enumerate}
	\item The essential spectrum is $[a, \infty)$ for $a > 0$
	\item There are $n$ negative eigenvalues clustered near the negative eigenvalue of $A_0$ for the single pulse.
	\item $n$ small eigenvalues near 0, one of which is actually at 0 (1D kernel)
\end{enumerate}

Using Lin's method (from San98) we can compute the small eigenvalues to leading order. For convenience, assume we a have an $n$-pulse, but assume that all the pulse distances are the same, say $2L$. (Of course, mixed-distance multipulses exist, but we will ignore that for now.) The, by Theorem 2 in San98, there are $n$ small eigenvalues which are given (to leading order) by solving $\det(aA - M \lambda I) = 0$. $M$ is the Melnikov integral, which in this case is

\begin{equation}
M = \int_{-\infty}^\infty (q'(x))^2 dx
\end{equation}

$A$ is the matrix

\begin{equation}
A = \begin{pmatrix}
-1 & 1 \\
1  & -2 & 1 \\
   & 1  & -2 & 1 \\
   & & \vdots & \vdots \\
   & & & & & 1 & -1
\end{pmatrix}
\end{equation}

and $a = \langle \Psi(L), Q'(-L) \rangle$, as in San98. For a 2-pulse, this becomes

\begin{align*}
\det \begin{pmatrix}
-a - M \lambda & a \\
a & -a - M \lambda
\end{pmatrix} = 0
\end{align*}

which has solutions $\lambda = \{ 0, -2a/M \}$. This is easy to compute numerically, so let's see if this agrees with what we get if we use \texttt{eig} on $A_0$.

\begin{table}[H]
\begin{tabular}{llll}
Double Pulse & Eigenvalue from eig(A0) & Predicted eigenvalue (-2a/M) & Error  \\
1            & 0.1278                  & 0.1915                       & .5     \\
2            & -0.0278                 & -0.0254                      & 0.08   \\
3            & 0.0090                  & 0.0072                       & .20    \\
4            & -0.0024                 & -0.0025                      & 0.0417 \\
5            & 6.8497e-04              & 7.2475e-04                   & 0.0581
\end{tabular}
\caption{Calculated and predicted small eigenvalue of $A_0$ about Double Pulse 2. $c = 1.2$. Finite difference methods, $N = 1500$}.
\end{table}

Well, it's not great, but the error does decrease as $L$ increases, so it should be ok. For the triple pulse we have to solve

\begin{align*}
\det \begin{pmatrix}
-a - M \lambda & a & 0\\
a & -2a - M \lambda & a \\
0 & a & -a - M \lambda
\end{pmatrix} = 0
\end{align*}

which has solutions $\lambda = \{ 0, -a/M -3a/M \}$.\\

For $c = 1.2$ and triple pulse 2,2, we have small eigenvalues from \texttt{eig} of $\{ -0.0145, -0.0401 \}$. The formula above predicts eigenvalues of $( -0.0127, -0.0381 \}$. The errors here are $0.049$ and $0.124$, which are not terrible.\\

We don't really care about this, except to contrast to the quadratic eigenvalue problem.

\subsection{Krein Matrix}

\subsubsection{Background and Numerics}

Before we do that, we can look at the Krein matrix as means of computing eigenvalues in the case where they are (predicted to be) purely imaginary. The hope is that we can get theoretical results for the 2-pulse and nice numerics for the n-pulse.
\\

This is an application Todd Kapitula's unpublished paper Krein Matrix for Star-Even Operator Polynomials (2018). Here we look at the polynomial eigenvalue problem for the suspension bridge problem linearized about stationary solution $q$. This second degree eigenvalue problem is given by

\begin{equation}
p(\lambda)v = (A_0 + A_1 \lambda + A_2 \lambda^2 )v = 0
\end{equation}

where

\begin{align*}
A_0 &= \partial_x^4 + c^2 \partial_x^2 + e^{q} \\
A_1 &= - 2 c \partial_x \\
A_2 &= I 
\end{align*}

Note that $A_0$ and $A_2$ are Hermitian, and $A_1$ is skew-Hermitian.\\

The Krein matrix should work perfectly in this case, since we don't have to deal with invertibility of operators and since the essential spectrum is purely imaginary and bounded away from the origin.\\

We know from the discussion in the previous section what the spectrum/eigenvalues of $A_0$ looks like. Since $A_0$ is Hermitian, the spectrum of $A_0$ is real. We also note that if we take $\lambda = i z$, the quadratic operator $p(iz)$ is Hermitian.
\\

Let $S = \spn\{s_1, \dots, s_n \}$, where $s_i$ are the eigenfunctions of $A_0$ corresponding to the small eigenvalues $\{\nu_1, \dots, \nu_n$. (We found these in the previous section using Lin's method). WLOG, we take $\nu_1 = 0$ and $s_1 = q'$, since we know the kernel of $A_0$ is 1-dimensional, and we know what the kernel eigenfunction is.\\

With this setup, we can easily compute the Krein matrix using the top of p.4 in Kap2018. As long as as choose $c$ such that the Krein collision has not yet occurred, we get very accurate numerical results from this method, or at least they compare extremely well to the quadratic eigenvalue solver we are using. 

\begin{table}[H]
\begin{tabular}{lll}
Pulse & From \texttt{quadeig} & imag part (from Krein Matrix) \\
Double Pulse 2    & $\pm 0.0622i$ & 0.0622 \\
Triple Pulse 2,2  & $\pm 0.0446i, \pm 0.0764i$ & 0.0446, 0.0764 \\
Double Pulse 2    & $\pm 0.0176i$ & 0.0176 \\
\end{tabular}
\caption{Eigenvalues from quadratic eigenvalue problem, computed with \texttt{quadeig} and from the Krein matrix. $c = 1.2$, finite difference method, $N = 512$. }
\end{table}

So, basically, the Krein matrix nails it every time.\\

\subsubsection{Approxmation for Small Eigenvalues}

Now that we know we can use the Krein matrix, we should be able to use the same approximation as in Section 5 of Kap2018 since these eigenvalues are small.

Recall that we  are looking for purely imaginary eigenvalues, thus we take $\lambda = i z$, $z \in \R$. Then the operator $p(iz)$ is Hermitian. By Lemma 5.6 in Kap2018, for small $|z|$ the Krein matrix is given by

\begin{equation}
\tilde{K}_S(z) = \text{diag}(\nu_1, \dots, \nu_n) + z(i A_1|_S) - z^2\left( A_2 - A_1 P_{S^\perp} (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} A_1 \right)|_S + \mathcal{O}(|z|^3)
\end{equation}

This form is slightly different from that in Lemma 5.6. Since $z$ is real, we don't have to write $\overline{z}$. We also omit the $z$ out front, i.e. we use the form of the Krein matrix from p.4 rather than the form in Theorem 3.1. The key part is that this is a polynomial in $z$, i.e. $z$ does not appear in any of the coefficients. To leading order, this is quadratic in $z$.\\

From our specific problem, we know the following.

\begin{align*}
\langle s_i, A_1 s_j \rangle &= -\langle A_1 s_i, s_j \rangle \\
&= -\langle s_j, A_1 s_i \rangle
\end{align*}

where the inner product is symmetric since everything involved is real. Thus the matrix $A_1|_S$ is skew-Hermitian. In particular, this implies that its diagonal terms are 0.\\

For the other term, we have

\begin{align*}
\langle s_i, A_1 P_{S^\perp} (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} A_1 s_j \rangle &= -\langle A_1 s_i, P_{S^\perp} (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} A_1 s_j \rangle \\
&= -\langle P_{S^\perp} (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} A_1 s_i, A_1 s_j \rangle \\
&= -\langle A_1 s_j, P_{S^\perp} (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} A_1 s_i \rangle \\
&= \langle s_j, A_1 P_{S^\perp} (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} A_1 s_i \rangle
\end{align*}

where, again, the inner product is symmetric since everything involved is real. This should be legit since $A_0$ is self-adjoint and orthogonal projections are as well. From this, we see that the matrix $\left( A_1 P_{S^\perp} (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} A_1 \right)|_S$ is Hermitian.\\

The matrix $A_2|_S = I|_S = I$. Thus the Krein matrix is the sum of a diagonal matrix, a skew-Hermitian matrix, and a Hermitian matrix.

\subsubsection{1-pulse}

The easiest case, which is almost trivial, is the single pulse, which we know has exactly one eigenvalue at the origin. For the Krein matrix, we take $S = \ker A_0$ = $\spn \{q_x\}$. Thus the Krein matrix will be 1x1, i.e. a scalar. In this case, $\nu_1 = 0$ and $A_1|_S = 0$, thus we have

\begin{align*}
K(z) &= -z^2\left( \langle q_x, q_x \rangle - \langle q_x, A_1 P_{S^\perp} (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} A_1 q_x \rangle \right) + \mathcal{O}(z^3) \\
&= -z^2\left( \langle q_x, q_x \rangle + \langle A_1 q_x, P_{S^\perp} (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} A_1 q_x \rangle \right) + \mathcal{O}(z^3) \\
&= -4 c^2 z^2\left( \langle q_x, q_x \rangle + \langle \partial_x q_x, P_{S^\perp} (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} \partial_x q_x \rangle \right) + \mathcal{O}(z^3) \\
&= -4 c^2 z^2\left( \langle q_x, q_x \rangle + \langle q_{xx}, P_{S^\perp} (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} q_{xx} \rangle \right) + \mathcal{O}(z^3) \\
\end{align*}

All that is left is to evaluate $P_{S^\perp} (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} q_{xx}$. Since $S$ is the span of a single odd function, and $q_{xx}$ is an even function, $P_{S^\perp} q_{xx} = q_{xx}$. Thus we only need to evaluate $P_{S^\perp} (P_{S^\perp} A_0 P_{S^\perp})^{-1} q_{xx}$. Let $s = (P_{S^\perp} A_0 P_{S^\perp})^{-1} q_{xx}$. Then $(P_{S^\perp} A_0 P_{S^\perp}) s = q_{xx}$. From \eqref{uc}, we have

\begin{equation*}\label{uc}
A_0 \left( -\frac{1}{2c} q_c \right) = q_{xx}
\end{equation*}

Thus we have

\[
s = -\frac{1}{2c} q_c + k q_x
\]

for some scalar $k$. Substituting this into the equation for $K(z)$, and noting that the $k q_x$ term is wiped out by the projection $P_{S^\perp}$, we have

\[
K(z) = -b z^2 + \mathcal{O}(z^3)
\]

where

\begin{align*}
b &= 4 c^2 \left( \langle q_x, q_x \rangle - \frac{1}{2c} \langle q_{xx}, q_c \rangle \right)  \\
\end{align*}

Numerics suggests this is negative, but hopefully we can find a way to show that.

\subsubsection{2-pulse}

The next easiest case is the 2-pulse, since the Krein matrix will be 2x2. Let's look at that first. \\

Let 

\[
A_1|_S = \begin{pmatrix} 0 & a \\ -a & 0 \end{pmatrix}
\]

and let

\[
\left( A_2 - A_1 P_{S^\perp} (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} A_1 \right)|_S = \begin{pmatrix} b_{11} & b_{12} \\ b_{12} & b_{22} \end{pmatrix}
\]

Since the 2-pulse is an even function, it is not hard to show that the eigenfunction $s_2$ corresponding to the small nonzero eigenvalue $\nu_2$ is an even function as well. The kernel eigenfunction $s_1$ is an odd function since it is the derivative of an even function. Thus $\langle s_1, A_2 s_2 \rangle = \langle s_1, s_2 \rangle = 0$. Furthermore, we have

\begin{align*}
\langle s_1, A_1 P_{S^\perp} (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} A_1 s_2 \rangle 
&= -\langle A_1 s_1, P_{S^\perp} (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} A_1 s_2 \rangle 
\end{align*}

The operator $A_0$ is even, i.e. it preserved the even/odd property of functions. The projection $P_{S^\perp}$ wipes out the components in $S$, which consists of one even function and one odd function. Thus $P_{S^\perp}$ is also an even operator. The differential operator $A_1$ is an odd operator. The inner product above is the inner product of an even function and an odd function, and thus must be 0, so we have $b_{12} = 0$. Thus we have

\[
\left( A_2 - A_1 P_{S^\perp} (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} A_1 \right)|_S = \begin{pmatrix} b_{11} & 0 \\ 0 & b_{22} \end{pmatrix}
\]

The Krein matrix $K(z)$ is then, to leading order,

\begin{equation}
\tilde{K}(z) = \begin{pmatrix}
-b_{11} z^2 & i a z \\
i a z & -b_{22} z^2 + \nu
\end{pmatrix}
\end{equation}

where $\nu$ is the small, nonzero eigenvalue of $A_0$. Taking the determinant, setting it equal to 0, and solving for $z$, we get a double root at $z = 0$, as well as

\begin{align*}
z = \pm \sqrt{\nu /b_{22} + a^2/(b_{11} b_{22})}
\end{align*} 

The idea is that if $\nu$ is negative, we expect (from numerics) for this to be real. If both $b_{ii}$ are negative, which numerics suggests, this will be the case, so all we have to do is show that.\\

\begin{align*}
b_{ii} &= \langle(s_i, A_2 - A_1 P_{S^\perp} (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} A_1 s_i \rangle \\
&= \langle s_i, s_i \rangle - \langle s_i, c \partial_x P_{S^\perp} (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} c \partial_x  s_i \rangle \\
&= ||s_i||^2 + 4 c^2 \langle \partial_x s_i, P_{S^\perp} (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} \partial_x s_i \rangle
\end{align*}

The idea here is that by Lin's method, we should be able to show that the $b_{ii}$ are, to leading order, equal to $b$ from the previous section. In that case, we are done, as long as we have showed that $b$ is negative. So let's show that.\\

First, we look at $s_1 = q_2'$, where $q_2$ is the 2-pulse solution. Rather than using the piecewise form of $q$, as we do in Lin's method, we write the 2-pulse as

\[
q_2 = q(\cdot + X) + q(\cdot - X) + \mathcal{O}(e^{-\alpha X})
\]

Since the operator $A_0$ depends on the equilibrium solution we are linearizing about, we make that dependency explicit.

\begin{equation}
A_0(u^*) = \partial_x^4 + c^2 \partial_x^2 + e^{u^*}
\end{equation}

Let $A_0^+ = A_0(q(\cdot + X))$ and $A_0^- = A_0(q(\cdot - X))$. First, we would like to write $A_0(q_2)$ in terms of $A_0^+$ and $A_0^-$. The only issue here is the $e^{q2}$ term, so that is what we will deal with now.\\

\begin{align*}
\exp(q_2(x)) &= \exp( q(x + X) + q(x - X) + \mathcal{O}(e^{-\alpha X}) ) \\
&= \exp( q(x + X) + q(x - X)) \exp( \mathcal{O}(e^{-\alpha X}) )\\
&= \exp( q(x + X) + q(x - X)) (1 + \mathcal{O}(e^{-\alpha X})) \\
&= \exp( q(x + X) + q(x - X)) + \mathcal{O}(e^{-\alpha X}) \\
&= \exp( q(x + X))\exp(q(x - X)) + \mathcal{O}(e^{-\alpha X})
\end{align*}

where the last line holds since $q$ is bounded. For convenience, let $q^+(x) = q(x + X)$ and $q^-(x) = q(x - X)$. Looking at the first term on the RHS, we have

\begin{align*}
\exp( q^+(x))\exp(q^-(x)) &=
\sum_{m=0}^\infty \frac{q^+(x)^m}{m!}
\sum_{n=0}^\infty \frac{q^-(x)^n}{n!}\\
&= \sum_{m=0}^\infty \frac{q^-(x)^m}{m!} 
+ \sum_{n=0}^\infty\frac{q^-(x)^n}{n!} - 1 +
\sum_{m=1}^\infty \frac{q^+(x)^m}{m!}
\sum_{n=1}^\infty \frac{q^-(x)^n}{n!} \\
&= \exp( q^+(x)) + \exp(q^-(x)) - 1 +
q^+(x)q^-(x)\sum_{m=0}^\infty \frac{q^+(x)^m}{(m+1)!}
\sum_{n=0}^\infty \frac{q^-(x)^n}{(n+1)!}
\end{align*}

For the final terms on the RHS, we have

\begin{align*}
\left| q^+(x)q^-(x)\sum_{m=0}^\infty \frac{q^+(x)^m}{(m+1)!} \sum_{n=0}^\infty \frac{q^-(x)^n}{(n+1)!} \right| 
&\leq \left| q^+(x)q^-(x)\sum_{m=0}^\infty \frac{q^+(x)^m}{m!} \sum_{n=0}^\infty \frac{q^-(x)^n}{n!} \right| \\
&\leq \left| q^+(x)q^-(x) \right| | e^{q^+(x)}e^{q^-(x)} |\\
&\leq C \left| q^+(x)q^-(x) \right|
\end{align*}

since $q^\pm$ are bounded. Since the two peaks are well separated, $q^+(x)q^-(x) = \mathcal{O}(e^{-\alpha X)}$. Thus, putting this all together, we have

\begin{equation}
\exp(q_2(x)) = \exp( q^+(x)) + \exp(q^-(x)) - 1 + \mathcal{O}(e^{-\alpha X}) ) 
\end{equation}

Thus we have

\begin{equation}
A_0(q_2) = A_0^+ + A_0^- - 1
\end{equation}

Unfortunately, this is not what we have do deal with, since $A_0$ appears in the form $(P_{S^\perp}A_0 P_{S^\perp})^{-1}$.


GARBAGE FOLLOWS


for $i = 1, 2$. Assume we have the bound

\[
|(u_i^\pm)^{(n)}(x)| \leq C e^{-\alpha X}
\] 

which is the same as Theorem 1 in San98 with $u_i^\pm$ replaced with its $n$th derivative (this should follow from San93). Then it is not hard to see that 

\[
||q_2||^2 = 2 ||q||^2 + \mathcal{O}(e^{-\alpha X})
\]

For the other term, since $(P_{S^\perp} A_0 P_{S^\perp})^{-1}$ is bounded, we should have

\begin{align*}
\langle \partial_x q_2', P_{S^\perp} (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} \partial_x q_2 \rangle 
= 2 \langle q'', P_{S^\perp} (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} q'' \rangle + \mathcal{O}(e^{-\alpha X})
\end{align*}

Putting this all together, we conclude that 

\[
b_{11} = 2b + \mathcal{O}(e^{-\alpha X})
\]

Now we look at $b_{22}$. In this case, recall from San98 that the eigenfunction $s_2$ is a piecewise perturbation of $q'$. Writing this eigenfunction as $s$, we have

\[
s_i^\pm = d_i q_2' + w_i^\pm
\]

for $i = 1, 2$, where the $d_i$ are constants and we have the bound $|w_i^\pm| \leq C e^{-2 \alpha X}|d|$. Since the eigenvalue associated with this eigenfunction is real, the $d_i$ are real. Furthermore, $s$ is even (as remarked above). Since $q_2'$ is odd, we must have $d_2 = -d_1$, and the $w_i^\pm$ have the appropriate symmetry to make $s$ even. Furthermore, since we can scale eigenfunctions by any constant, we can take $d_1 = 1$. Thus we only need to look at the left half of this. It is not hard to see in this case that we will have the same thing as above, i.e. 

\[
b_{22} = 2b + \mathcal{O}(e^{-\alpha X})
\]

Thus the Krein matrix should be

\begin{equation}
K(z) = \begin{pmatrix}
-b z^2 & i a z \\
i a z & -b z^2 + \nu
\end{pmatrix} + \mathcal{O}(e^{-\alpha X} z^2 + z^3)
\end{equation}

Thus, we have, to leading order,

\begin{align*}
z = \pm \sqrt{\nu / b + a^2/ b^2 }
\end{align*} 

where 

\[
a = \langle 2c q_2'', s_2 \rangle
\]

\begin{align*}
b &= 4 c^2 \left( \langle q_x, q_x \rangle - \frac{1}{2c} \langle q_{xx}, q_c \rangle \right)  \\
\end{align*}

As long as $\nu$ and $b$ have the same sign, $z$ is real, and thus we have two complex conjugate eigenvalues, to leading order. Since everything is analytic, we should be able to conclude we have two complex conjugate eigenvalues.\\

Numerics suggests that $b < 0$, thus this should hold for $\nu < 0$.\\

Here is table showing how good these approximations are.

\begin{table}[H]
\begin{tabular}{llll}
Pulse & $z$ (full Krein matrix) & $z$ (approx Krein matrix)\\
Double Pulse 2  & 0.0625 & 0.0614 \\
Double Pulse 4  & 0.0176 & 0.0175 \\
\end{tabular}
\caption{Eigenvalues and approximate eigenvalues from Krein matrix. $c = 1.2$, finite difference method, $N = 512$. }
\end{table}

\subsubsection{Multi-pulse}

We can extend this to the multipulse. Let

\[
A_1|_S = \begin{pmatrix} 0 & a_{12} & a_{13} & \dots & a_{1n} \\ 
-a_{12} & 0 & a_{23} & \dots & a_{2n} \\
-a_{13} & -a_{23} & 0 & \dots & a_{3n} \\
\vdots & \vdots & & \vdots \\
-a_{1n} & -a_{2n} & -a_{3n} & \dots & 0
\end{pmatrix} 
\]

and let

\[
\left( A_2 - A_1 P_{S^\perp} (P_{S^\perp} A_0 P_{S^\perp})^{-1} P_{S^\perp} A_1 \right)|_S = 
\begin{pmatrix} b_{11} & b_{12} & \dots & b_{1n} \\ 
b_{12} & b_{22}  & \dots & b_{2n} \\
\vdots & \vdots & & \vdots \\
b_{1n} & b_{2n}  & \dots & b_{nn}
\end{pmatrix}
\]

Numerics suggests that the $a_{ij}$ are small compared to the diagonal entries $b_{ii}$ (some appear to be 0, but not all of them). Numerics also suggests that the off-diagonal entires $b_{ij}, i \neq j$ are small compared to the $b_{ii}$ (again, some appear to be 0, but not all of them, as in the 2-pulse case). Given these observations it seems reasonable to hypothesize that the approximate Krein matrix is diagonally dominant, and can thus be approximated with the diagonal matrix

\begin{align*}
\tilde{K}_2(z) &= \begin{pmatrix}
-b_{11} z^2 & 0 & \dots & 0\\
0 & -b_{22} z^2 + \nu_2 & \dots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \dots & -b_{nn} z^2 + \nu_n
\end{pmatrix}
\end{align*}

Thus the small eigenvalues are approximately given by $\tilde{z}_i = \pm \sqrt{\nu_i / b_{ii}}$. We can check how close this is for the triple pulse 2,2.

\begin{table}[H]
\begin{tabular}{lll}
Pulse & Eigenvalue from \texttt{quadeig} & Approximate eigenvalue $\tilde{z}$ \\
Triple Pulse 2,2  & $\pm 0.0446i, \pm 0.0764i$ & 0.0440, 0.0724 \\
\end{tabular}
\caption{Eigenvalues from quadratic eigenvalue problem, computed with \texttt{quadeig} and from the approximate Krein matrix $\tilde{K}_2$. $c = 1.2$, finite difference method, $N = 512$. }
\end{table}

\subsection{Lin's Method (part 2)}

We will try the same thing as was done in San98. The main difference here is that we have a quadratic eigenvalue problem. The linearization about the rest state is hyperbolic, so that should present no problem. And we know that we have an eigenvalue at 0, so we should be able to perturb that eigenfunction to construct the interaction eigenfunctions. Numerical plots of the interaction eigenfunctions suggest that should be possible, and there is no reason to suspect this won't work. This is a sketch of what we need to do; if it looks promising, details can be filled it.

\begin{enumerate}

\item Let $q(x)$ be a single pulse equilibrium solution to \eqref{eqODE}. Suppose for now that it exists and decays exponentilly with rate $\alpha$. Suppose we have also constructed a double pulse $q_2(x)$, and it has the same decay properties. We have not proven we can do this, but we can likely follow SanStrut. The double pulse is a perturbation of the single pulse, and can be written piecewise on the appropriate intervals as

\[
q_2(x) = q(x) + r_i^\pm(x)
\]

where the remainder term $r_i^\pm$ is small and hopefully we can get estimates on it.

\item Write the EVP as 1st order system following San98. In other words, split off the $\lambda$ terms. Let $V = (v_1, v_2, v_3, v_4) = (v, v_x, v_{xx}, v_{xxx})$. Then we have

\begin{equation}\label{splitevp}
V' = A(q_2)V + \lambda B_1 V + \lambda^2 B_2 V
\end{equation}

where

\begin{equation}
A(q_2) = \begin{pmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
-e^{q_2} & 0 & -c^2 & 0 
\end{pmatrix}
\end{equation}

\begin{align*}
B_1 = \begin{pmatrix}
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 2 c & 0 & 0 
\end{pmatrix} &&
B_2 = \begin{pmatrix}
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
-1 & 0 & 0 & 0 
\end{pmatrix}
\end{align*}

\item The variational and adjoint variational equations we will use are

\begin{align}
V' &= A(q)V \\
W' &= -A(q)^* W
\end{align}

For now, we assume that $Q'$ is the unique bounded solution to the variational equation, and that therefore the adjoint variational equation has a unique bounded solution $\Psi$. Let $Z = \C \Psi(0)$.

\item To exploit all of this, we will write our eigenfunction piecewise as

\[
V_i^\pm = d_i Q_2' + W_i^\pm = d_i(Q' + (R_i^\pm)') + W_i^\pm
\]


Now we substitute this into \eqref{splitevp}. Using the fact that $(Q_2')' = A(q_2)Q_2'$, this becomes

\[
(W_i^\pm)' =A(q_2)W_i^\pm + \lambda B_1 W_i^\pm + \lambda^2 B_2 W_i^\pm 
+ \lambda B_1 d_i Q_2' + \lambda^2 B_2 d_i Q_2' 
\]

Let 

\begin{align*}
\tilde{H}_i^\pm &= Q_2' = Q' + (R_i^\pm)' \\
H &= Q'
\end{align*}

Then this becomes

\[
(W_i^\pm)' =A(q_2)W_i^\pm + \lambda B_1 W_i^\pm + \lambda^2 B_2 W_i^\pm 
+ \lambda B_1 d_i \tilde{H}_i^\pm + \lambda^2 B_2 d_i \tilde{H}_i^\pm 
\]

Finally, let $G_i^\pm = A(q_2) - A(q) = A(q + r_i^\pm) - A(q)$. Then this becomes

\[
(W_i^\pm)' =A(q)W_i^\pm + (G_i^\pm + \lambda B_1 + \lambda^2 B_2) W_i^\pm 
+ d_i( \lambda B_1 + \lambda^2 B_2 ) \tilde{H}_i^\pm 
\]

We don't have any bounds on anything yet, but we can show easily that

\begin{align*}
G_i^\pm(x) &= \mathcal{O}(e^{q(x) + r_i^\pm(x)} - e^{q(x)} )\\
&= \mathcal{O}(e^{q(x)}(e^{r_i^\pm(x)} - 1)) \\
&= \mathcal{O}(e^{r_i^\pm(x)} - 1)
\end{align*}

since $q(x)$ is bounded and is given. The idea is that $r_i^\pm$ is small, thus $G_i^\pm$ is small as well.\\

Thus we have the following system we need to solve.

\begin{enumerate}[(i)]
\item $(W_i^\pm)' A(q)W_i^\pm + (G_i^\pm + \lambda B_1 + \lambda^2 B_2) W_i^\pm + d_i( \lambda B_1 + \lambda^2 B_2 ) \tilde{H}_i^\pm$
\item $W_i^\pm(0) \in Z \oplus Y^+ \oplus Y^-$
\item $W_i^+(0) - W_i^-(0) \in Z$
\item $W_i^+(X_i) - W_{i+1}^-(-X_i) = D_i d$
\end{enumerate}

where

\begin{equation}
D_i d = (Q'(-X_i) + (R_{i+1}^-)(-X_i))d_{i+1}
- (Q'(X_i) + (R_i^+(X_i))d_i
\end{equation}

Compare this to (3.7) and (3.8) in San98. This is essentially the same except for the quadratic terms in $\lambda$.\\

\item Let $\Phi(y, x)$ be the evolution operator of the variational equation. Since the linearization about the zero solution is hyperbolic, the adjoint variational equation has an exponential dichotomy on $\R^\pm$, and we split the evolution operator up into pieces $\Phi_\pm^{s/u}$ on the appropriate pieces of the dichotomy. We have the usual estimates for these.\\

Next, we write this in integrated form. This will be the same as in San98, except we have the presence of the $\lambda^2$ terms.

\begin{align*}
W_i^-(x) = \Phi_-^s(&x, -X_{i-1})a_{i-1}^- + \Phi_-^u(x, 0)b_i^- \\
&+ \int_0^x \Phi_-^u(x, y)[(G_i^-(y) + \lambda B_1 + \lambda^2 B_2) W_i^-(y) + d_i (\lambda B_1 + \lambda^2 B_2) \tilde{H}_i^-(y) ] dy \\
&+ \int_{-X_{i-1}}^x \Phi_-^s(x, y)[(G_i^-(y) + \lambda B_1 + \lambda^2 B_2) W_i^-(y) + d_i (\lambda B_1 + \lambda^2 B_2) \tilde{H}_i^-(y) ] dy \\
W_i^+(x) = \Phi_+^u(&x, X_i)a_i^+ + \Phi_+^s(x, 0)b_i^+ \\
&+ \int_0^x \Phi_+^s(x, y)[(G_i^+(y) + \lambda B_1 + \lambda^2 B_2) W_i^+(y) + d_i (\lambda B_1 + \lambda^2 B_2) \tilde{H}_i^+(y) ] dy \\
&+ \int_{X_i}^x \Phi_+^u(x, y) [(G_i^+(y) + \lambda B_1 + \lambda^2 B_2) W_i^+(y) + d_i (\lambda B_1 + \lambda^2 B_2) \tilde{H}_i^+(y) ] dy
\end{align*}

Again, these are similar to (3.14) in San98, except for the presence of the $\lambda^2$ terms.\\

\item Before we keep going, by what we have learned so far, this will produce two Melkinov integrals, one for each of the $B_i$. These should be, based on what we have from before

\begin{align*}
M_1 &= \int_{-\infty}^\infty \langle \Psi(x), B_1 H(x) \rangle dx \\
M_2 &= \int_{-\infty}^\infty \langle \Psi(x), B_2 H(x) \rangle dx
\end{align*}

Since $H(x) = Q'(x)$, we know what that is. We need to figure out what $\Psi$ is. The variational problem, if we write it as a 4th order ODE, is

\[
(\partial_x^4 + c^2 \partial_x^2 + e^{q})]v = 0
\]

This has unique bounded solution $q'(x)$. Since only even derivatives are involved, this is self-adjoint, thus the adjoint variational problem, when written in this form, also has a unique solution $q'(x)$. We know from before that this implies that for the system-version $\Psi' = -A(q)^* \Psi$, the fourth component of $\Psi$ is $q'(x)$. From this we can determine the rest. Written out, the matrix equation $W' = -A(q)^* W$ is

\begin{equation}
\begin{pmatrix}w_1 \\ w_2 \\ w_3 \\ w_4 \end{pmatrix}' =
\begin{pmatrix}
0 & 0 & 0 & e^q \\
-1 & 0 & 0 & 0 \\
0 & -1 & 0 & c^2 \\
0 & 0 & -1 & 0 
\end{pmatrix}
\begin{pmatrix}w_1 \\ w_2 \\ w_3 \\ w_4 \end{pmatrix}
\end{equation}

This is equivalent to the system of four equations

\begin{align*}
w_1' &= e^q w_4 \\
w_2' &= -w_1 \\
w_3' &= -w_2 + c^2 w_4 \\
w_4' &= -w_3
\end{align*}

Rearranging to solve for $(w_1, w_2, w_3)$ in terms of $w_4$, we have

\begin{align*}
w_3 &= -w_4' \\
w_2 &= w_4'' + c^2 w_4 \\
w_1 &= -w_4''' - c^2 w_4'
\end{align*}

The final equation becomes $-w_4'''' - c^2 w_4'' = e^q w^4$, which is satisfied, as expected, by $w_4 = q'$. Thus we have

\begin{align*}
\Psi = \begin{pmatrix}
-q'''' - c^2 q''\\
q''' + c^2 q'\\
-q''\\
q'
\end{pmatrix}
\end{align*}

Using this, we can compute the Melnikov integrals.

\begin{align*}
M_1 &= \int_{-\infty}^\infty \langle \Psi(x), B_1 \tilde{H}(x) \rangle dx \\
&= \int_{-\infty}^\infty c q'(x) q''(x) dx \\
&= 0 \\
M_2 &= \int_{-\infty}^\infty \langle \Psi(x), B_2 \tilde{H}(x) \rangle dx \\
&= \int_{-\infty}^\infty  (q'(x))^2 dx \\
&\neq 0
\end{align*}

Since one of these is nonzero, we should be good. In fact, we need $M_1$ to be 0 to get the eigenvalues we expect.

\item If we work through all of this naively and try to guess what the result is, we do not get the right result. The naive guess for the double pulse is that, after all is said and done, we get $\lambda^2 = \sqrt{-2a / M}$, i.e. these eigenvalues are the square roots of the eigenvalues of $A_0$. From our numerics and Krein matrix calculations, we have shown this is not the case. Thus either we set the problem up incorrectly, or there are other terms contributing to the leading order estimate which we have not accounted for in the naive guess.


\end{enumerate}

\subsection{References}

Y.Chen and P.J.McKenna. Traveling Waves in a Nonlinearly Suspended Beam: Theoretical Results and Numerical Observations. Journal of Differential Equations Volume 136, Issue 2, 20 May 1997, Pages 325-355.\\

Will add other references when I do the actual writeup.


\end{document}