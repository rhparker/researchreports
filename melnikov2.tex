\documentclass[12pt]{article}
\usepackage[pdfborder={0 0 0.5 [3 2]}]{hyperref}%
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}%
\usepackage[shortalphabetic]{amsrefs}%
\usepackage{amsmath}
\usepackage{enumerate}
% \usepackage{enumitem}
\usepackage{amssymb}                
\usepackage{amsmath}                
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage[table,xcdraw]{xcolor}
\usepackage{tikz}
\usepackage{float}
\usepackage{booktabs}
\usepackage{svg}
\usepackage{mathtools}
\usepackage{cool}
\usepackage{url}
\usepackage{graphicx,epsfig}
\usepackage{makecell}
\usepackage{array}

\def\noi{\noindent}
\def\T{{\mathbb T}}
\def\R{{\mathbb R}}
\def\N{{\mathbb N}}
\def\C{{\mathbb C}}
\def\Z{{\mathbb Z}}
\def\P{{\mathbb P}}
\def\E{{\mathbb E}}
\def\Q{\mathbb{Q}}
\def\ind{{\mathbb I}}

\graphicspath{ {images17/} }

\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{hypothesis}{Hypothesis}

\begin{document}

\section*{Melnikov Integrals for Eigenfunctions of KdV5}

\subsection*{Introduction}

\subsubsection*{Prelimiaries}

A single-pulse solution $q(x; c)$ to KdV5 is a homoclinic orbit connecting the equilibrium solution at 0 to itself. The single pulse solves the stationary KdV5 equation

\begin{equation}
u_{xxxxx} - u_{xxx} + c u_x - 2 u u_x = \partial_x(u_{xxxx} - u_{xx} + c u - u^2) = 0
\end{equation}

as well as the integrated equation

\begin{equation}\label{4thorder}
u_{xxxx} - u_{xx} + c u - u^2 = 0
\end{equation}

where we can take the constant of integration to be 0 since the pulse decays to 0 at $\pm \infty$. \\

The energy of the system is given by 

\begin{equation} \label{energy}
E(u) = -\int_{-\infty}^{\infty} \left( \frac{1}{2}u_{xx}^2 + \frac{1}{2}u_x^2 + \frac{1}{2}cu^2 - \frac{1}{3}u^3 \right) dx
\end{equation}

and the full KdV5 equation (time-dependent, nonstationary) can be written as $u_t = \partial_x E'(u)$. It can be shown that this quantity is conserved (in time).\\

For a stationary solution $u$, we have $u_t = 0$, which implies $\partial_x E'(u(x)) = 0$, i.e. $E'(u(x))$ is constant. Since our homoclinic orbit connects the equilibrium at 0 to itself, and since $E'(0) = 0$, we can take this constant to be 0, which is the equation \eqref{4thorder} above. \\

We also have a Hamiltonian $H$ for the equilibrium ODE, which is (Champneys, 1998)

\begin{equation}\label{Hamiltonian}
H(u, u', u'', u''') = u'u''' - \frac{1}{2}(u'^2) - \frac{1}{2}(u'')^2 + \frac{c}{2}u^2 - \frac{1}{3}u^3 
\end{equation}

We can show this is conserved, i.e. independent of $x$

\begin{align*}
\frac{\partial H}{\partial x} &= u'u'''' + u''u''' - u'u'' - u''u''' + c u u' - u^2 u' \\
&= u'( u'''' - u'' + cu - u^2 ) \\
&= 0
\end{align*}

where in the last line we substituted in the 4th order stationary equation $\eqref{4thorder}$.\\

Since we know the form of the Hamiltonian explicitly, we can compute its total derivative.

\begin{equation}
DH(u, u', u'', u''') = (cu - u^2, u''' - u', -u'', u') = (u(c - u), u''' - u', -u'', u') 
\end{equation}

\subsubsection*{Single Pulse Solutions}

We are interested in the derivative of the Hamiltonian at the peak of a single pulse solution. We make the following hypothesis about the existence of a single-pulse solution (although Chugunova and Pelinovsky actually have an exact solution for a specific value of $c$, so it's not really necessary)

\begin{hypothesis}\label{singlepulseexists}
For a speed $c_0 > 0$, a single pulse solution $q_0(x) = q(x; c_0)$ exists to \eqref{4thorder}. This single pulse solution is a homoclinic orbit connecting the equilibrium at $u = 0$ to itself and is exponentially localized.
\end{hypothesis}

First, we want to show that the total derivative $DH$ of the Hamiltonian \eqref{Hamiltonian} is nonzero at $q_0(0)$, the center of the single pulse.

\begin{lemma}\label{DHnonzero}
For the single pulse solution $q_0(x)$ from Hypothesis \ref{singlepulseexists}, the total derivative of the Hamiltonian \eqref{Hamiltonian} evaluated at $q_0(0)$ is nonzero.
\begin{proof}
We want to evaluate $DH$ at $q_0(0)$. Note that this point is the peak (maximum) of the single pulse and the single pulse is an even function, so all odd derivatives are zero. Thus we have

\[
DH(q_0, q_0', q_0'', q_0''')\Big|_{x = 0} = \Big(q_0(0)(c - q_0(0)), 0, -q_0''(0), 0 \Big)
\]

We want to show that this is nonzero. We can do that by showing that either the first component or the third component cannot be 0. The first component can only be 0 if $q_0(0) = 0$ or $q_0(0) = c$. Since the maximum of the pulse occurs at $x = 0$, and the pulse connects the equilibrium at $u = 0$ to itself, we cannot have $q_0(0) = 0$, otherwise we would not have a pulse in the first place. If $q_0(0) = c$, then we can substitute this into the ODE \eqref{4thorder} to get

\begin{align*}
q_0''''(0) - q_0''(0) + c q_0(0) - q_0(0)^2 &= 0 \\
q_0''''(0) - q_0''(0) + c^2 - c^2 &= 0 \\
q_0''''(0) = q_0''(0)
\end{align*}

If $q_0''(0) = 0$, this implies $q_0''''(0)$ = 0. In other words, the constant solution $q_0(x) = c$ is the unique solution to \eqref{4thorder} with initial condition $(u(0), u'(0), u''(0), u'''(0)) = (c, 0, 0, 0)$. This is not an exponentially decaying pulse, so this cannot be the case. Thus we conclude that if $q_0(0) = c$, we must have $q_0''(0) \neq 0$. In either case, the total derivative $DH(q_0, q_0', q_0'', q_0''')$ cannot vanish at $x = 0$.

\end{proof}
\end{lemma}

Before we continue, we will write $\eqref{4thorder}$ as a 4th order system. Note that we are following our usual notational convention, where capital letters represent vectors in the 4th order system, while small letters represent their scalar counterparts. As a 4th-order system, we have

\begin{equation}\label{nonlinearsystem}
U' = F(U; c) = A(0; c) U + N(U)
\end{equation}

where $U = (u_1, u_2, u_3, u_4) = (u, u', u'', u''')$ and $N(U) = N(u_1, u_2, u_3, u_4) = (0, 0, 0, u_1^2)$. Note that we have separated the equation in to a linear and a nonlinear part, and that for the nonlinear part, $N(0) = DN(0) = 0$. \\

Let $Q(x; c) = (q(x; c) q'(x; c), q''(x; c), q'''(x; c))$. From here, we show that we can define a manifold in $H^{-1}(0)$ at $Q(0; c_0)$.

\begin{lemma}\label{manifoldinH0}
The zero level set of the Hamiltonian $H^{-1}(0)$ contains a 3-dimensional manifold in a neighborhood of $Q(0; c_0)$.
\begin{proof}
By Lemma \ref{DHnonzero}, $DH(Q(0; c_0)) \neq 0$. For convenience, write $Q(x) = Q(x; c_0)$. From the proof of that lemma, either the first or the the third component of $DH(Q(0))$ is nonzero. Taking the first component to nonzero, using the IFT we can write $q_1$ locally as a graph over $(q_2, q_3, q_4)$. In other words, there exists a unique $C^1$ function $g: U \rightarrow \R$, where $U$ is an open neighborhood of $(q_2(0), q_3(0), q_4(0))$, such that $g(q_2(0), q_3(0), q_4(0)) = q_1(0)$ and $H(g(q_2, q_3, q_4), q_2, q_3, q_4 ) = 0$ on $U$. The three-dimensional surface:

\[
\{ (g(q_2, q_3, q_4), q_2, q_3, q_4) : (q_2, q_3, q_4) \in U \}
\]

is our desired manifold, which is contained in $H^{-1}(0)$. A similar formulation holds if the third component of $DH(Q(0))$ is nonzero.
\end{proof}
\end{lemma}

Next, we linearize \eqref{nonlinearsystem} about the equilibrium solution at 0. In doing so, we obtain the constant-coefficient matrix system $U' = A(0; c) U$, where

\[
A(0; c) = 
\begin{pmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
-c & 0 & 1 & 0 \\ 
\end{pmatrix}
\]

The eigenvalues of $A(0; c)$ are
\[
\nu = \pm \sqrt{ \frac{1 \pm \sqrt{1 - 4c} }{2}}
\]

For $c < 1/4$, all four eigenvalues are real (two positive and two negative), and for $c > 1/4$, the eigenvalues are two complex conjugate pairs (one pair with positive real part and one pair with negative real part). In both cases, we have a two-dimensional stable eigenspace and a two-dimensional unstable eigenspace. We designate the two stable eigenvalues by $\nu_1^s(c)$ and $\nu_2^s(c)$, where

\begin{align*}
\nu_1^s(c) &= \sqrt{ \frac{1 + \sqrt{1 - 4c} }{2}}\\
\nu_2^s(c) &= \sqrt{ \frac{1 - \sqrt{1 - 4c} }{2}}
\end{align*}

The corresponding eigenvectors are $v_1^s(c)$ and $v_2^s(c)$ (these can be written down exactly, but it's a mess). The unstable eigenvalues are $\nu_1^u(c)$ and $\nu_2^u(c)$, with corresponding eigenvectors $v_1^u(c)$ and $v_2^u(c)$. The stable and unstable eigenspaces are $E^s(c) = \text{span}\{ v_1^s(c), v_2^s(c) \}$ and $E^u(c) = \text{span}\{ v_1^u(c), v_2^u(c) \}$.\\

Let $\alpha^u$ be the smallest real part of the unstable eigenvalues and $-\alpha^s$ be the largest real part of the stable eigenvalues. Let $\alpha = \min \{ \alpha^u, \alpha^s \}$.\\

For what follows, we will be working in an exponentially weighted function space $X_\eta$, defined in the usual way by 

\[
X_\eta = \{ f \in C^0([0, \infty), \R^n) : \sup_{x \in [0, \infty)} |e^{\eta x} f(x)| < \infty 
\]

where the norm is given by

\[
||f||_\eta = \sup_{x \in [0, \infty)} |e^{\eta x} f(x)|
\]

This space is known to be a Banach space, and the weight $\eta > 0$ will be chosen later. Since we wish our functions to decay exponentially at $+\infty$, we want the weight to be positive.\\

We would like to show that we can take the derivative of our single pulse $q(x)$ with respect to the wave speed $c$, and that the derivative $q_c(x)$ is exponentially localized. For this to be the case, we will need the following hypothesis.

\begin{hypothesis}\label{transverseint}
For some $c_0 > 0$, the stable manifold $W^s(0; c_0)$ and the unstable manifold $W^u(0; c_0)$ for the equilibrium solution $u = 0$ intersect transversely in $H^{-1}(0)$ at a point $Q(0; c_0) \neq 0$, where $H$ is the Hamiltonian for the ODE. Since by Lemma \eqref{manifoldinH0} $H^{-1}(0)$ is a manifold in a neighborhood of $Q(0; c_0)$, this hypothesis makes sense.
\end{hypothesis}

This hypothesis is reasonable for the following two reasons.
\begin{enumerate}
	\item Numerics show that a single pulse solution exists for a wide range of speeds $c$.
	\item We know that $\dim W^s(0) = \dim E^s = 2$ and $\dim W^u(0) = \dim E^u = 2$. Since we are in $\R^4$, $\dim H^{-1}(0) = 3$. Since the intersection $W^s(0) \cap W^u(0)$ must contain $q'(0; c_0)$, the derivative of the pulse, it must have at least dimension 1. Since $W^s(0) + W^u(0) \in \dim H^{-1}(0)$, we have
	\begin{align*}
	\dim( W^s(0) + W^u(0) ) &= \dim W^s(0) + \dim W^u(0) - \dim W^s(0) \cap W^u(0) \\
	&= 2 + 2 - 1 = 3 \\
	&\geq \dim H^{-1}(0)
	\end{align*}
	and $H^{-1}(0)$ is a 3-dimensional manifold near $Q(0; c_0)$.
\end{enumerate}

What does this hypothesis get us? Although the existence of a pulse solution does not require transverse intersection (any type of intersection is sufficient), transverse intersection makes it likely (although we still need to prove it) that the intersection of stable and unstable manifolds persists for $c$ near $c_0$. If the intersection were not transverse, we could have a solution for $c = c_0$, but the manifolds could separate when $c$ is perturbed away from $c_0$, in which case we would not have a solution for $c$ near $c_0$.\\

We will show that $q_c(x)$ exists and is exponentially localized in a series of lemmas.\\

\begin{lemma}\label{transverseint}
Assume Hypothesis \ref{transverseint} is met. Then there exists a constant $\delta > 0$ such that $c \in (c - \delta, c + \delta)$, there is a unique intersection of the stable and unstable manifolds $W^s(0; c)$ and $W^u(0; c)$. This intersection is transverse. Thus we still have a single-pulse (homoclinic orbit) for $c$ near $c_0$. This homoclinic orbit $Q(x; c)$ is differentiable with respect to $c$ at $c = c_0$.
\begin{proof}

By Hypothesis \ref{transverseint}, $W^u(0; c_0)$ and $W^s(0; c_0)$ intersect transversely in $H^{-1}(0)$ at the point $Q_0 = Q(0; c_0)$. By Lemma \ref{manifoldinH0}, the level set $H^{-1}(0)$ contains a 3-dimensional manifold $M(c_0)$ in a neighborhood of this intersection point. Since $W^u(0; c_0)$ and $W^s(0; c_0)$ intersect transversely in $H^{-1}(0)$ at this intersection point, this means that

\[
T_{Q_0} W^u(0; c_0) + T_{Q_0} W^s(0; c_0) = T_{Q_0} M
\]

The intersection of the tangent spaces of the manifolds $W^u(0; c_0)$ and $W^s(0; c_0)$ is thus one-dimensional, and is the span of the single vector $v^c = F(Q_0; c_0)$, where $F$ is the RHS of \eqref{nonlinearsystem} (i.e. the derivative of the homoclinic orbit put into a 4-dimensional system). Since the tangent spaces of stable and unstable manifolds are two-dimensional, each contains another basis vector (in addition to $v_c$.) Thus we have vectors $v^s$ and $v^u$ in $\R^3$ such that

\begin{align*}
T^s &= T_{Q_0} W^s(0; c_0) = \text{span}\{ v^s, v^c \} \\
T^u &= T_{Q_0} W^u(0; c_0) = \text{span}\{ v^u, v^c \} \\
\end{align*}

For convenience, for these three basis vectors let $V^i = \text{span }\{v^i\}$. Since in a neighborhood of $Q_0$, $M = H^{-1}(0)$ is a manifold, $M$ is isomorphic (via a smooth bijection with smooth inverse) to $R^3$. Thus we can without loss of generality take $M$ to be $R^3$ with basis given by $\{v^s, v^u, v^c\}$. \\

Now let $\Sigma$ be the plane passing through $Q_0$ normal to $v^c$. In other words, 

\[
\Sigma = Q_0 + \text{span}\{ v^s, v^u \} = Q_0 + \text{span}\{v_c\}^\perp
\]

Next we show that $W^u(0; c) \cap \Sigma$ is a smooth one-dimensional manifold for $c$ near $c_0$. To do this, we first note that $W^u(0; c)$ is a smooth two-dimensional manifold near $Q_0$. This follows from the following two facts:
\begin{enumerate} 
\item By the stable manifold theorm, the local unstable manifold $W^u_{loc}(0; c)$ has the same smoothness in $U$ and $c$ as $F$ does. Since $F$ is smooth ($C^\infty)$ in $U$ and $c$, so is $W^u_{loc}(0; c)$.
\item The evolution operator $\Phi(x; c)$ for \eqref{nonlinearsystem} has the same smoothness in $x$ and $c$ as $F$ does, thus $\Phi(x; c)$ is smooth in $c$.
\item The unstable manifold is the forward evolution of the local unstable manifold in $x$ under the evolution operator $\Phi(x; c)$.
\item Thus since both $W^u_{loc}(0; c)$ and $\Phi(x; c)$ are smooth, so is $W^u(0; c)$.
\end{enumerate}

Next, for $c \in (c_0 - \delta, c_0 + \delta)$, we write $W^u(0; c)$ over the tangent space $T^u$. Note that $T^u$ is the tangent space of $W^u(0; c)$ at $Q_0$, i.e. $c$ is fixed at $c_0$. Since $W^u(0; c)$ depends smoothly on $c$, we can find $R > 0$, $\delta > 0$, and a smooth function $h^u: T^u \times (c_0 - \delta, c_0 + \delta) \rightarrow V^s$ such that for $c \in (c_0 - \delta, c_0 + \delta)$

\begin{align*}
W^u(0; c) = Q_0 + \{ (u, h^u(u; c)) : u \in T^u \text{ with } |u| \leq R \}
\end{align*}

Note that if we take $c = c_0$ we have 

\begin{align*}
h^u(0; c_0) &= 0 \\
D_u h^u(0; c_0) &= 0
\end{align*}

where the derivative is 0 since at $c = c_0$ we have written the manifold $W^u(0; c)$ as a graph over its own tangent space. \\

Now we want to restrict this map to our planar section $\Sigma$. Since the map $(x, y) \mapsto x$ from $V^s \times V^c$ to $V^s$ is smooth, $W^u(0; c) \cap \Sigma$ is a smooth manifold which we can write as

\[
W^u(0; c) \cap \Sigma = Q_0 + \{ (u, h^u(u; c)) : u \in V^u \text{ with } |u| \leq R \}
\]

We can repeat all of the above for the unstable manifold and obtain a similar smooth function $h^s: T^s \times (c_0 - \delta, c_0 + \delta) \rightarrow V^u$. We then have for the intersection $W^s(0; c) \cap \Sigma$

\[
W^s(0; c) \cap \Sigma = Q_0 + \{ (h^s(v; c), v) : v \in V^s \text{ with } |u| \leq R \}
\]

Note that we have ordered the coordinates in $\Sigma$ as $(u, v)$, where $u \in V^u$ and $v \in V^s$.\\

Recall that for $c = c_0$, $W^s(0; c_0) \cap W^u(0; c_0) \cap \Sigma$ is the single point $Q_0$. All that is left is to show that for $c$ near $c_0$ we still have a point of intersection in $W^s(0; c) \cap W^u(0; c) \cap \Sigma$. In other words, for $c$ near $c_0$, we seek a point $(u, v) \in V^u \times V^s$ such that $(u, v)$ is contained in both $W^u(0; c) \cap \Sigma$ and $W^s(0; c) \cap \Sigma$. Using the expressions above, this is equivalent to

\[
Q_0 + (u, h^u(u; c)) = Q_0 + (h^s(v; c), v)
\]

which is satisfied if

\begin{align*}
u &= h^s(v; c) \\
v &= h^u(u; c)
\end{align*}

What we would like to do is use the IFT on this to get the $(u, v)$ coordinates of the unique intersection of $W^s(0; c_0) \cap \Sigma$ and $W^u(0; c_0) \cap \Sigma$ as a function of $c$ for $c$ near $c_0$. To to this, define a function $K: V^u \times V^s \times (c_0 - \delta, c_0 + \delta) \rightarrow V^u \times V^s$ by

\begin{equation}
K(u, v, c) = \begin{pmatrix}
u - H^s(v; c) \\ v - h^u(u; c)
\end{pmatrix}
\end{equation}

and note that $K(0, 0, c_0) = 0$. In order to use the IFT to write $(u, v)$ as a function of $c$, we need to check that the Jacobian matrix in the $(u, v)$ variables is invertible at $(u, v, c) = (0, 0, c_0)$.

\begin{align*}
D_{(u, v)}K(u, v, c) = \begin{pmatrix}
1 & -h^s_v(v; c) \\
-h^u_u(u; c) & 1
\end{pmatrix}
\end{align*}

Evaluating this at $(u, c) = (0, c_0)$ we get the invertible matrix

\begin{align*}
D_{(u, v)}K(0, 0, c_0) = \begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}
\end{align*}

where we used the fact that $D_v h^s(0; c_0) = D_u h^s(0; c_0) = 0$. Thus by the IFT we can find an open interval $U = (c_0 - \tilde{\delta}, c_0 + \tilde{\delta})$ with $0 < \tilde{\delta} \leq \delta$ and a unique smooth function $g: U \rightarrow V_u \times V_s$ such that

\begin{align*}
g(c_0) &= 0 \\
K(g(c), c) &= 0 \text{ for all } c \in U
\end{align*}

For $c \in U$, $P(c) = Q_0 + g(c)$ is the unique intersection point of $W^s(0; c) \cap W^u(0; c) \cap \Sigma$. (Uniqueness comes from the fact that this is a unique function of $c$, so for $c$ near $c_0$, there is exactly one $u = g(c)$ for which $K(g(c),c) = 0$). This intersection is transverse in $H(c)^{-1}(0)$ by uniqueness as well, since a single point of intersection in $\Sigma$ means that the intersection of $W^s(0; c)$ and $W^u(0; c)$ at $P(c)$ is one-dimensional. Since each manifold has one more dimension, the sum of their tangent spaces is three-dimensional.\\

Since we have a unique intersection of the stable and unstable manifolds for $c$ near $c_0$ at the point $P(c)$, we have a unique homoclinic orbit $Q(x; c)$ connecting the equilibrium at $U = 0$ to itself, which we can define by

\[
Q(x; c) = \Phi(x; c) P(c)
\]

where $\Phi(x; c)$ is the evolution operator of \eqref{nonlinearsystem}.\\

To show differentiability of $Q(0, c)$ at $c = c_0$, we note that near $c = c_0$,

\[
Q(0; c) = Q_0 + (g(c), h^u(g(c); c))
\]

which is smooth in $c$ since $g(c)$ and $h^u(u, c)$ are smooth in $c$.

\end{proof}
\end{lemma}

\begin{lemma}\label{qcexists}
$Q(x; c)$ is differentiable in $c$ for all $c$ near $c_0$ and for all $x$. 
\begin{proof}
Let $\Phi(x; c)$ be the solution operator for \eqref{nonlinearsystem}, where the initial condition is given at $x = 0$. Taking the initial condition to be $Q(0; c)$, by the property of the solution operator, for $c$ near $c_0$, $Q(x; c) = \Phi(x; c)Q(0; c)$.\\

By the existence and uniqueness theorem, $\Phi(x; c)$ is smooth in $c$ since the the RHS of \eqref{nonlinearsystem} is $C^\infty$ in $c$ (since $A(0; c)$ is a linear function of $c$ and $N(u)$ does not involve $c$ at all). By Lemma \ref{transverseint}, $Q(0; c)$ is smooth in $c$.
\end{proof}
\end{lemma}

Let $A(c) = A(0; c)$. Then $e^{A(c)x}$ is the fundamental matrix solution to the linear problem $U' = A(c) U$. Let $P^s(c)$ and $P^u(c)$ are the stable and unstable projections onto the stable and unstable eigenspaces of $A(c)$.\\

Since for $c > 0$ the matrix $A(c)$ is hyperbolic and since the eigenvalues of $A(c)$ are continuous functions of $c$ (in fact, we have explicit formulas for them, we can find $\alpha^s, \alpha^u > 0$ such that for all $c \in (c - \delta, c + \delta)$, where $\delta$ is the constant from Lemma \ref{transverseint}, all eigenvalues of $A(c)$ lie outside the open interval $(-\alpha^s, \alpha^u)$. For $c \in (c - \delta, c + \delta)$, we have the estimates

\begin{align*}
||e^{A(c)x}P^s(c)|| &\leq Me^{-\alpha^s x} && x \geq 0\\
||e^{A(c)x}P^u(c)|| &\leq Me^{\alpha^u x} && x \leq 0
\end{align*}

We are now in position to show that the derivative $q_c(x)$ of the single pulse with respect to the speed $c$ is exponentially localized.

\begin{lemma}\label{qc}
For the stationary, single-pulse solution $q(x; c)$ to the 5th order KdV equation in a frame moving with velocity $c$, the derivative $q_c(x)$ of the pulse with respect to $c$ is exponentially localized, thus $q_c(x) \in L^2(\R)$.
\begin{proof}
$q_c(x)$ exists for $c$ near $c_0$ by Lemma \ref{qcexists}, so we need to show that it is exponentially localized.\\

We proceed as in the proof of the stable manifold theorem (parameter-dependent version). We use an exponentially weighted function space $X_\eta$, defined above, and the exponential weight $\eta > 0$ will be chosen later.\\

Next we write \eqref{nonlinearsystem} in integrated form. For convenience, let $A(c) = A(0; c)$, so that the matrix exponential $e^{A(c)x}$ is the fundamental matrix solution to the linear problem $U' = A(c) U$. Let $P^s(c)$ and $P^u(c)$ be the stable and unstable projections onto the stable and unstable eigenspaces of $A(c)$. Then as long as $|U(x)|$ is uniformly bounded for $x \geq 0$, say $|U(x)| \leq \rho$ for $x \geq 0$, then $U(x)$ is given by

\[
U(x) = e^{A(c)x} P^s(c) a + \int_\infty^x e^{A(c)(x - y)}P^u(c) N(U(y))dy + \int_0^x e^{A(c)(x - y)}P^s(c) N(U(y))dy
\]

where $a$ is the initial condition. Note that $U$ will depend on $c$, but that (our specific case), $N$ does not.\\

First we define the spaces

\begin{align*}
B_1 &= (c_0 - \delta, c_0 + \delta) \\
B_2 &= \{ a \in \R^n : |P^s(c) a| \leq \rho/2M \text{ for all } c \in B_1\} \\
D &= \{ u \in X_\eta : ||u||_\eta \leq \rho \}
\end{align*}

where $\delta$ is the constant from Lemma \ref{transverseint}. Next define
\[
L_\eta(\rho) = \max \{ || DN(u) || : ||u||_\eta \leq \rho \}
\]

Note that since we are on the domain $[0, \infty)$ and $\eta > 0$, $|u(x)| \leq |u(x)e^{\eta x}|$ and so $||u|| \leq ||u||_\eta$. Since $N \in C^1$ and $||u|| \leq \rho$, $L(\rho)$ is well-defined and finite. (In this case, we know exactly what $N$ is. The Jacobian matrix $DN$ consists entirely of zeros except for one entry which is $2 u_1$, so here we have $L(\rho) \leq 2 \rho$.) From this, we can get a bound on $|N(U(x))|$ for $U$ with $||U||_\eta \leq \rho$. Let $U \in D$ and fix $x \geq 0. $Since $N \in C^1$ and $N(0) = 0$,

\begin{align*}
|N(U(x))| &= |N(U(x)) - N(0)| \\
&= \left| \int_0^1 DN(t U(x)) t U(x) du \right| \\
&\leq  \int_0^1 ||DN(tU(x))|| \: |U(x)| t dt \\
&\leq L(\rho) |U(x)|
\end{align*}

Multiplying both sides by $e^{\eta x}$ gives us the bound (pointwise in $x$, for $x \geq 0$)

\[
|N(U(x)) e^{\eta x}| \leq L(\rho)|U(x) e^{\eta x}|
\]

taking the supremum in $x$ gives us the bound

\begin{equation}\label{NUetabound}
||N(U)||_\eta \leq L(\rho) ||U||_\eta \leq L(\rho) \rho
\end{equation}

where everything is finite since we are assuming $U \in D$.\\

Note that since $DN(0) = 0$ and $N \in C^1$, $DN(u) \rightarrow 0$ as $u \rightarrow 0$, thus by the definition of $L(\rho)$, $L(\rho) \rightarrow 0$ as $\rho \rightarrow 0$. We also have for $U, V$ with $||U||_\eta, ||V||_\eta \leq \rho$

\[
||N(U) - N(V)||_\eta \leq L(\rho)(||U||_\eta - ||V||_\eta) 
\]
which is a consequence of the mean value inequality.\\

Now we define the map $F: D \times B_1 \times B_2 \rightarrow X_\eta$ by

\begin{equation}\label{F}
[F(U, c, a)](x) = e^{A(c)x} P^s(c) a + \int_\infty^x e^{A(c)(x - y)}P^u(c) N(U(y))dy + \int_0^x e^{A(c)(x - y)}P^s(c) N(U(y))dy
\end{equation}

First we show that $F$ is well-defined, i.e. it actually maps into $X_\eta$. We will look at each term on the RHS separately. Recall that since the domain of $F$ is $D$, we always have $||U||_\eta \leq \rho$.

\begin{align*}
|e^{\eta x} e^{A(c)x } P^s(c) a | &\leq M e^{\eta x} e^{-\alpha^s x} | P^s(c) a |\\
&= M e^{(\eta - \alpha^s)x} | P^s(c) a|
\end{align*}
For this to be uniformly bounded in $x$, we require $\eta \leq \alpha^s$, in which case we have $||e^{A(c)x}a ||_\eta \leq M| P^s(c) a |$.

\begin{align*}
\left| e^{\eta x} \int_\infty^x e^{A(c)(x - y)}P^u(c) N(U(y))dy \right| &= \left| \int_\infty^x e^{\eta x} e^{A(c)(x - y)}P^u(c) N(U(y))dy \right|\\
&\leq \int_x^\infty M e^{\eta x}e^{\alpha^u(x -y)}|N(U(y))|dy \\
&= M \int_x^\infty e^{\eta (x - y)}e^{\alpha^u(x -y)}| e^{\eta y} N(U(y))|dy \\
&\leq M \int_x^\infty e^{(\eta+\alpha^u)(x - y)} || N(U)||_\eta dy \\
&\leq M L(\rho) ||U||_\eta \int_x^\infty e^{(\eta+\alpha^u)(x - y)} dy \\
&= M L(\rho) ||U||_\eta \frac{1}{\alpha^u + \eta}
\end{align*}

Since $\eta$ and $\alpha^u$ are both positive, this imposes no additional restrictions on $\eta$.

\begin{align*}
\left| e^{\eta x} \int_0^x e^{A(c)(x - y)}P^s(c) N(U(y)) \right| &= \left| \int_0^x e^{\eta x} e^{A(c)(x - y)}P^s(c) N(U(y)) dy \right|\\
&\leq \int_0^x M e^{\eta x}e^{-\alpha^s(x -y)}|N(U(y))|dy \\
&= M \int_0^x e^{\eta (x - y)}e^{-\alpha^s(x -y)}| e^{\eta y} N(U(y))|dy \\
&\leq M \int_0^x e^{(\eta-\alpha^s)(x - y)} || N(U)||_\eta dy \\
&\leq M L(\rho) ||U||_\eta \int_0^x e^{(\eta-\alpha^s)(x - y)} dy \\
&= M L(\rho) ||U||_\eta \frac{1 - e^{(\eta-\alpha^s)x} }{\alpha^s - \eta}
\end{align*}

Since $x \geq 0$, as long as $\eta < \alpha^s$ (we have this condition from above), $0 < e^{(\eta-\alpha^s)x} < 1$, thus we have

\[
\left| e^{\eta x} \int_0^x e^{A(c)(x - y)}P^s(c) N(U(y)) \right| \leq M L(\rho) ||U||_\eta \frac{1}{\alpha^s - \eta}
\]

Putting all of this together and taking the supremum over $x \in [0, \infty)$, we have for $||U||_\eta \leq \rho$

\begin{equation}
||F(U, c, a)](x)||_\eta \leq M |a| + M L(\rho) \left( \frac{1}{\alpha^u+\eta}+\frac{1}{\alpha^s-\eta} \right) ||U||_\eta
\end{equation}

Since everything on the RHS is finite, we have shown that the codomain of $F$ is in fact $X_\eta$, so $F$ is well-defined. Since $a \in B_2$, $|P^s(c) a| \leq \rho/2M$. Since $L(\rho) \rightarrow 0$ as $\rho \rightarrow 0$, we can choose $\rho$ sufficiently small so that $L(\rho) < \frac{1}{2M} \left( \frac{1}{\eta+\alpha^u}+\frac{1}{\eta-\alpha^s} \right)^{-1}$. Having done this, and using the fact that $||U||_\eta \leq \rho$, we obtain the bound $||F(U, a, c)](x)||_\eta \leq \rho$. Thus for this choice of $\rho$ we actually have $F: D \times B_2 \times B_1 \rightarrow D$, which is what we want.\\

Now we need to show that $F: D \times B_2 \times B_1 \rightarrow D$ is a uniform contraction. For $U, V \in D$, following what we did above, 

\begin{align*}
| &e^{\eta x} ( F(U, c, a) - F(V, c, a) ) | \\
&= \left| \int_\infty^x e^{\eta x} e^{A(c)(x - y)}P^u(c) [N(U(y)) - N(V(y))]dy + \int_0^x e^{\eta x} e^{A(c)(x - y)}P^s(c)[N(U(y)- N(V(y))]dy \right| \\
&\leq M L(\rho) \left( \int_x^\infty e^{(\eta + \alpha^u)(x-y)}||U - V||_\eta dy + \int_0^x e^{(\eta - \alpha^s)(x-y)}||U - V||_\eta dy \right) \\
&= ML(\rho)||U - V||_\eta \left( \frac{1}{\eta + \alpha^u}+\frac{1}{\eta-\alpha^s} \right) \\
&\leq \frac{1}{2} ||U - V||_\eta 
\end{align*}

where in the last line we used our choice of $\rho$ from above. Since this is independent of $c$ and $a$, $F$ is a uniform contraction. By the uniform contraction mapping principle, there is a unique map $G: B_1 \times B_2 \rightarrow D$ such that $F(G(c, a), c, a) = G(c, a)$ for all $c \in B_1$ and $a \in B_2$. In other words, $G$ maps the pair of parameters $(c, a)$ to the unique fixed point of $F$ with those parameters. By the uniform contraction mapping principle, the maps $G$ and $F$ have the same smoothness in the parameters $c$ and $a$.\\

What does this get us? We note the following

\begin{enumerate}
	\item For each speed $c \in B_1$ and initial condition $a \in B_2$, $u(x) = G(c, a)(x)$ is the unique solution on $[0, \infty)$ to KdV5 with speed $c$ and initial condition $a$.

	\item $G(c, a)(x) \in D$, which implies $\sup_{x \in [0, \infty)} |e^{\eta x} G(c, a)(x)| \leq \rho$. In other words, for all $c \in B_1$, $a \in B_2$, and $x \geq 0$
	\begin{equation}
		|G(c, a)(x)| \leq \rho e^{-\eta x}
	\end{equation}
	Thus for these $a$ and $c$, the unique solution $G(c, a)(x)$ is uniformly exponentially bounded on $[0, \infty)$.

	\item $G(c, a)$ is as smooth as $F$ in $c$ and $a$.
\end{enumerate}

The key is this last property (smoothness). We would like the map $G$ to be differentiable in $c$. Let's first show that is indeed what we want. Then we will show that $F$ is smooth in $c$, implying $G$ is as well.\\

Recall that for $c = c_0$, a homoclinic orbit $q(x; c_0)$ exists. By the stable manifold theorem, since $q(x; c_0)$ is contained in the stable manifold $W^s(0; c_0)$, we can find $\tilde{x} \geq 0$ such that $q(x; c_0) \leq \rho$ for all $x \geq \tilde{x}$. Let $a_0 = q(\tilde{x}; c_0)$ be our initial condition. Then by uniqueness of the map $G$, we have 

\[
G(c_0, a_0)(x) = q(\tilde{x} + x; c_0)
\]

Taking the derivative with respect to $c$,

\[
G_c(c_0, a_0)(x) = q_c(\tilde{x} + x; c_0)
\]

For fixed $a_0$, $G$ maps $B_1 \rightarrow X_\eta$. The Frechet derivative $G_c$ is a bounded linear map from the tangent space of $B_1$ (which is $\R$, since $B_1 \subset \R$) to the tangent space of $X_\eta$ (which is isomorphic to $X_\eta$ itself since $X_\eta$ is a Banach space). We thus have for a perturbation $h \in R$

\[
G_c(c_0, a_0)h = q_c(\tilde{x} + \cdot \: ; c_0) h
\]

Thus since $G_c(c_0, a_0)$ is a bounded linear operator, $q_c(\tilde{x} + \cdot \: ; c_0) \in X_\eta$, and so the function $q_c(\tilde{x} + x ; c_0)$ decays exponentially in $x$. Since all we care about is asymptotic behavior, we conclude that $q_c(x ; c_0)$ decays exponentially in $x$.\\

All we need to do to complete the proof is show that the map $F$ defined in \eqref{F} is differentiable in $c$. Note that $F: D \times B_1 \times B_2 \rightarrow X_\eta$. Since the codomain is an exponentially weighted function space with exponentially weighted norm, we need to show that $F$ is Frechet differentiable in $c$ with respect to this norm. We will do this in a series of lemmas which follow this one. \\

We can similarly show (by, say, replacing $x$ with $-x$), that $q_c(x)$ at $c = c_0$ decays exponentially as $x \rightarrow -\infty$. From this, we will have the additional condition that $\eta < \alpha^u$. Thus as long as $0 < \eta < \alpha^s, \alpha^u$, we are all set, and the proof is complete.

\end{proof}
\end{lemma}

We now prove that the map $F: D \times B_1 \times B_2 \rightarrow X_\eta$ defined in \eqref{F} is Frechet differentiable with respect to $c$. We do this in a series of lemmas.\\

First, we prove a lemma about Frechet derivatives in exponentially weighted spaces with respect to a parameter under a change of variables dependent on that parameter. I'm sure this is known, but stuff on Frechet derivatives is hard to find.\\

\begin{lemma}\label{frechetfun}
Let $F(c): \R \rightarrow X_\eta$, where $X_\eta$ is the weighted exponential space

\[
X_\eta = \{ f \in C^0([0, \infty), \R^n) : \sup_{x \in [0, \infty)} |e^{\eta x} f(x)| < \infty \}
\]

with norm given by

\[
||f||_\eta = \sup_{x \in [0, \infty)} |e^{\eta x} f(x)|
\]

Suppose $F$ has a Frechet derivative $L$ at parameter value $c$, i.e. $L$ is a bounded linear operator from $\R \rightarrow X_\eta$ such that 
\[
\lim_{h \rightarrow 0} \frac{|| F(c+h) - F(c) - Lh ||_\eta }{|h|} = 0
\]

\begin{enumerate}[(i)]

\item Let $T(c)$ be an $n \times n$ matrix which is smooth in the parameter $c$. Then $T(c)F(c): \R \rightarrow X_\eta$ has Frechet derivative $T'(c)F(c) + T(c)L$ at $c$.

\item Let $S(c)$ be an $n \times n$ matrix which is smooth in the parameter $c$. Then $F(c)S(c): \R \rightarrow X_\eta$ has Frechet derivative $L S(c) + F(c)S'(c)$ at $c$.

\item Let $S(c) \in GL(n, \R)$ be a family of smooth invertible matrices paramaterized by $c$, where $S(c)$ is smooth and invertible in an open interval around a specific parameter value $c$. Then $S^{-1}(c)F(c)S: \R \rightarrow X_\eta$ has Frechet derivative $(S^{-1})'(c) F(c) S(c) + S^{-1}(c) L S (c) + S^{-1}(c) F(c) S'(c)$ at $c$.

\end{enumerate}

\begin{proof}

Let $||F(c)||_\eta = M$.\\

For (i), since by property of the norm, $||T(c)F(c)||_\eta = |T(c)| ||F(c)||_\eta = |T(c)|M$, $T(c)F(c)$ has codomain $X_\eta$ as claimed.

\begin{align*}
\lim_{h \rightarrow 0}&\frac{||T(c+h)F(c+h) - T(c)F(c) - (T'(c)F(c) + T(c)L)h||_\eta}{|h|} \\
&= \lim_{h \rightarrow 0}\frac{||T(c+h)F(c+h) - T(c)F(c+h) + T(c)F(c+h) - T(c)F(c) - (T'(c)F(c) + T(c)L)h||_\eta}{|h|} \\
&\leq \lim_{h \rightarrow 0}\frac{||T(c+h)F(c+h) - T(c)F(c+h) - T'(c)F(c) h ||_\eta}{|h|} \\
&\:\:+ \lim_{h \rightarrow 0}\frac{||T(c)F(c+h) - T(c)F(c) - T(c)L)h||_\eta}{|h|}
\end{align*}
We will evaluate the two limits on the RHS separately. For the second limit, we have

\begin{align*}
\lim_{h \rightarrow 0}&\frac{||T(c)F(c+h) - T(c)F(c) - T(c)L)h||_\eta}{|h|} \\
&= |T(c)| \lim_{h \rightarrow 0}\frac{||F(c+h) - F(c) - L)h||_\eta}{|h|} \\
&= 0
\end{align*}
since $|T(c)|$ is a constant, and the remaining limit is the definition of the Frechet derivative of $F(c)$ at $c$, thus is zero. For the first limit on the RHS,

\begin{align*}
\lim_{h \rightarrow 0}&\frac{||T(c+h)F(c+h) - T(c)F(c+h) - T'(c)F(c) h ||_\eta}{|h|} \\
&= \lim_{h \rightarrow 0}\frac{||T(c+h)F(c+h) - T(c)F(c+h) - T'(c)F(c+h)h + T'(c)F(c+h)h - T'(c)F(c) h ||_\eta}{|h|} \\
&\leq \lim_{h \rightarrow 0}\frac{||T(c+h)F(c+h) - T(c)F(c+h) - T'(c)F(c+h)h||_\eta}{|h|} + \lim_{h \rightarrow 0} \frac{||T'(c)F(c+h)h - T'(c)F(c) h ||_\eta}{|h|} \\
&\leq \lim_{h \rightarrow 0} \left| \frac{T(c+h) - T(c)}{h} - T'(c) \right| ||F(c+h)||_\eta + |T'(c)| \lim_{h \rightarrow 0} ||F(c+h) - F(c) ||_\eta \\
&= 0 \cdot M + |T'(c)| \cdot 0 \\
&= 0
\end{align*}

where we used the fact that $F$ is continuous as a function from $\R$ to $X_\eta$ (since it is Frechet differentiable) and that the norm is a continuous function, so we can pull the limit inside it. Thus we have proved (i).\\

For (ii), the proof is similar. For (iii), take $T(c) = S^{-1}(c)$ in (i), then use (ii).

\end{proof}
\end{lemma}

In the next lemma, we exhibit a property of 

\begin{lemma}\label{expproj}
If $A$ is an $n \times n$ matrix and $P$ is a projection on $\R^n$, 
\[
e^{Ax} P = e^{APx} P 
\]
Note that this does not require $P$ to commute with $A$.
\begin{proof} 
Using the fact that for a projection $P^2 = P$, thus $P^n = P$ for all $n$,
\begin{align*}
e^{Ax} P &= \left( \sum_{n = 0}^\infty \frac{ A^n x^n }{n!} \right) P \\
&= \left(1 + Ax + \frac{A^2 x^2}{2!} + \cdots \right) P \\
&= \left(P + AP^2 x + \frac{A^2 P^3 x^2}{2!} + \cdots \right)\\
&= \left(1 + AP x + \frac{A^2 P^2 x^2}{2!} + \cdots \right) P \\
&= \left( \sum_{n = 0}^\infty \frac{ [ A P x]^n}{n!} \right) P \\
&= e^{AP x} P
\end{align*}
\end{proof}
\end{lemma}

This next lemma seems irrelevant, but be used in the proof of the lemma following it.

\begin{lemma}\label{expineq}
For
\[
f(x) = \frac{e^x - x - 1}{x^2},
\]

\begin{enumerate}[(i)]
	\item $\lim_{x \rightarrow 0} f(x) = 1/2$
	\item $f(x) \geq 0$ for all $x$
	\item $f(x) \leq e^x$ for $x \geq 0$
	\item $f(x) \leq 1$ for $x \leq 0$
	\item $f(x) \leq 1 + e^x$ for all $x$
\end{enumerate}
\begin{proof}
These all make sense visually, but a rigorous proof follows.
\begin{enumerate}[(i)]
	\item Use L'Hopital's rule twice. $f(x)$ is continuous everywhere except at $x = 0$, which is a removable discontinuity.
	\item It suffices to look at the numerator since the denominator is nonnegative. Let $g(x) = e^x - x - 1$ and note that $g(0) = 0$ and $g'(x) = e^x - 1$. For $x < 0, g'(x) < 0$ and for $x > 0, g'(x) > 0$, thus $g(0) = 0$ is the minimum of $g$, and so $g(x)$ and $f(x) \geq 0$ for all $x$.
	\item 
	\[
	e^x - f(x) = \frac{1 + x + e^x (x^2 - 1)}{x^2}
	\]
	Let $g(x) = 1 + x + e^x (x^2 - 1)$ be the numerator of this. For $x \geq 0$, $e^x (x^2 - 1) \geq -1$, thus $g(x) \geq 0$ and so $e^x - f(x) \geq 0$ for $x \geq 0$.
	\item There are probably easier ways to show this, but this should work
	\[
	1 - f(x) = \frac{x^2 - e^x + x + 1)}{x^2}
	\]
	As before, let $g(x) = x^2 - e^x + x + 1$ be the numerator of this. We have $g(0) = 0$ and would like to show $g'(x) \leq 0$ for $x \leq 0$, from which it would follow that $g(x) \geq 0$ for $x 
	\leq 0$ since it would then decrease to 0 from the left. $g'(x) = 2x - e^x + 1$. We have $g'(0) = 0$ as well, and to show that $g'(x) \leq 0$ for all $x \leq 0$, it suffices to show that $g''(x) \geq 0$ for all $x \leq 0$ since then $g'(x)$ would increase to 0 from the left. $g''(x) = 2 - e^x \geq 0$ for $x \leq 0$, so our assertion is proved.
	\item This follows from the previous two.
\end{enumerate}
\end{proof}
\end{lemma}

We are in position to show Frechet differentiability of the function $F$ in \eqref{F}.

\begin{lemma}\label{derivatives2}
Let $0 < \eta < \alpha^s, \alpha^u$. For $c > 0$, the function $F: D \times B_1 \times B_2 \rightarrow X_\eta$ defined by 

\begin{equation}\label{F}
[F(U, c, a)](x) = e^{A(c)x} P^s(c) a + \int_\infty^x e^{A(c)(x - y)}P^u(c) N(U(y))dy + \int_0^x e^{A(c)(x - y)}P^s(c) N(U(y))dy
\end{equation}

is Frechet differentiable in $c$. The spaces $D$, $B_1$, and $B_2$ are defined in Lemma \ref{qc}.

\begin{proof}

Since $F$ is a the sum of three functions, we will show that the following three functions are Frechet differentiable in $c$.

\begin{enumerate}[(i)]
	\item $e^{A(c)x} P^s(c)$
	\item $\int_\infty^x e^{A(c)(x - y)}P^u(c) N(U(y))dy$
	\item $\int_0^x e^{A(c)(x - y)}P^s(c) N(U(y))dy$
\end{enumerate}

Note that since the initial condition $a$ is constant, we have removed it from (i) since it does not affect the derivative at all. In all three cases, we come up with candidate for the Frechet derivative using regular differentiation, and then show that it is also the Frechet derivative using the definition of the Frechet derivative. Recall that in the definition of $X_\eta$, the domain of the functions is $[0, \infty)$.\\

First, we look at (i). To do this, we employ the following change of variables. Recall that the two stable eigenvalues of $A(c)$ are designated $\lambda^s_1(c)$ and $\lambda^s_1(c)$. Their corresponding eigenvectors are $v^s_1(c)$ and $v^s_2(c)$. The unstable eigenvalues are $\lambda^u_1(c)$ and $\lambda^u_1(c)$, with corresponding eigenvectors $v^u_1(c)$ and $v^u_2(c)$. We are assuming $c > 0, c \neq 1/4$ so that the eigenvalues are distinct, thus the eigenvectors are linearly independent. \\

Let $S(c) = (v^s_1(c) | v^s_2(c) | v^u_1(c) | v^u_2(c))$, i.e. the matrix whose columns are the eigenvectors of $A(c)$. Then $S(c)^{-1} A(c) S(c) = D(c)$, where $D(c)$ is the diagonal matrix

\[
D(c) = \begin{pmatrix}
\lambda^s_1(c) &&& \\ & \lambda^s_2(c)&& \\ && \lambda^u_1(c) & \\ &&& \lambda^u_2(c) 
\end{pmatrix}
\]

It is easy to show (and is known from an ODE course) that $S(c)^{-1} e^{A(c)x} S(c) = e^{D(c) x}$. In the eigenbasis, the stable projection $P^s(c)$ onto $E^s(c)$ has a nice form which is independent of $c$.

\[
S(c)^{-1} P^s(c) S(c) = P_1 = \begin{pmatrix}
1 &&& \\ & 1 && \\ && 0 & \\ &&& 0 
\end{pmatrix}
\]

So we have
\begin{align*}
S(c)^{-1} e^{A(c) x} P^s(c) S(c) = S(c)^{-1} e^{A(c) x} S(c)^{-1} S(c) P^s(c) S(c) = e^{D(c) x} P_1
\end{align*}

Next, note that by the same argument as above, using the known formula for the matrix exponential of a diagonal matrix and Lemma \ref{expproj},

\begin{align*}
e^{D(c) x} P_1 &= e^{ D(c)P_1 x} P_1 = 
\begin{pmatrix}
e^{\lambda^s_1(c) x} &&& \\ & e^{\lambda^s_2(c) x} && \\ && 0 & \\ &&& 0 
\end{pmatrix}
\end{align*}

We can take the derivative with respect to $c$ of $e^{D(c) x}$:

\begin{align*}
\frac{\partial}{\partial c} e^{D(c) P_1 x} &=
\begin{pmatrix}
e^{\lambda^s_1(c) x} (\lambda^s_1)'(c) &&& \\ & e^{\lambda^s_2(c) x} (\lambda^s_2)'(c) && \\ && 0 & \\ &&& 0 
\end{pmatrix} \\
&= \begin{pmatrix}
e^{\lambda^s_1(c) x} &&& \\ & e^{\lambda^s_2(c) x}  && \\ && 0 & \\ &&& 0 
\end{pmatrix}
\begin{pmatrix} (\lambda^s_1)'(c) x &&& \\ & (\lambda^s_2)'(c) x && \\ && 0 & \\ &&& 0 \end{pmatrix}  \\
&= e^{D(c) P_1 x} P_1 \Lambda'(c) x
\end{align*}

where

\[
\Lambda(c) = \begin{pmatrix} \lambda^s_1(c) &&& \\ & \lambda^s_2(c)  && \\ 
&& \lambda^u_1(c)  & \\ &&& \lambda^u_2(c)  \end{pmatrix}
\]

Thus since $P_1$ is constant, we have

\begin{equation}\label{frechetcandidate}
\frac{\partial}{\partial c} e^{D(c) P_1 x} P_1 = e^{D(c) P_1 x} P_1 \Lambda'(c) x P_1
\end{equation}

Since $e^{A(c)x }P(c) = S(c) e^{D(c) x} P_1 S^{-1}(c)$, by Lemma \ref{frechetfun}, it suffices to show that $e^{D(c) x} P_1 = e^{D(c) P_1 x} P_1$ has a Frechet derivative. We will use \eqref{frechetcandidate} as our candidate. Note that as long as $0 < \eta < \alpha^s$, this is a bounded linear transformation from $\R$ to $X_\eta$ since

\begin{align*}
|e^{\eta x} e^{D(c) P_1 x} P_1 \Lambda'(c) x P_1| &\leq e^{\eta x} e^{-\alpha^s x} x |\Lambda'(c)||P_1| \\
&= e^{(\eta -\alpha^s) x} x |\Lambda'(c)| \\
&\leq C 
\end{align*}
since $|\Lambda'(c)|$ is constant, and $e^{(\eta -\alpha^s) x} x$ is bounded on $[0, \infty)$ as long as $0 < \eta < \alpha^s$.\\

We need to show that

\[
\lim_{h \rightarrow 0} \frac{|| e^{D(c+h) P_1 x} P_1 - e^{D(c) P_1 x} P_1 - e^{D(c) P_1 x} P_1 \Lambda'(c) x P_1||_\eta}{|h|} = 0
\]

Note that this is a diagonal matrix with only the first two entries nonzero. Thus it suffices to show that for $i = 1, 2$

\[
\lim_{h \rightarrow 0} \frac{|| e^{\lambda^s_i(c+h) x} - e^{\lambda^s_i(c) x} 
- e^{\lambda^s_i(c) x} (\lambda^s_i)'(c)x h ||_\eta}{|h|} = 0
\]

For convenience, we will let $\lambda(c) = \lambda^s_i(c)$. Using the Taylor theorem on $\lambda(c+h)$, we have
\[ 
\lambda(c+h) = \lambda(c) + \lambda'(c)h + r(h)h
\]
where the remainder term $r(h) \rightarrow 0$ as $h \rightarrow 0$.

\begin{align*}
\frac{e^{\eta x}| e^{\lambda(c+h) x} - e^{\lambda(c) x} - e^{\lambda(c) x} \lambda'(c)x h|}{|h|} &=
 \frac{e^{\eta x}| e^{(\lambda(c) + \lambda'(c)h + r(h)h) x} - e^{\lambda(c) x} - e^{\lambda(c) x} \lambda'(c)x h|}{|h|} \\
&= \frac{e^{\eta x} e^{\lambda(c)x}| e^{\lambda'(c)hx} e^{r(h)h x} - 1 - \lambda'(c)x h|}{|h|} 
\end{align*}

We also expand $e^{r(h)h x}$ using the Taylor theorem to one term (i.e. the ordinary mean value theorem)

\[
e^{r(h)h x} = 1 + e^{r(h)h\xi}r(h)hx
\]
where $\xi \in [0, x]$. Substituting this in, we get

\begin{align*}
&\frac{e^{\eta x}| e^{\lambda(c+h) x} - e^{\lambda(c) x} - e^{\lambda(c) x} \lambda'(c)x h|}{|h|}\\
&= \frac{e^{\eta x} e^{\lambda(c)x}| e^{\lambda'(c)hx}( 1 + e^{r(h)h\xi}r(h)hx )- 1 - \lambda'(c)x h|}{|h|} \\
&\leq \frac{e^{\eta x} e^{\lambda(c)x}| e^{\lambda'(c)hx} - 1 - \lambda'(c)x h|}{|h|} + \frac{e^{\eta x} e^{\lambda(c)x} e^{\lambda'(c)hx} e^{r(h)h\xi} | r(h)h x|}{|h|} \\
&= \frac{e^{\eta x} e^{\lambda(c)x}| e^{\lambda'(c)hx} - 1 - \lambda'(c)x h|}{|h|} + e^{\eta x} e^{\lambda(c)x} e^{\lambda'(c)hx} e^{r(h)h\xi}| r(h)| x \\
&\leq \frac{e^{\eta x} e^{\lambda(c)x}| e^{\lambda'(c)hx} - 1 - \lambda'(c)x h|}{|h|} + e^{\eta x} e^{\lambda(c)x} e^{\lambda'(c)hx} (1 + e^{r(h)h x}) | r(h)| x  \\
&\leq \frac{e^{\eta x} e^{\lambda(c)x}| e^{\lambda'(c)hx} - 1 - \lambda'(c)x h|}{|h|} + e^{\eta x} e^{\lambda(c)x} e^{\lambda'(c)hx} x | r(h)| + e^{\eta x} e^{\lambda(c)x} e^{\lambda'(c)hx} e^{r(h)h x} x | r(h) |
\end{align*}
where in the penultimate line we used the fact that the exponential function is monotonic, so since $\xi \in [0, x]$, $e^{r(h)h\xi}$ is bounded by either $e^{r(h)h x}$ or 1 depending on whether $r(h)h$ is positive or negative. Thus $e^{r(h)h\xi} \leq 1 + e^{r(h)h x}$.\\

All that remains is to show each of the three terms goes to 0 uniformly in $x$ as $h \rightarrow \infty$. First, we choose $\eta$ so that $0 \leq \eta \leq \text{Re} \lambda_1^s(c), \text{Re} \lambda_2^s(c)$, so we have $0 \leq \eta \leq \lambda$ here. Then let $a = \lambda - \eta$. Note that $\lambda'(c)$ is just some constant.\\

For the second term and third terms, choose $h$ sufficiently small so that $|\lambda'(c) h| \leq a/4$ and  $|r(h)h| \leq a/4$. (Recall that $r(h) \rightarrow 0 $ as $h \rightarrow 0$, so we can do this.) For the second term, we have

\begin{align*}
e^{\eta x} e^{\lambda(c)x} e^{\lambda'(c)hx} x | r(h)| &= e^{(\eta + \lambda(c) + \lambda'(c)h)x}x|r(h)| \\
&= e^{(\eta + \lambda(c) + a/4 )x}x|r(h)| \\
&= e^{-\alpha x}x|r(h)|
\end{align*}
where $\alpha = \eta + \lambda(c) + a/2  < 0$. Since $e^{-\alpha x}x$ is bounded in $x$ and $|r(h)| \rightarrow 0$ as $h \rightarrow \infty$, this term decays to 0 uniformly in $x$.\\

For the third term, we have

\begin{align*}
e^{\eta x} e^{\lambda(c)x} e^{\lambda'(c)hx} e^{r(h)h x} x |r(h)| &= e^{(\eta + \lambda(c) + \lambda'(c)h + r(h)h)x}x|r(h)| \\
&= e^{(\eta + \lambda(c) + a/4 + a/4)x}x|r(h)| \\
&= e^{(\eta + \lambda(c) + a/2)x}x|r(h)| \\
&= e^{-\alpha x}x|r(h)|
\end{align*}

where $\alpha = \eta + \lambda(c) + a/2  < 0$ is the same as above. Since $e^{-\alpha x}x$ is bounded in $x$ and $|r(h)| \rightarrow 0$ as $h \rightarrow \infty$, this term decays to 0 uniformly in $x$ as well.\\

Finally, for the first term, we have

\begin{align*}
\frac{e^{\eta x} e^{\lambda(c)x}| e^{\lambda'(c)hx} - 1 - \lambda'(c)x h|}{|h|} &= \frac{1}{|h|}e^{\eta x} e^{\lambda(c)x} \left| 1 + \lambda'(c)xh + \sum_{n = 2}^\infty \frac{(\lambda'(c)xh)^n}{n!} - 1 - \lambda'(c)xh\right|\\
&= \frac{1}{|h|}e^{\eta x} e^{\lambda(c)x} \left| \sum_{n = 2}^\infty \frac{(\lambda'(c)xh)^n}{n!} \right| \\
&= e^{\eta x} e^{\lambda(c)x} \left| \sum_{n = 2}^\infty \frac{(\lambda'(c)x)^n h^{n-1}}{n!} \right| \\
&= e^{\eta x} e^{\lambda(c)x} \left| \sum_{n = 0}^\infty \frac{(\lambda'(c)x)^{n+2} h^{n+1}}{(n+2)!} \right|\\
&= e^{\eta x} e^{\lambda(c)x} x^2 |\lambda'(c)|^2 |h| \left| \sum_{n = 0}^\infty \frac{(\lambda'(c)x)^{n} h^{n}}{(n+2)!} \right|\\
&= e^{\eta x} e^{\lambda(c)x} x^2 |\lambda'(c)|^2 |h| \left( \frac{e^{\lambda'(c)xh} - \lambda'(c)xh - 1}{(\lambda'(c)xh)^2} \right)
\end{align*}

The infinite sum was evaluated using Mathematica, but it is easy to verify manually. First, we note that as we take $h \rightarrow 0$, the term in parentheses above (the closed form for the sum) presents no problem by Lemma \ref{expineq}(i). Since the closed form expression for this sum is annoying, we will use the inequality in Lemma \ref{expineq}(v).

\begin{align*}
\frac{e^{\eta x} e^{\lambda(c)x}| e^{\lambda'(c)hx} - 1 - \lambda'(c)x h|}{|h|} &\leq e^{\eta x} e^{\lambda(c)x} x^2 |\lambda'(c)|^2 |h| \left( 1 + e^{\lambda'(c)xh} )\right) \\
&= |\lambda'(c)|^2  \left( e^{(\eta + \lambda(c)) x}x^2 + e^{(\eta + \lambda(c) + \lambda'(c)xh)x} x^2 \right)|h|
\end{align*}

By our choice of $\eta$ and for sufficiently small $h$ (as above) these exponential terms all have negative exponents, thus the RHS on the last line above is less than or equal to a constant multiplied by $|h|$ and so it decays to 0 uniformly in $x$.\\

Since everything decays to 0 uniformly in $x$, we have proved that (i) is Frechet differentiable.\\

The derivative of (iii) follows from that of (i) together with what we showed in Lemma \ref{qc}. For $x \geq 0$, let $L(c, x)$ be the Frechet derivative of $e^{A(c)(x)}P^s(c)$ with respect to $c$. We proved that this exists in (i) above. Our candidate for the Frechet derivative is

\begin{equation}\label{frechetcandidateint}
\int_0^x L(c, x - y) N(U(y)) dy
\end{equation}

First, we need to show this is a bounded linear transformation from $\R$ to $X_\eta$. To do this, we will use a little trick. Recall that we chose $\eta$ such that $0 < \eta < \alpha^s, \alpha^u$. Since this inequality is strict, we can find $\delta > 0$ such that $0 < \eta + \delta < \alpha^s, \alpha^u$. Since $e^{A(c)(x)}P^s(c)$ is Frechet differentiable for all $\eta < \alpha^s, \alpha^u$, it is Frechet differentiable for $\eta + \delta$. Thus we have

\begin{align*}
\left| e^{\eta x} \int_0^x L(c, x - y) N(U(y)) dy \right| &\leq \int_0^x |e^{\eta (x-y)} L(c, x - y)| |e^{\eta y} N(U(y))| dy\\
&= \int_0^x e^{-\delta (x-y)} |e^{(\eta + \delta) (x-y)} L(c, x - y)| |e^{\eta y} N(U(y))| dy\\
&\leq L(\rho)\rho ||L(c, \cdot) ||_{\eta + \delta} \int_0^x e^{-\delta(x-y)} dy\\
&= L(\rho)\rho ||L(c, \cdot) ||_{\eta + \delta} \frac{1 - e^{-\delta x}}{\delta}\\
&\leq \frac{L(\rho)\rho}{\delta}||L(c, \cdot) ||_{\eta + \delta}
\end{align*}

where in the third line we used \eqref{NUetabound} from Lemma \ref{qc}, since our domain for $U$ is $D$. $D$, $L(\rho)$, and $\rho$ are defined in Lemma \ref{qc}. We also used the fact that the Frechet derivative $L(c, x)$ is bounded for $\eta + \delta$, which we showed above. In the penultimate line we used the fact that $x \geq 0$ and $\delta > 0$. Since the RHS is a constant, \eqref{frechetcandidateint} is a bounded linear transformation from $\R$ to $X_\eta$.\\

To show that \eqref{frechetcandidateint} is actually the Frechet derivative, using the definition of the Frechet derivative,

\begin{align*}
\frac{1}{|h|}&e^{\eta x}\left| \int_0^x e^{A(c+h)(x - y)}P^s(c) N(U(y))dy - \int_0^x e^{A(c)(x - y)}P^s(c) N(U(y))dy - h \int_0^x L(c, x - y) N(U(y)) dy \right|\\
&= \frac{1}{|h|}e^{\eta x}\left| \int_0^x \left( e^{A(c+h)(x - y)}P^s(c) - e^{A(c)(x - y)}P^s(c) - h L(c, x - y) \right) N(U(y)) dy \right| \\
&\leq \int_0^x \left| \frac{ e^{\eta(x-y)} (e^{A(c+h)(x - y)}P^s(c) - e^{A(c)(x - y)}P^s(c) - h L(c, x - y))}{h}\right| \left| e^{\eta y} N(U(y)) \right| dy \\
&\leq \int_0^x \left| \frac{ e^{\eta(x-y)} (e^{A(c+h)(x - y)}P^s(c) - e^{A(c)(x - y)}P^s(c) - h L(c, x - y))}{h}\right| || e^{\eta y} N(U(y)) ||_\eta dy \\
&\leq L(\rho)\rho \int_0^x \left| \frac{ e^{\eta(x-y)} (e^{A(c+h)(x - y)}P^s(c) - e^{A(c)(x - y)}P^s(c) - h L(c, x - y))}{h}\right| dy
\end{align*}

where in the last line we used \eqref{NUetabound} from Lemma \ref{qc}.We need this to decay uniformly in $x$, so we use the same $\delta$ trick we used above. Using the same $\delta > 0$,

\begin{align*}
\frac{1}{|h|}&e^{\eta x}\left| \int_0^x e^{A(c+h)(x - y)}P^s(c) N(U(y))dy - \int_0^x e^{A(c)(x - y)}P^s(c) N(U(y))dy - h \int_0^x L(c, x - y) N(U(y)) dy \right|\\
&\leq L(\rho)\rho \int_0^x e^{-\delta(x-y)} \left| \frac{ e^{(\eta+\delta)(x-y)} (e^{A(c+h)(x - y)}P^s(c) - e^{A(c)(x - y)}P^s(c) - h L(c, x - y))}{h}\right| dy\\
&\leq L(\rho)\rho \int_0^x e^{-\delta(x-y)} \frac{ || e^{A(c+h)(\cdot)}P^s(c) - e^{A(c)(\cdot)}P^s(c) - h L(c, \cdot) ||_{\eta + \delta} }{|h|} dy\\
&= \leq L(\rho)\rho \frac{ || e^{A(c+h)(\cdot)}P^s(c) - e^{A(c)(\cdot)}P^s(c) - h L(c, \cdot) ||_{\eta + \delta} }{|h|} \int_0^x e^{-\delta(x-y)} dy\\
\end{align*}

In (i), we showed that $L(c, x)$ is the Frechet derivative of $e^{A(c)(x)}P^s(c)$ with respect to $c$ for any $0 < \eta < \alpha^s, \alpha^u$. In particular, this is true for $\eta + \delta$. Thus by the definition of the Frechet derivative, given $\epsilon > 0$, we can find $h$ sufficiently small such that 

\[
\frac{ || e^{A(c+h)(\cdot)}P^s(c) - e^{A(c)(\cdot)}P^s(c) - h L(c, \cdot) ||_{\eta + \delta} }{|h|} < \epsilon
\]

Thus for sufficiently small $h$, 

\begin{align*}
\frac{1}{|h|}&e^{\eta x}\left| \int_0^x e^{A(c+h)(x - y)}P^s(c) N(U(y))dy - \int_0^x e^{A(c)(x - y)}P^s(c) N(U(y))dy - h \int_0^x L(c, x - y) N(U(y)) dy \right|\\
&\leq L(\rho)\rho \epsilon \int_0^x e^{-\delta(x-y)} dy\\
&= L(\rho)\rho \epsilon \frac{1 - e^{-\delta x}}{\delta}\\
&\leq \frac{L(\rho)\rho}{\delta}\epsilon
\end{align*}

since $\delta > 0$ and $x \geq 0$. Since $\epsilon$ is arbitrary and everything else is a constant not depending on $x$, we have proved that (iii) is Frechet differentiable with respect to $c$.\\

For (ii), we first show Frechet differentiability of $e^{A(c)(x}P^u(c) N(U(y))$ on the exponentially weighted space $\{ f \in C^0((-\infty, 0], \R^n) : \sup_{x \in (-\infty, 0]} |e^{\eta x} f(x)| < \infty \}$, which is similar to (i). The Frechet differentiability of (ii) then follows similarly to that of (iii).

\end{proof}
\end{lemma}

We will also need the following hypotheses. Numerics suggest that these are reasonable.

\begin{hypothesis}\label{1dkernel}
The kernels of $H$, $\partial_x H$, and $H \partial_x$ are all one-dimensional. Thus we have
\begin{align*}
\ker H &= \text{span}\{ q' \} \\
\ker \partial_x H &= \text{span}\{ q' \} \\
\ker H \partial_x &= \text{span}\{ q \}
\end{align*}
Note that these only apply on the unbounded domain. For a bounded domain, for example, $\ker H \partial_x$ also contains the constant functions.
\end{hypothesis}

\begin{hypothesis}\label{qcIP}
$\langle q, q_c\rangle_{L^2(\R)} \neq 0$
\end{hypothesis}
Note that by Cauchy-Schwarz, $\langle q, q_c\rangle_{L^2(\R)} \leq ||q||_{L^2(\R)} ||q_c|_{L^2(\R)}$. Since the single pulse $q$ is exponentially localized, $q \in L^2(\R)$. By Lemma \ref{qc}, $q_c \in L^2(\R)$. Thus this inner product is well-defined.

\subsection*{Single Pulse, Construction of Eigenfunction}

We are interested in constructing the eigenfunction of the linearization about the single pulse solution of KdV5. Although we know that the derivative of the single pulse is the only (non-generalized) eigenfunction, this should allow us to construct the Evans function, which can then be extended to the multipulse case. There are many ways in which we could do this. Here is a summary of some things we could or did try, together with remarks about why they did not work.

\begin{enumerate} 
	\item Use construction from Sandstede (1998). We cannot do this directly, since the matrix representing linearization of KdV5 about the zero-solution is not hyperbolic. (There is always an eigenvalue at 0 representing translation invariance.) We could, however, use this method in an exponentially weighted space.
	\item Form an ``integrated eigenvalue problem'' by integrating both sides of $\partial_x H v = \lambda v$ with sufficient limits of integration. If we do that, integrating the ``negative piece'' from $-\infty$ and the ``positive'' piece from $\infty$, we need an additional ``mean-zero'' condition to get the Melnikov term we want.
\end{enumerate}

There are a few more things left to try, so let's do that now.

\subsubsection*{Integrated eigenvalue problem, integrate from below}

Here we present a different version of the integrated eigenvalue problem, where we integrate from $-\infty$ on both pieces. Recall that the eigenvalue problem we are interested in is given by

\[
\partial_x H v = \lambda v
\]

We write the eigenfunction piecewise as a perturbation of the zero-eigenfunction, which is the derivative of the single pulse: $v^\pm = q' + w^\pm$. Substituting this in, and using the fact that $Hq' = 0$, we get

\[
\partial_x H w^\pm = \lambda q' + \lambda w^\pm
\]

Now we integrate from $-\infty$ to $x$. Note that when we do this for the ``positive'' piece, we have to integrate across the entire ``negative'' piece. There is no boundary term at $-\infty$ since we are assuming the pulse decays (exponentially) at the ends.

\begin{align*}
H w^-(x) &= \lambda q + \lambda \int_{-\infty}^x w^-(y) dy \\
H w^+(x) &= \lambda q + \lambda \int_{-\infty}^0 w^-(y) dy + \lambda \int_0^x w^+(y) dy \\
\end{align*}

If we define $z^\pm$ by

\begin{align*}
z^- &= 0 \\
z^+ &= \int_{-\infty}^0 w^-(y) dy
\end{align*}

we can combine these into a single line

\[
H w^\pm = \lambda(q + z^\pm) + \lambda \int_{a^\pm}^x w^\pm dy
\]

where $a^- = -\infty$ and $a^+ = 0$. Converting this into a first order system, we have

\begin{equation}
(W^\pm)' = A(Q)W^\pm + \lambda B (Q + Z^\pm) + \lambda K^\pm B W^\pm
\end{equation}

where $K$ is the integration operator with lower limits $a^\pm$ and $B$ is the matrix
\[
\begin{pmatrix}0 & 0 & 0 & 0 \\0 & 0 & 0 & 0 \\0 & 0 & 0 & 0 \\1 & 0 & 0 & 0 \end{pmatrix}
\]


This we obtain the following system of equations, which is analogous to (3.7) in Sandstede (1998).

\begin{align*}\label{inteig1}
(W^\pm)' &= A(Q)W^\pm + \lambda B (Q + Z^\pm) + \lambda K^\pm B W^\pm\\
W^\pm(x) &\in \C \psi(0) \oplus Y^+ \oplus Y^- \\
W^+(0) - W^-(0) &\in \C \psi(0) 
\end{align*}

For the above problem to make sense, we need the RHS of the $W^\pm$ equation in to be continuous at $x = 0$. Suppose we have found a solution to this such that $W^-(0) = W^+(0)$. Then since we always integrating from $-\infty$, the RHS is continuous at 0 by construction, so we don't have a ``mean-zero'' condition to worry about.\\

Analogous to (3.14) in Sandstede (1998), we can write the fixed point equations for $W^\pm$ as

\begin{align*}
W^-(x) = \Phi^u_-(x, 0)b^- &+ \int_0^x \Phi^u_-(x, y)[\lambda (K^- B W^-)(y) + \lambda B (Q(y) + Z^-) ] dy \\
&+ \int_{-\infty}^x \Phi^s_-(x, y)[\lambda (K^- B W^-)(y) + \lambda B (Q(y) + Z^-)] dy \\
W^+(x) = \Phi^s_+(x, 0)b^+ &+ \int_0^x \Phi^s_+(x, y)[\lambda (K^+ B W^+)(y) + \lambda B(Q(y) + Z^+)] dy \\
&+ \int_{\infty}^x \Phi^u_+(x, y)[\lambda (K^+ B W^+)(y) + \lambda B (Q(y) + Z^+) ] dy
\end{align*}

To estimate the single jump at $x = 0$ we compute
\[
\xi = \langle \Psi(0), W^+(0) - W^-(0) \rangle
\]

Plugging in the fixed point equations, and using $Z^- = 0$, this becomes

\begin{align*}
\langle\Psi(0), W^-(0) &- W^+(0)\rangle = \langle \Psi(0), \Phi^u_-(0, 0)b^- \rangle + \int_{-\infty}^0 \Phi^s_-(0, y)[\lambda (K^- B W^-)(y) + \lambda B Q(y) ] dy  \\
&- \langle \Psi(0), \Phi^s_+(0, 0)b^+ \rangle - \int_\infty^0 \Phi^u_+(0, y)[\lambda (K^+ B W^+)(y) + \lambda B (Q(y) + Z^+) ] dy \rangle\\
&= \langle \Psi(0), b^- \rangle - \langle \Psi(0), b^+ \rangle + \int_{-\infty}^0 \langle \Psi(0), \Phi^s_-(0, y)[\lambda (K^- B W^-)(y) + \lambda B Q(y) ] \rangle dy  \\
&- \int_\infty^0 \langle \Psi(0), \Phi^u_+(0, y)[\lambda (K^+ B W^+)(y) + \lambda B (Q(y) + Z^+) ] \rangle dy  \\
&= \int_{-\infty}^0 \langle \Psi(y), \lambda (K^- B W^-)(y) + \lambda B Q(y) \rangle dy \\
&+ \int_0^\infty \langle \Psi(y), \lambda (K^+ B W^+)(y) + \lambda B (Q(y) + Z^+)  \rangle dy \\
&= \lambda\left( \int_{-\infty}^0 \langle \Psi(y), (K^- B W^-)(y) \rangle dy + \int_0^\infty \langle \Psi(y), (K^+ B W^+)(y) \rangle dy \right) \\
&+ \lambda \int_{-\infty}^\infty \langle\Psi(y), BQ(y) \rangle dy + \lambda \int_{-\infty}^0 \langle \Psi(y), B Z^+ \rangle dy
\end{align*}

where we terms involving the $b^\pm$ are zero since $b^\pm \perp \Psi(0)$.\\

The second integral on the RHS is the standard Melnikov integral, which we have shown is 0. Thus we have for our jump

\begin{align*}
\langle\Psi(0), W^-(0) &- W^+(0)\rangle \\
&= \lambda\left( \int_{-\infty}^0 \langle \Psi(y), (K^- B W^-)(y) \rangle dy + \int_0^\infty \langle \Psi(y), (K^+ B W^+)(y) \rangle dy \right) + \lambda \int_{-\infty}^0 \langle \Psi(y), B Z^+ \rangle dy
\end{align*}

Before we go any further, we will integrate by parts on the RHS. We have already shown that the 4th component of $\Psi$ is $q'$, so we will use that here.

\begin{align*}
\int_{-\infty}^0 \langle \Psi(y), &(K^- B W^-)(y) \rangle dy + \int_0^\infty \langle \Psi(y), (K^+ B W^+)(y) \rangle dy\\
&= \int_{-\infty}^0 q'(y) \int_{-\infty}^y w^-(z) dzdy + \int_0^\infty q'(y) \int_{0}^y w^+(z) dz dy \\
&=  q(y) \int_{-\infty}^y w^-(z) dz \Big|_{-\infty}^0 - \int_{-\infty}^0 q(y)w^-(y) dy + q(y) \int_{0}^y w^+(z) dz \Big|_0^\infty - \int_0^\infty q(y)w^+(y) dy \\
&= q(0) \int_{-\infty}^0 w^-(z) dz - \left( \int_{-\infty}^0 q(y)w^-(y) dy + \int_0^\infty q(y)w^+(y) dy \right)
\end{align*}

The term in parentheses on the RHS looks good, but we have a boundary term before that, which doesn't look good. Evaluating our other boundary term, we have

\begin{align*}
\int_{-\infty}^0 \langle \Psi(y), B Z^+ \rangle dy &= \int_{-\infty}^0 q'(y) \int_{-\infty}^0 w^-(z) dz dy \\
&= \int_{-\infty}^0 q'(y) dy \int_{-\infty}^0 w^-(y) dy \\
&= q(0) \int_{-\infty}^0 w^-(y)
\end{align*}

Combining these for our jump, we have

\begin{align*}
\langle\Psi(0), W^-(0) &- W^+(0)\rangle \\
&= \lambda\left[ q(0) \int_{-\infty}^0 w^-(y) dy + q(0) \int_{-\infty}^0 w^-(y) - \left( \int_{-\infty}^0 q(y)w^-(y) dy + \int_0^\infty q(y)w^+(y) dy \right) \right] \\
&= \lambda\left[ 2 q(0) \int_{-\infty}^0 w^-(y) - \left( \int_{-\infty}^0 q(y)w^-(y) dy + \int_0^\infty q(y)w^+(y) dy \right) \right] 
\end{align*}

Thus we have a similar boundary condition to before and conclude that the integrated eigenvalue problem is likely not going to be useful, even if we use limits of integration which guarantee continuity across the join point.

\subsubsection*{Exponentially weighted space}

We will pose our eigenvalue problem in the exponentially weighted space $H_\eta^k(\R)$, where $a$ is the exponential weight, and the norm of the space is given by

\[
||u||_{H_\eta^k(\R)} = ||e^{\eta x}u||_{H^k}
\]

The (unweighted) linear operator is

\[
L = \partial_x H = \partial_x^5 - \partial_x^3 + (c - 2q(x))\partial_x - 2 q_x(x)
\]

We can write the eigenvalue problem as a first-order system. After doing this, the eigenvalue problem becomes (for fixed $c$) $V' = A(q, 0)V + \lambda B V$, where V = $(v, v', v'', v''', v'''')$ and we have the matrices

\begin{align*}
B = \begin{pmatrix}0 & 0 & 0 & 0 & 0 \\0 & 0 & 0 & 0 & 0 \\0  & 0 & 0 & 0 & 0 \\0 & 0 & 0 & 0 & 0 \\1 & 0 & 0 & 0 & 0 \end{pmatrix} && 
A(q, 0) = \begin{pmatrix}0 & 1 & 0 & 0 & 0 \\0 & 0 & 1 & 0 & 0 \\0 & 0 & 0 & 1 & 0 \\0 & 0 & 0 & 0 & 1 \\
2q'(x) & 2q(x) - c & 0 & 1 & 0 \end{pmatrix} 
\end{align*}

In the exponentially weighted space, this becomes the operator

\[
L_\eta = e^{\eta x} L e^{-\eta x} = e^{\eta x} \partial_x H e^{-\eta x}
\]

where we have chosen $\eta$ such that $0 < \eta < \alpha$ so that the exponentially weighted single pulse $e^{\eta x} q(x)$ still decays exponentially at both ends (albeit at a smaller rate at one end).\\

The single element in the kernel is $\tilde{q}(x) = e^{\eta x} q'(x)$, since

\begin{align*}
L_\eta e^{\eta x} q'(x) &= e^{\eta x} L e^{-\eta x} e^{\eta x} q'(x) = e^{\eta x} L q'(x) = 0
\end{align*}

where we used the fact that $q' \in \ker L$. For the generalized kernel, we have the single element $-e^{\eta x} q_c(x)$, since

\begin{align*}
L_\eta e^{\eta x} (-q_c(x)) &= e^{\eta x} L e^{-\eta x} e^{\eta x} (-q_c(x)) = e^{\eta x} L (-q_c(x)) = e^{\eta x} q'(x)
\end{align*}

where we used the fact that $L q_c(x) = -q'(x)$. In system-form, this generalized kernel element will be called $Q^\eta_c(x)$.\\

We can also write this as a first order system in the exponentially weighted space. In that case, the eigenvalue problem becomes $V' = A(q, a)V + \lambda B V$, where

\[
A(q, \eta) = 
\begin{pmatrix}0 & 1 & 0 & 0 & 0 \\0 & 0 & 1 & 0 & 0 \\0 & 0 & 0 & 1 & 0 \\0 & 0 & 0 & 0 & 1 \\
\eta^5 - \eta^3 + \eta c - 2 \eta q(x) + 2q'(x) & -5\eta^4 + 3\eta^2 + 2q(x) - c & -10\eta^3 + 3\eta & -10\eta^2 + 1 & 5\eta \end{pmatrix} 
\]

We probably don't need to write this down explicitly. If we linearize about the zero-solution, the matrix $A(0, \eta)$ is hyperbolic for $\eta \neq 0$ (easy to show numerically, so for now that will do). Thus we can follow the procedure in Sandstede (1998), as long as we set up the problem correctly. So let's do that.\\

Consider the eigenvalue problem

\[
V' = A(q, \eta)V + \lambda B V
\]

and take the eigenvalue $V$ to be a perturbation of the zero-eigenfunction, which in the weighted space we will call $Q^\eta_x$. Recall from above that the first component of this is $Q^\eta_x(x) = e^{ax} q'(x)$ and the remaining components are the derivatives of this. In other words, we write

\[
V' = Q^\eta_x + W^\pm
\]

Plugging this into the eigenvalue problem, we have

\begin{align*}
(W^\pm)' &= A(q, \eta)W^\pm + \lambda B Q^\eta_x + \lambda B W^\pm \\
W^\pm(x) &\in \C \psi(0) \oplus Y^+ \oplus Y^- \\
W^+(0) - W^-(0) &\in \C \psi(0) 
\end{align*}

Following Sandstede (1998), we write the fixed-point equations for $W$ as 

\begin{align*}
W^-(x) = \Phi^u(x, 0)b^- &+ \int_0^x \Phi^u(x, y)[\lambda B W^-(y) + \lambda B Q^\eta_x(y) ] dy \\
&+ \int_{-\infty}^x \Phi^s(x, y)[\lambda B W^-(y) + \lambda B Q^\eta_x(y) ] dy \\
W^+(x) = \Phi^s(x, 0)b^+ &+ \int_0^x \Phi^s(x, y)[\lambda B W^+(y) + \lambda B Q^\eta_x(y) ] dy \\
&+ \int_{\infty}^x \Phi^u(x, y)[\lambda B W^+(y) + \lambda B Q^\eta_x(y) ] dy
\end{align*}

The jump at $x = 0$ is given by

\begin{align*}
\langle\Psi(0), W^-(0) &- W^+(0)\rangle = \langle \Psi(0), \Phi^u(0, 0)b^- + \int_{-\infty}^0 \Phi^s(0, y)[\lambda B W^-(y) + \lambda B Q^\eta_x(y) ] dy  \\
&- \Phi^s(0, 0)b^+ - \int_\infty^0 \Phi^u(0, y)[\lambda B W^+(y) + \lambda B Q^\eta_x(y) ] dy \rangle\\
&= \langle \Psi(0), b^- \rangle - \langle \Psi(0), b^+ \rangle + \int_{-\infty}^0 \langle \Psi(0), \Phi^s_-(0, y)[\lambda B W^-(y) + \lambda B Q^\eta_x(y) ] \rangle dy  \\
&- \int_\infty^0 \langle \Psi(0), \Phi^u_+(0, y)[\lambda B W^+(y) + \lambda B Q^\eta_x(y) ] \rangle dy  \\
&= \int_{-\infty}^0 \langle \Psi(y), \lambda B W^-(y) + \lambda B Q^\eta_x(y) \rangle dy 
+ \int_0^\infty \langle \Psi(y), \lambda B W^+(y) + \lambda B Q^\eta_x(y)  \rangle dy \\
&= \lambda\left( \int_{-\infty}^0 \langle \Psi(y), B W^-(y) \rangle dy + \int_0^\infty \langle \Psi(y), B W^+(y) \rangle dy \right) + \lambda \int_{-\infty}^\infty \langle\Psi(y), BQ^\eta_x(y) \rangle dy
\end{align*}

The final integral on the RHS is our first order Melnikov integral. We would like to show this is zero. To do that, we need to determine what $\Psi(y)$ is in this case. Recall that $\Psi(y)$ is the solution to the adjoint equation $\Psi'(y) = -A(q, \eta)^*\Psi(y)$. Because of the matrix $B$, we only care about the last component of $\Psi(y)$. To do this, we use some facts about linear differential operators and first order systems. This will be done without proof (for now), but the basic ideas come from Pego and Weinstein, among others, and I verified this in simple cases.\\

Suppose $L$ is an $n$th degree differential operator

\[
L = \frac{d^n}{dx^n} + \sum_{i = 1}^{n=1}a_i(x) \frac{d^i}{dx^i}
\]

and $A(L)$ is the representation of the operator $L$ as a first order system. Form the adjoint problem $W' = -A(L)^* W$ from the matrix $A(L)$. Then the $n$th component of $W$ is in the kernel of the adjoint differential operator $L^*$, i.e. $L^* w_n = 0$.\\

Returning to our problem, we have $\psi_5(y) \in \ker (e^{\eta x} \partial_x H e^{-\eta x})^*$. Since

\[
(e^{\eta x} \partial_x H e^{-\eta x})^* = (e^{-\eta x})^* H^* (\partial_x)^* (e^{\eta x})^* = -e^{-\eta x} H \partial_x e^{\eta x}
\]

we have $\psi_5(y) \in \ker e^{-\eta x} H \partial_x e^{-\eta x}$. Since $q' \in \ker H$, we can see that $e^{-\eta x} q \in \ker e^{-\eta x} H \partial_x e^{-\eta x}$. Since we are assuming that $\ker H$ is 1-dimensional (I will make sure that assumption is formally stated somewhere above), we conclude that $\psi_5(y) = e^{-\eta x} q$. Thus the first order Melnikov integral is

\begin{align*}
\int_{-\infty}^\infty \langle\Psi(y), BQ^\eta_x(y) \rangle dy &= \int_{-\infty}^\infty e^{-\eta x} q(x) e^{\eta x} q'(x) dx\\
&= \int_{-\infty}^\infty  q(x) q'(x) dx \\
&= \int_{-\infty}^\infty \left( \frac{1}{2} q^2 \right)_x dx\\
&= 0
\end{align*}

since the single pulse decays (exponentially) at both ends. Our jump equation then becomes

\begin{align*}
\langle\Psi(0), W^-(0) &- W^+(0)\rangle = \lambda\left( \int_{-\infty}^0 \langle \Psi(y), B W^-(y) \rangle dy + \int_0^\infty \langle \Psi(y), B W^+(y) \rangle dy \right)
\end{align*}

Since the first order Melnikov integral is zero, we will look for a higher order Melnikov integral. To do this, we look for an expansion for $W^\pm$. From what we have proved (or will prove in the double pulse case), we know that $W^\pm$ is analytic in $\lambda$, so we can expand it as a Taylor series in $\lambda$ about $\lambda = 0$. The constant term in the Taylor expansion must be zero since we know what happens when $\lambda = 0$. Thus we have

\[
W(\lambda, x)^\pm = \lambda V^\pm(x) + \mathcal{O}(\lambda^2)
\]

If we substitute this into our expression for $W^\pm(x)$ we get (neglecting the higher order term)

\begin{align*}
\lambda (V^\pm)'(x) &= A(q, \eta) \lambda V^\pm(x) + \lambda B Q^\eta_x(x) + \lambda^2 B V^\pm(x) 
\end{align*}

Since we are interested in $\lambda \neq 0$ and we know what happens when $\lambda = 0$, we can divide by $\lambda$ to get

\begin{align*}
(V^\pm)'(x) &= A(q, \eta)V^\pm(x) + B Q^\eta_x(x) + \lambda B V^\pm(x) 
\end{align*}

To find $V^\pm(x)$, we set $\lambda = 0$ to cancel the last term on the RHS to get

\begin{align*}
(V^\pm)'(x) &= A(q, a) V^\pm(x) + B Q^\eta_x(x) 
\end{align*}

which we have shown has unique solution $V^\pm(x) = -Q^\eta_c(x)$. In this case, $-Q_c(x)$ is continuous, so the two pieces $V^\pm(x)$ are automatically connected at $x = 0$. So we have written $W^\pm$ as

\[
W(\lambda, x)^\pm = -\lambda Q^\eta_c(x) + \mathcal{O}(\lambda^2)
\]

It remains to verify the other pieces to the original problem. First we need to verify that

\[
W^\pm(0) \in \C \psi(0) \oplus Y^+ \oplus Y^- 
\]

In other words, we need to show that $W^\pm(0) \perp Q^\eta_x(0)$ for the expression we have for $W^\pm$. For this to be true, it must hold for the lowest order term, i.e. we need to show that $Q^\eta_c(0) \perp Q^\eta_x(0)$. If we do this, taking the standard $\R^5$ inner product, we do not get 0, even though we do in the unweighted case. Maybe the unweighted case is what we care about, since once we construct an eigenfunction, we can multiply it by $e^{-\eta x}$ to get an eigenfunction to the unweighted problem. 
\\

In any case, it might not be legit to actually do this, but we can formally substitute $W(\lambda, x)^\pm = -\lambda Q^\eta_c(x) + \lambda^2 \tilde{W}^\pm(x)$ into our jump equation to get

\begin{align*}
\langle\Psi(0) &, W^-(0) - W^+(0)\rangle \\
&= -\lambda^2 \int_{-\infty}^\infty \langle \Psi(y), B Q^\eta_c(y) \rangle dy + \lambda^3 \left( \int_{-\infty}^0 \langle \Psi(y), B \tilde{W}^-(y) \rangle dy + \int_0^\infty \langle \Psi(y), B \tilde{W}^+(y) \rangle dy \right) \\
&= -\lambda^2 \int_{-\infty}^\infty e^{-\eta y}q(y) e^{\eta y} q_c(y) dy + \lambda^3 \left( \int_{-\infty}^0 \langle \Psi(y), B \tilde{W}^-(y) \rangle dy + \int_0^\infty \langle \Psi(y), B \tilde{W}^+(y) \rangle dy \right) \\
&= -\lambda^2 \int_{-\infty}^\infty q(y) q_c(y) dy + \lambda^3 \left( \int_{-\infty}^0 \langle \Psi(y), B \tilde{W}^-(y) \rangle dy + \int_0^\infty \langle \Psi(y), B \tilde{W}^+(y) \rangle dy \right) \\
&= -\lambda^2 \langle q, q_c \rangle_{L^2(\R)} + \lambda^3 \left( \int_{-\infty}^0 \langle \Psi(y), B \tilde{W}^-(y) \rangle dy + \int_0^\infty \langle \Psi(y), B \tilde{W}^+(y) \rangle dy \right) \\
\end{align*}

The first term on the RHS is the higher order Melnikov integral, and it agrees with Pego and Weinstein and our intuition.

\subsubsection*{Double Pulse}

Now we combine stuff from the single pulse case and from Sandstede (1998) to generalize this to the double pulse. Recall from that paper that for a double pulse, we can write the eigenfunction as a perturbation of the derivative of the double pulse, where on the two ``big pieces'', the derivative is multiplied by a scaling constant. Since we are doing this in the exponentially weighted space and our notation is rapidly becoming complicated, we will represent the double pulse by $\tilde{q}$, thus the eigenfunction will be a perturbation of $\tilde{Q}^\eta_x$, where the first component of this is $e^{\eta x} \tilde{q}_x$.

\[
V_i^\pm = d_i \tilde{Q}^\eta_x + W_i^\pm 
\]

Numerics suggest that the $d_i$ are not identical. (For the interaction eigenfunction of the first unstable double pulse, for example, one side is significantly larger than the other.) Recalling what we did in the previous section, we can write this as

\begin{equation}\label{dpeigform}
V_i^\pm = d_i( \tilde{Q}^\eta_x - \lambda \tilde{Q}^\eta_c ) + W_i^\pm
\end{equation}

The perturbation $W_i^\pm$ is different in this case. When $\lambda = 0$, we can take $d_i = 1$ to get the correct eigenfunction $\tilde{Q}^\eta_x$, so that is good. Also we will eventually want this in terms of the single pulse $Q$, but we leave it like this for now to get some nice cancelation. Recall that the eigenvalue problem in the exponentially weighted space is given in piecewise system form by

\[
(V_i^\pm)' = A(\tilde{q}, \eta)V_i^\pm + \lambda B V_i^\pm
\]

If we plug \eqref{dpeigform} into this, we get 

\begin{align*}
(d_i \tilde{Q}^\eta_x &- d_i \lambda \tilde{Q}^\eta_c  + W_i^\pm)' = \\
& d_i A(\tilde{q}, \eta) \tilde{Q}^\eta_x - d_i \lambda A(\tilde{q}, \eta)\tilde{Q}^\eta_c
+ A(\tilde{q}, \eta) W_i^\pm + d_i \lambda B \tilde{Q}^\eta_x - d_i \lambda^2 B \tilde{Q}^\eta_c 
+ \lambda B W_i^\pm 
\end{align*}

We note the following relationships which are analogous to the single pulse case.

\begin{align*}
(\tilde{Q}^\eta_x)' &= A(q,\eta)\tilde{Q}^\eta_x \\
(\tilde{Q}^\eta_c)' &= A(q, \eta)\tilde{Q}^\eta_c - B \tilde{Q}^\eta_x  
\end{align*} 


Using these expressions to cancel things, we get

\begin{align*}
(W_i^\pm)' = A(\tilde{q}, \eta) W_i^\pm - d_i \lambda^2 B \tilde{Q}^\eta_c 
+ \lambda B W_i^\pm 
\end{align*}

We would like to write things in terms of the single pulse $Q$. Luckily, from Sandstede (1998) we know that

\[
\tilde{Q} = Q + R_i^\pm
\]

where the remainder $R_i^\pm$ is small (and we have estimates on it). Then $\tilde{Q}_c = Q_c + (R_i^\pm)_c$. Let

\[
G(\eta)_i^\pm = A(\tilde{q},\eta) - A(q,\eta) = A(Q + R_i^\pm, \eta) - A(Q, \eta)
\]

Following Sandstede (1998) we are then looking to solve the following system

\begin{equation}\label{inteigQ}
(W_i^\pm)' = A(q, \eta)W_i^\pm + G(\eta)_i^\pm W_i^\pm - d_i \lambda^2 B \tilde{Q}^\eta_c + \lambda B W_i^\pm
\end{equation}  

subject to conditions

\begin{align*}
W_i^-(0) &= W_i^+(0) \\
W_1^+(L) - W_2^-(-L) &= D_1(\eta) d \\
W_i^\pm(x) &\in \C \Psi(0) \oplus Y^+ \oplus Y^- \\
W_i^+(0) - W_i^-(0) &\in \C \Psi(0) 
\end{align*}

where 

\begin{align*}
G(\eta)_i^\pm &= A(\tilde{q},\eta) - A(q,\eta) = A(Q + R_i^\pm, \eta) - A(Q, \eta) \\
D_1(\eta) d &= d_2 [ \tilde{Q}^\eta_x(-L) + \lambda \tilde{Q}^\eta_c(-L)] 
- d_1 [ \tilde{Q}^\eta_x(L) + \lambda \tilde{Q}^\eta_c(L)]
\end{align*}

and $D_1(\eta)$ is given by the row matrix 

\[
D_1(\eta) = \begin{pmatrix} -(\tilde{Q}^\eta_x(L) + \lambda \tilde{Q}^\eta_c(L)) & \tilde{Q}^\eta_x(-L) + \lambda \tilde{Q}^\eta_c(-L) \end{pmatrix}
\]

In the next lemma, we show some estimates similar to that in Lemma 3.1 in Sandstede (1998).

\begin{lemma}\label{estimates}
We have the estimates
\begin{align*}
|G(\eta)_i^\pm(x)| &\leq C|R_i^\pm(x)| \leq C \sup_{|x| \geq L} |Q(x)| \\
| B \tilde{Q}^\eta(x) - B Q^\eta(x) | & \leq C | e^{\eta x} R_i^\pm(x)| \leq C \sup_{|x| \geq L} |e^{\eta x} Q(x)| \\
D_1 d &= [Q^\eta_x(-L) + Q^\eta_x(L) + 
\lambda(Q^\eta_c(-L) + Q^\eta_c(L))](d_2 - d_1) + \mathcal{O}\left(e^{-\alpha L} |d| \sup_{|x| \geq L} |e^{\eta x} Q(x)| \right) \\
|A(\tilde{Q}, \eta)| &\leq C \textrm{ for all }x
\end{align*}
where $\alpha > 0$ is defined as on pages 432 and 434 of Sandstede (1998) and as above.

\begin{proof}
The first estimate is the same as in Sandstede (1998) with $Q$ replacing $Q^+$ and $Q^-$, and follows from the smoothness of $A$ together with (2.6)(i) in Sandstede (1998). The second estimate follows from (2.6)(i) in Sandstede (1998) and the expansion of $\tilde{Q}$ as $Q + R_i^\pm$. The additional factor of $e^{\eta x}$ comes into play since we are in the exponentially weighted space, so everything is multiplied by that factor. The fourth estimate follows since the double pulse $\tilde{q}(x)$ is bounded and the rest of the matrix $A$ is constant (depending on $\eta$, but that is constant). For the third estimate, we first write $D_1 d$ in terms of the single pulse

\begin{align*}
D_1 d &= d_2 [ \tilde{Q}^\eta_x(-L) + \lambda \tilde{Q}^\eta_c(-L)] 
- d_1 [ \tilde{Q}^\eta_x(L) + \lambda \tilde{Q}^\eta_c(L)] \\
&= d_2 [ Q^\eta_x(-L) + \lambda Q^\eta_c(-L) + (R_2^-)^\eta_x(-L) + \lambda (R_2^-)^\eta_c(-L)] \\
&- d_1 [ Q^\eta_x(L) + \lambda Q^\eta_c(L) + (R_1^+)^\eta_x(-L) + \lambda (R_1^+)^\eta_c(-L)] 
\end{align*}

Adapting Lemma 2.6 in Sandstede (1998), and noting that $Q = Q^+ = Q^-$ in our case, we have

\begin{align*}
|R_{i+1}^-(-X_i) - Q(X_i)| \leq C e^{-\alpha L} \sup_{|x| \geq L} |Q(x)| \\
|R_{i}^+(X_i) - Q(-X_i)| \leq C e^{-\alpha L} \sup_{|x| \geq L} |Q(x)| \\
\end{align*}

If we do this in the exponentially weighted space, we should just get a factor of $e^\eta x$ on the RHS, so this should be

\begin{align*}
|(R_{i+1}^-)^\eta(-X_i) - Q^\eta(X_i)| \leq C e^{-\alpha L} \sup_{|x| \geq L} |e^{\eta x} Q(x)| \\
|(R_{i}^+)^\eta(X_i) - Q^\eta(-X_i)| \leq C e^{-\alpha L} \sup_{|x| \geq L} |e^{\eta x} Q(x)| \\
\end{align*}

These should hold as well for derivatives with respect to $x$ and $c$, or at least I think that was used in the proof of Lemma 3.1 in Sandstede (1998) for the derivative with respect to $x$, so we will do that here. 

Thus, if we substitute these into the expression for $D_1 d$ we get the result we want

\begin{align*}
D_1 d &= [Q^\eta_x(-L) + Q^\eta_x(L) + 
\lambda(Q^\eta_c(-L) + Q^\eta_c(L))](d_2 - d_1) + \mathcal{O}\left(e^{-\alpha L} |d| \sup_{|x| \geq L} |e^{\eta x} Q(x)| \right) \\ \\
\end{align*}

\end{proof}
\end{lemma}

Now, as in Sandstede (1998), we write the system in a more general form. As in section 3.2 in Sandstede (1998), consider the general system

\begin{enumerate}[(i)]
\item $(W_i^\pm)' = A W_i^\pm + G_i^\pm W_i^\pm + \lambda B W_i^\pm + \lambda^2 d_i \tilde{H}_i^\pm$
\item $W_i^\pm(x) \in \C \psi(0) \oplus Y^+ \oplus Y^-$
\item $W_i^+(0) - W_i^-(0) \in \C \psi(0) $
\item $W_1^+(L) - W_2^-(-L) = D_1 d $
\end{enumerate}

where we have the bounds 

\begin{align*}
|G_i^\pm| & \leq \delta \\
|\tilde{H}_i^\pm(x) - H(x)| &\leq \Delta H \leq e^{\eta L} \delta\\
|B| &\leq 1 \\
|\lambda| &\leq \delta \\
|D_1| &\leq e^{\eta L} \delta
\end{align*}

where $H$ is a bounded function with supremum norm $||H||$ and $\delta$ is a constant we can choose later.

Following Sandstede (1998) and what we did above, we can write the fixed point equations for $W_i^\pm$ as

\begin{align*}
W_i^-(x) = \Phi^s(&x, -X_{i-1})a_{i-1}^- + \Phi^u(x, 0)b_i^- \\
&+ \int_0^x \Phi^u(x, y)[G_i^-(y) W_i^-(y) + \lambda B W_i^-(y) + \lambda^2 d_i \tilde{H}_i^-(y) ] dy \\
&+ \int_{-X_{i-1}}^x \Phi^s(x, y)[G_i^-(y) W_i^-(y) + \lambda B W_i^-(y) + \lambda^2 d_i \tilde{H}_i^-(y) ] dy \\
W_i^+(x) = \Phi^u(&x, X_i)a_i^+ + \Phi^s(x, 0)b_i^+ \\
&+ \int_0^x \Phi^s(x, y)[G_i^+(y) W_i^+(y) + \lambda B W_i^+(y) + \lambda^2 d_i \tilde{H}_i^+(y)] dy \\
&+ \int_{X_i}^x \Phi^u(x, y)[G_i^+(y) W_i^+(y) + \lambda B W_i^+(y) + \lambda^2 d_i \tilde{H}_i^+(y)]dy
\end{align*}

where in this case the join points are $(X_0, X_1, X_2) = (-\infty, L, \infty)$. Next we write down the equation for the jump distance (i.e. evaluate this at $x = 0)$. \\

Following Sandstede (1998), we proceed to invert this in a series of lemmas. 

\subsubsection*{The Inversion}

Define the various spaces as in (3.12) in Sandstede (1998). Then we proceed as follows. In the first lemma, we define and bound a linear operator consisting of the terms in the fixed point equation which involve $W_i^\pm$.

\begin{lemma}

Let $L_1(\lambda): V_w \rightarrow V_w$ be the linear operator defined piecewise by

\begin{align*}
(L_1(\lambda)W)_i^-(x) = \int_0^x &\Phi^u(x, y)[G_i^-(y) W_i^-(y) + \lambda B W_i^-(y) ] dy \\
&+ \int_{-X_{i-1}}^x \Phi^s(x, y)[G_i^-(y) W_i^-(y) + \lambda B W_i^-(y) ] dy \\
(L_1(\lambda)W)_i^+(x) = \int_0^x &\Phi^s(x, y)[G_i^+(y) W_i^+(y) + \lambda B W_i^+(y)] dy \\
&+ \int_{X_{i}}^x \Phi^u(x, y)[G_i^+(y) W_i^+(y) + \lambda B W_i^+(y) ] dy
\end{align*}

where $G_i^\pm$ is uniformly bounded with norm $|G|$. Then $L_1(\lambda): V_w \rightarrow V_w$ is a bounded linear operator, and we have bound

\begin{equation}
||L_1(\lambda)W|| \leq C\left(|G| +|\lambda|\right)||W||
\end{equation}

Note that this bound does not depend on the interval endpoints $X_i$.

\begin{proof}
First, we look at the negative piece. Note that when taking the absolute value we have to integrate in the positive direction. For all of this, we use the estimates in Lemma 3.2 of Sandstede (1998) to bound $\Phi^u_\pm$ and $\Phi^s_\pm$.

\begin{align*}
|(L_1(\lambda)W_i^-)(x) | &\leq \int_x^0 |\Phi^u(x, y)|[|G_i^-(y)||W_i^-(y)| + |\lambda||B W_i^-(y)| ] dy \\
&+ \int_{-X_{i-1}}^x |\Phi^s(x, y)|[|G_i^-(y)||W_i^-(y)| + |\lambda||B W_i^-(y)| ] dy \\
&\leq C(|G| + |\lambda|) ||W|| \left( \int_x^0 e^{\alpha^u (x-y)} dy + \int_{-X_{i-1}}^x e^{-\alpha^s (x-y)} dy \right) \\
&\leq C(|G| + |\lambda|) ||W|| \left( \int_x^0 e^{\alpha^u (x-y)} dy + \int_{-\infty}^x e^{-\alpha^s (x-y)} dy \right) \\
&\leq C(|G| + |\lambda|) ||W|| \left( \frac{1 - e^{\alpha^u x}}{\alpha^u} + \frac{1}{\alpha^s} \right) \\
&= C (|G| + |\lambda|) ||W|| 
\end{align*}

The positive piece is similar.

\end{proof}
\end{lemma}

In the next lemma, we consider a linear operator consisting of the remaining terms in the fixed point equation, i.e. those which do not involve $W_i^\pm$.

\begin{lemma}\label{L2}

Let $L_2(\lambda): V_a \times V_b \times V_d \rightarrow V_w$ be the linear operator defined piecewise by

\begin{align*}
L_2(\lambda)(a, b, d)_i^-(x) &= \Phi^s(x, -X_{i-1})a^-_{i-1} + \Phi^u(x, 0)b_i^- \\
&+ \lambda^2 d_i \left( \int_0^x \Phi^u(x, y)\tilde{H}_i^-(y) dy  + \int_{-X_{i-1}}^x \Phi^s(x, y)\tilde{H}_i^-(y) dy \right)\\
L_2(\lambda)(a, b, d)_i^+(x) &= \Phi^u(x, X_i)a^+_{i} + \Phi^s(x, 0)b_i^+ \\
&+ \lambda^2 d_i \left( \int_0^x \Phi^s(x, y)\tilde{H}_i^+(y) dy + \int_{X_{i}}^x \Phi^u(x, y)\tilde{H}_i^+(y) dy \right)
\end{align*}

Then $L_2$ is a bounded linear operator with the following bound

\begin{align*}
||L_2(\lambda)(a, b, d)|| \leq C(|a| + |b| + |\lambda|^2 ( ||H|| + \Delta H ) |d|)  \\
\end{align*}

Again, this bound does not depend on the specific interval endpoints $X_i$.

\begin{proof}

As in the previous lemma, we look at the negative piece first. Recall that $a_0 = 0$ (so the term involving $a^-_{i-1}$ is zero when $i = 1$ and that $-X_{i-1} \leq x \leq 0$. 

\begin{align*}
| L_2(\lambda)(a, b, d)_i^-(x)| &\leq |\Phi^s_-(x, -X_{i-1})a^-_{i-1}| + |\Phi^u_-(x, 0)b_i^-| \\
&+ |\lambda|^2 |d_i| \left( \int_0^x |\Phi^u(x, y)||\tilde{H}_i^-(y)| dy  + \int_{-X_{i-1}}^x |\Phi^s(x, y)||\tilde{H}_i^-(y)| dy \right) \\
&\leq C |e^{-\alpha^s (x + X_{i-1})}||a^-_{i-1}| + C |e^{\alpha^u x}||b_i^-| \\
&+ C|\lambda|^2 |d_i| \left( \int_x^0 e^{\alpha^u (x - y)}|\tilde{H}_i^-(y)| dy + \int_{-X_{i-1}}^x e^{-\alpha^s (x - y)}|\tilde{H}_i^-(y)| dy\right) \\
&\leq C(|a| + |b|) + C|\lambda|^2 |d_i| \left( \int_x^0 e^{\alpha^u (x - y)}|\tilde{H}_i^-(y)| dy + \int_{-X_{i-1}}^x e^{-\alpha^s (x - y)}|\tilde{H}_i^-(y)| dy\right) \\
\end{align*}

For the integral terms, we have

\begin{align*}
\int_x^0 e^{\alpha^u (x - y)}|\tilde{H}_i^-(y)| dy &\leq \int_x^0 e^{\alpha^u (x - y)}|BH(y)| dy + \int_x^0 e^{\alpha^u (x - y)}|B||\tilde{H}_i^-(y) - H(y)| dy \\
&\leq (||H|| + \Delta H)\frac{1 - e^{\alpha_u x}}{\alpha_u} \\
&= C( ||H|| + \Delta H )
\end{align*}

and

\begin{align*}
\int_{-X_{i-1}}^x e^{-\alpha^s (x - y)}|\tilde{H}_i^-(y)| dy &\leq \int_{-X_{i-1}}^x e^{-\alpha^s (x - y)}|BH(y)| dy + \int_{-X_{i-1}}^x e^{-\alpha^s (x - y)}|B||\tilde{H}_i^-(y) - H(y)| dy \\
&\leq \int_{-\infty}^x e^{-\alpha^s (x - y)}|BH(y)| dy + \int_{-\infty}^x e^{-\alpha^s (x - y)}|B||\tilde{H}_i^-(y) - H(y)| dy \\
&\leq (||H|| + \Delta H)\frac{1}{\alpha_s} \\
&= C( ||H|| + \Delta H )
\end{align*}

Thus we have

\begin{align*}
| L_2(\lambda)(a, b, d)_i^-(x)| \leq C(|a| + |b| + |\lambda|^2 ( ||H|| + \Delta H ) |d|) 
\end{align*}

The positive piece is similar.

\end{proof}

\end{lemma}

This is essentially identical to what we have in (3.20) in Sandstede (1998). We can combine these two lemmas to solve for $W_i^\pm$.

\begin{lemma}\label{W1}
There exists a bounded linear operator $W_1: V_\lambda \times V_a \times V_b \times V_d \rightarrow V_w$ such that 
\[
w = W_1(\lambda)(a,b,d)
\]
This operator is analytic in $\lambda$ and linear in $(a, b, d)$. The operator $W_1$ satisfies the bound

\[
||W_1(\lambda)(a,b,d)|| \leq C(|a| + |b| + |\lambda|^2 ( ||H|| + \Delta H ) |d|) \\
\]

\begin{proof}
Define the linear operators $L_1$ and $L_2$ as in the previous two lemmas. Then we can rewrite the fixed point equation as
\[
(I - L_1(\lambda))W = L_2(\lambda)(a,b,d)
\]

For $L_1$ we have the estimate
\[
||L_1(\lambda)W|| \leq C (|G| + |\lambda|)||W||
\]

Since we have $|G|, |\lambda| \leq \delta$, this becomes

\[
||L_1(\lambda)W|| \leq C \delta ||W||
\]

Choose $\delta$ such that $\delta < 1/C$. Then we have
\[
||L_1(\lambda)W|| < ||W||
\]

in which case the operator $(I - L_1(\lambda))$ is invertible. The inverse $(I - L_1(\lambda))^{-1}$is analytic in $\lambda$ and has operator norm 

\[
||(I - L_1(\lambda))^{-1}|| \leq \frac{1}{1 - ||L_1||}
\]

We can then write $W$ as
\[
W = W_1(\lambda)(a,b,d) = (I - L_1(\lambda))^{-1} L_2(\lambda)(a,b,d)
\]

which depends linearly on $(a,b,d)$ and analytically on $\lambda$. Since the operator norm of $L_1$ is bounded by a constant (independent of $X$), we have the bound

\[
||W_1(\lambda)(a,b,d)|| \leq C(|a| + |b| + |\lambda|^2 ( ||H|| + \Delta H ) |d|)
\]

\end{proof}
\end{lemma}

In the next lemma we solve for the center join $W_1^+(L) - W_2^-(-L) = D_1 d$.

\begin{lemma}
There exist operators

\begin{align*}
A_1: V_\lambda \times V_b \times V_d \rightarrow V_a \\
W_2: V_\lambda \times V_b \times V_d \rightarrow V_w \\
\end{align*}

such that $(a,w) = ( A_1(\lambda)(b,d), W_2(\lambda)(b,d) )$ solves our system. These operators are analytic in $\lambda$, linear in $(b,d)$, and bounds for them are given below.

\begin{proof}
\begin{align*}
D_1 d &= W_1^+(L) - W_2^-(-L) \\
&= \Phi^u(L, L)a^+_{1} + \Phi^s(L, 0)b_1^+ 
+ \int_0^L \Phi^s(L, y)[G_1^+(y) W_1^+(y) + \lambda B W_1^+(y) + \lambda^2 d_1 \tilde{H}_1^+(y) ] dy \\  
&- \Phi^s(-L, -L)a^-_{1} - \Phi^u_-(-L, 0)b_2^- - \int_0^{-L} \Phi^u(-L, y)[G_2^-(y) W_2^-(y) + \lambda B W_2^-(y) + \lambda^2 d_2 \tilde{H}_2^-(y) ] dy \\
&= P_u^+(L) a^+_{1} - P_s^-(-L) a^-_{1} + \Phi^s(L, 0)b_1^+ - \Phi^u(-L, 0)b_2^- \\
&+ \int_0^L \Phi^s(L, y)[G_1^+(y) W_1^+(y) + \lambda B W_1^+(y) + \lambda^2 d_1 \tilde{H}_1^+(y) ] dy \\
&+ \int_{-L}^0 \Phi^u(-L, y)[G_2^-(y) W_2^-(y) + \lambda B W_2^-(y) + \lambda^2 d_2 \tilde{H}_2^-(y) ] dy 
\end{align*}

Where we have the projections from Lemma 3.2 in Sandstede (1998)
\begin{align*}
P^s(x) &= \Phi^s(x,x) \\
P^u(x) &= \Phi^u(x,x) \\
\end{align*}

Recall that $a_i^- \in E^s$ and $a_i^+ \in E^u$. Writing the projections on $E^s$ and $E^u$ as $P_0^s$ and $P_0^u$, we have $P_0^s a_i^- = a_i^-$ and $P_0^u a_i^+ = a_i^+$. Using these above with $i = 1$, we get 

\begin{align*}
D_1 d &=  a^+_1 - a^-_1 + (P^u(L) - P_0^u) a^+_1 - (P_0^s - P^s(-L)) a^-_1 + \Phi^s(L, 0)b_1^+ - \Phi^u(-L, 0)b_2^- \\
&+ \int_0^L \Phi^s(L, y)[G_1^+(y) W_1^+(y) + \lambda B W_1^+(y) + \lambda^2 d_1 \tilde{H}_1^+(y) ] dy \\
&+ \int_{-L}^0 \Phi^u(-L, y)[G_2^-(y) W_2^-(y) + \lambda B W_2^-(y) + \lambda^2 d_2 \tilde{H}_2^-(y) ] dy 
\end{align*}

which we will write as

\begin{equation}
D_1 d = (a^+_1 - a^-_1) + L_3(\lambda)(a,b,d)
\end{equation}

where 

\begin{align*}
L_3(\lambda)(a,b,d) &= (P^u(L) - P_0^u) a^+_1 - (P_0^s - P^s(-L)) a^-_1 + \Phi^s(L, 0)b_1^+ - \Phi^u(-L, 0)b_2^- \\
&+ \int_0^L \Phi^s(L, y)[G_1^+(y) W_1^+(y) + \lambda B W_1^+(y) + \lambda^2 d_1 \tilde{H}_1^+(y) ] dy \\
&+ \int_{-L}^0 \Phi^u(-L, y)[G_2^-(y) W_2^-(y) + \lambda B W_2^-(y) + \lambda^2 d_2 \tilde{H}_2^-(y) ] dy 
\end{align*}

If we substitute $W = W_1(\lambda)(a,b,d)$ from Lemma \ref{W1}, we see that this is linear in $(a,b,d)$ and analytic in $\lambda$. Now we will show that $L_3(\lambda)$ is bounded. First, let

\[
p(L) = \sup_{x \geq L} (|P^u(x) - P_0^u| + |P_0^s - P^s(-x)|)
\]

Then for our bound we have

\begin{align*}
|L_3(\lambda)(a,b,d)| &\leq |P^u(L) - P_0^u|| a^+_1| + |P_0^s - P^s(-L)||a^-_1| + |\Phi^s(L, 0)||b_1^+| + |\Phi^u(-L, 0)||b_2^-| \\
&+ \int_0^L |\Phi^s(L, y)|[|G_1^+(y)||W_1^+(y)| + |\lambda| |B W_1^+(y)| + |\lambda|^2 |d_1| |\tilde{H}_1^+(y)| ] dy \\
&+ \int_{-L}^0 |\Phi^u(-L, y)|[|G_2^-(y)||W_2^-(y)| + |\lambda| |B W_2^-(y)| + |\lambda|^2 |d_2| |\tilde{H}_2^-(y)| ] dy 
\end{align*}

Doing the bounds one at a time, we have

\begin{enumerate}

\item
\begin{align*}
|P^u(L) - P_0^u|| a^+_1| + |P_0^s - P^s(-L)||a^-_1| \leq p(L)|a|
\end{align*}

\item
\begin{align*}
|\Phi^s(L, 0)||b_1^+| + |\Phi^u(-L, 0)||b_2^-| &\leq C e^{-\alpha^s L} |b_1^+| + C e^{-\alpha^u L}|b_2^-| \\
&\leq C e^{-\alpha L}|b|
\end{align*}

\item
\begin{align*}
\int_0^L |\Phi^s(L, y)|[|G_1^+(y)||W_1^+(y)| dy &\leq C|G| \int_0^L e^{-\alpha^s(L-y)}|W_1^+(y)| dy \\
&\leq C|G|||W_1^+|| \frac{1 - e^{-\alpha^s L}}{\alpha^s} \\
&\leq C|G|||W_1^+||
\end{align*}

Similarly,

\begin{align*}
\int_{-L}^0 |\Phi^u(-L, y)|[|G_2^-(y)||W_2^-(y)| dy &\leq C|G|||W_2^-|| \frac{1 - e^{-\alpha^u L}}{\alpha^s} \\
&\leq C|G|||W_2^-||
\end{align*}

Combining these, we have

\begin{align*}
\int_0^L |\Phi^s(L, y)|&|G_1^+(y)||W_1^+(y)| dy + \int_{-L}^0 |\Phi^u(-L, y)||G_2^-(y)||W_2^-(y)| dy \leq C|G| ||W||
\end{align*}

Note that none of this depended on $L$.

\item The next term is the same as the previous, except the $G_i^\pm$ is swapped out for a $B$. Thus we have

\begin{align*}
\int_0^L |\Phi^s(L, y)||BW_1^+(y)| dy + \int_{-L}^0 |\Phi^u(-L, y)||BW_2^-(y)| dy \leq C ||W||
\end{align*}

\item The final integral term involving $\tilde{H}_i^\pm$ is similar to what we did before.

\begin{align*}
\int_0^L |\Phi^s(L, y)|| \tilde{H}_1^+(y)| ] dy &\leq \int_0^L e^{-\alpha^s (L - y)}|BH(y)| dy + \int_0^L e^{-\alpha^s (L - y)}|B||\tilde{H}_i^+(y) - H(y)| dy \\
&\leq (||H|| + \Delta H)\frac{1 - e^{-\alpha^s L}}{\alpha^s} \\
&= C( ||H|| + \Delta H )
\end{align*}

Similarly,

\begin{align*}
\int_{-L}^0 |\Phi^u(-L, y)||\tilde{H}_2^-(y)| ] dy &\leq (||H|| + \Delta H)\frac{1 - e^{-\alpha^u L}}{\alpha^u} \\
&= C( ||H|| + \Delta H )
\end{align*}

\end{enumerate}

Combining all of this, we have a bound for $L_3(\lambda)$

\begin{align*}
|L_3(\lambda)(a,b,d)| &\leq C \left( p(L)|a| + e^{-\alpha L}|b| + (|G| + |\lambda|)||W|| + |\lambda|^2|d|(||H|| + \Delta H ) \right)
\end{align*}

Since we are substituting $W = W_1(\lambda)(a,b,d)$ from Lemma \ref{W1}, we can use our bound for $|W||$ from there to get

\begin{align*}
|L_3(\lambda)(a,b,d)| &\leq C \left( p(L)|a| + e^{-\alpha L}|b| + (|G| + |\lambda|)(|a| + |b| + |\lambda|^2 ( ||H|| + \Delta H ) |d|) + |\lambda|^2|d|(||H|| + \Delta H ) \right) \\
&\leq C \left( (p(L) + |G| + |\lambda|)|a| + (e^{-\alpha L} + |G| + |\lambda|)|b| + |\lambda|^2|d|(||H|| + \Delta H ) \right)
\end{align*}

Recalling that we are choosing $|G|\leq \delta$ and $|\lambda| \leq \delta$, we this becomes 

\[
|L_3(\lambda)(a,b,d)| \leq C\delta(|a| + |b| + |d|)
\]

Let $J_1: V_a \rightarrow \C^n$ be defined by $J_1(a) = a_1^+ - a_1^-$. (These are the only two nonzero components of $a$). Since $\C^n = E^u \oplus E^s$, the map $J_1$ is a linear isomorphism. Now consider the operator $S_1$ defined by

\[
S_1(a) = J_1 a + L_3(\lambda)(a, 0, 0) = J_1( I + J_1^{-1} L_3(\lambda)(a, 0, 0) )
\]

For suffiently small $\delta$, we will have the operator norm $||J_1^{-1} L_3(\lambda)(\cdot, 0, 0)|| < 1$, thus the map $a \rightarrow I + J_1^{-1} L_3(\lambda)(a, 0, 0)$ is invertible (for same reason as in Lemma \ref{W1}) and so the operator $S_1$ is invertible.\\

Recall that we have the following expression for $D_1 d$.

\[
D_1 d = (a^+_1 - a^-_1) + L_3(\lambda)(a,b,d)
\] 

Since $L_3$ is linear, we can write this as

\begin{align*}
D_1 d &= J_1 a + L_3(\lambda)(a, 0, 0) + L_3(\lambda)(0, b, d) \\
&= S_1(a) + L_3(\lambda)(0, b, d) 
\end{align*}

Writing this as 

\[
S_1(a) = D_1 d - L_3(\lambda)(0, b, d) 
\]

We can solve this for $a$ to get

\begin{equation}
a = A_1(\lambda)(b,d) = S_1^{-1}[D_1 d - L_3(\lambda)(0, b, d)] 
\end{equation}

We can find a bound for $A_1$. Note that $S_1^{-1}$ is a bounded linear operator.

\begin{align*}
|A_1(\lambda)(b,d)| &\leq ||S_1^{-1}|||D_1 d - L_3(\lambda)(0, b, d)| \\
&\leq C (|D_1||d| + |L_3(\lambda)(0, b, d)|) \\
&\leq C (|D_1||d| + (e^{-\alpha L} + |G| + |\lambda|)|b| + |\lambda|^2|d|(||H|| + \Delta H ) )\\
&= C (e^{-\alpha L} + |G| + |\lambda|)|b| + (|D_1| + |\lambda|^2(||H|| + \Delta H ))|d| )
\end{align*}

Next we define

\begin{equation}\label{W2def}
W_2(\lambda)(b,d) = W_1(\lambda)(A_1(\lambda)(b,d),b,d)
\end{equation}

We can get a bound on this by plugging in the bound for $A_1(\lambda)$ into that for $W_1(\lambda)$.

\begin{align*}
||W_2(\lambda)(a,b,d)||_\eta &\leq C(|A_1(\lambda)(b,d)| + |b| + |\lambda|^2 ( ||H|| + \Delta H ) |d|) \\
&\leq C((e^{-\alpha L} + |G| + |\lambda|)|b| + (|D_1| + |\lambda|^2(||H|| + \Delta H ))|d| + |b| + |\lambda|^2 ( ||H|| + \Delta H ) |d|) \\
&\leq C(|b| + (|D_1| + |\lambda|^2(||H|| + \Delta H ))|d|) \\
\end{align*} 

Next we take the equation

\[
D_1 d = (a^+_1 - a^-_1) + L_3(\lambda)(a,b,d)
\]

and project both sides by $P_0^s$ and $P_0^u$, the projections on $E^s$ and $E_u$. Recalling that $a^-_1 \in E^u$ and $a^+_1 \in E_s$, we have

\begin{align*}
a^-_1 &= -P_0^s D_1 d + P_0^s L_3(\lambda)(a,b,d) \\
a^+_1 &= P_0^u D_1 d - P_0^u L_3(\lambda)(a,b,d)
\end{align*}

Substituting $a = A_1(\lambda)(b,d)$ into this, we get

\begin{align*}
(A_1(\lambda)(b,d))^- &= -P_0^s D_1 d + P_0^s L_3(\lambda)(A_1(\lambda)(b,d),b,d) \\
(A_1(\lambda)(b,d))^+ &= P_0^u D_1 d - P_0^u L_3(\lambda)(A_1(\lambda)(b,d),b,d)
\end{align*}

Define $A_2(\lambda): V_\lambda \times V_b \times V_d \rightarrow V_a$ piecewise by
\begin{align*}
(A_2(\lambda)(b,d))^- &= P_0^s L_3(\lambda)(A_1(\lambda)(b,d),b,d) \\
(A_2(\lambda)(b,d))^+ &= - P_0^u L_3(\lambda)(A_1(\lambda)(b,d),b,d)
\end{align*}

Then we have
\begin{align*}
(A_1(\lambda)(b,d))^- &= -P_0^s D_1 d + (A_2(\lambda)(b,d))^-\\
(A_1(\lambda)(b,d))^+ &= P_0^u D_1 d + (A_2(\lambda)(b,d))^+
\end{align*}

Finally, we have the following estimate for $A_2(\lambda)$

\begin{align*}
|(A_2&(\lambda)(b,d))| \leq |P_0^{s/u}|| L_3(\lambda)(A_1(\lambda)(b,d),b,d) |\\
&\leq C \left( (p(L) + |G| + |\lambda|)|(A_1(\lambda)(b,d),b,d)| + (e^{-\alpha L} + |G| + |\lambda|)|b| + |\lambda|^2|d|(||H|| + \Delta H ) \right) \\
&\leq C ( (p(L) + |G| + |\lambda|)[(e^{-\alpha L} + |G| + |\lambda|)|b| + (|D_1| + |\lambda|^2(||H|| + \Delta H ))|d| ] \\
&+ (e^{-\alpha L} + |G| + |\lambda|)|b| + |\lambda|^2|d|(||H|| + \Delta H ) ) \\
&\leq C( (p(L) + |G| + |\lambda|)|b| + (|D_1|(p(L) + |G| + |\lambda|) + |\lambda|^2(||H|| + \Delta H ))|d| ) \\
\end{align*}

\end{proof}
\end{lemma}

At this point, we are ready to look at the remaining equations

\begin{align*}
W_i^\pm(x) &\in \C \Psi(0) \oplus Y^+ \oplus Y^- \\
W_i^+(0) - W_i^-(0) &\in \C \Psi(0) 
\end{align*}

First, we take $x = 0$ in the fixed point equations.

\begin{align*}
W_i^-(0) &= \Phi^s(0, -X_{i-1})a^-_{i-1} + \Phi^u(0, 0)b_i^- \\
&+ \int_{-X_{i-1}}^x \Phi^s(0, y)[G_i^-(y) W_i^-(y) + \lambda B W_i^-(y) + \lambda^2 d_i \tilde{H}_i^-(y)] dy \\
W_i^+(0) &= \Phi^u(0, X_i)a^+_{i} + \Phi^s(0, 0)b_i^+ \\
&+ \int_{X_{i}}^x \Phi^u(0, y)[G_i^+(y) W_i^+(y) + \lambda B W_i^+(y) + \lambda^2 d_i \tilde{H}_i^+(y) ] dy
\end{align*}

Recall that $\Phi^u(0, 0) = P^u(0)$ and $\Phi^s(0, 0) = P^s_+(0)$. Since $b_i^-$ and $b_i^+$ are in the range of these projections, $\Phi^u(0, 0)b_i^- = b_i^-$ and $\Phi^s(0, 0)b_i^+ = b_i^+ $. Thus we have

\begin{align*}
W_i^-(0) &= \Phi^s(0, -X_{i-1})a^-_{i-1} + b_i^- \\
&+ \int_{-X_{i-1}}^x \Phi^s(0, y)[G_i^-(y) W_i^-(y) + \lambda B W_i^-(y) + \lambda^2 d_i \tilde{H}_i^-(y)] dy \\
W_i^+(0) &= \Phi^u(0, X_i)a^+_{i} + b_i^+ \\
&+ \int_{X_{i}}^x \Phi^u(0, y)[G_i^+(y) W_i^+(y) + \lambda B W_i^+(y) + \lambda^2 d_i \tilde{H}_i^+(y) ] dy
\end{align*}

We use $W = W_2(\lambda)(b,d)$ and $a = A_1(\lambda)(b,d)$ in this. By the definition of $V_b$, we can decompose the $b_i^\pm$ uniquely as
\[
b_i^\pm = x_i^\pm + y_i^\pm
\]
where $x_i^\pm \in \C Q'(0)$ and $y_i^\pm \in Y^\pm$. 

Since

\[
\C^n = \C\Psi(0) \oplus \C Q'(0) \oplus Y^- \oplus Y^+
\]

the conditions above are equivalent to

\begin{align*}\label{projeq}
P(\C Q'(0), \C\Psi(0) \oplus Y^- \oplus Y^+)W_i^-(0) &= 0 \\
P(\C Q'(0), \C\Psi(0) \oplus Y^- \oplus Y^+)W_i^+(0) &= 0 \\
P(Y^+ \oplus Y^-, \C Q'(0) \oplus \C\Psi(0) )(W_i^+(0) - W_i^-(0)) &= 0 \\
\end{align*}

where $P(X,Y)$ is the projection onto $X$ with kernel $Y$.

\begin{lemma}
There exist operators 
\begin{align*}
B_1: V_\lambda \times V_d \rightarrow V_b \\
A_3: V_\lambda \times V_d \rightarrow V_a \\
W_3: V_\lambda \times V_d \rightarrow V_w
\end{align*}
such that 
\[
(a,b,w) = (A_3(\lambda)d, B_1(\lambda)d, W_3(\lambda)d)
\]
solves the thing we want for any $d$ and $\lambda$. The operators are analytic in $\lambda$ and linear in $d$. Bounds for them are given in the proof.

\begin{proof}
Substituting the fixed point equations evaluated at $x = 0$ into the projection equations above, we have 
\begin{align*}
0 &= x_i^- + P(\C Q'(0), \C\Psi(0) \oplus Y^- \oplus Y^+) \Big( \Phi^s(0, -X_{i-1})a^-_{i-1}  \\
&+ \int_{-X_{i-1}}^0 \Phi^s(0, y)[G_i^-(y) W_i^-(y) + \lambda B W_i^-(y) + \lambda^2 d_i \tilde{H}_i^-(y) ] dy \Big) \\
0 &= x_i^+ + P(\C Q'(0), \C\Psi(0) \oplus Y^- \oplus Y^+) \Big( \Phi^u(0, X_i)a^+_{i} \\
&+ \int_{X_{i}}^0 \Phi^u(0, y)[G_i^+(y) W_i^+(y) + \lambda B W_i^+(y) + \lambda^2 d_i \tilde{H}_i^+(y)(y) ] dy \Big)\\
0 &= y_i^+ - y_i^- + P(Y^+ \oplus Y^-, \C Q'(0) \oplus \C\Psi(0) )\Big( \Phi^u(0, X_i)a^+_{i} - \Phi^s(0, -X_{i-1})a^-_{i-1} \\
&+ \int_{X_{i}}^0 \Phi^u(0, y)[G_i^+(y) W_i^+(y) + \lambda B W_i^+(y) + \lambda^2 d_i \tilde{H}_i^+(y)(y) ] dy \Big)\\
&- \int_{-X_{i-1}}^0 \Phi^s(0, y)[G_i^-(y) W_i^-(y) + \lambda B W_i^-(y) + \lambda^2 d_i \tilde{H}_i^-(y)(y) ] dy \Big)
\end{align*}

We can write this in the form

\[ 
\begin{pmatrix}x_i^- \\ x_i^+ \\ y_i^+ - y_i^- \end{pmatrix} = (L_4(\lambda)(b,d))_i = 0
\]
where $L_4(\lambda)(b,d)$ is the rest of the RHS above.\\

We can get an estimate for $L_4(\lambda)$ as follows. Projections have norm 1, so those are not an issue. Since we need the maximum over the three components and all the terms in the first two are in the third, we only have to look at the third component. Since the two integrals are the same as in the definition of $L_3(\lambda)$, we have already done the initial bound.

\begin{align*}
|L_4(\lambda)(b,d)| &\leq C\left( e^{-\alpha^u L}|a_i^+| +  e^{-\alpha^s L}|a_i^-|
+ (|G| + |\lambda|)||W|| + |\lambda|^2|d|(||H|| + \Delta H ) \right) \\
&\leq C\left( e^{-\alpha L}|a|
+ (|G| + |\lambda|)||W|| + |\lambda|^2|d|(||H|| + \Delta H ) \right)
\end{align*}

Substituting in $A_1(\lambda(b,d)$ and $W_2(\lambda)(b,d)$ and using their bounds, we get

\begin{align*}
|L_4&(\lambda)(b,d)| \leq C\left( e^{-\alpha L}|A_1(\lambda(b,d)|
+ (|G| + |\lambda|)||W_2(\lambda)(b,d)|| + |\lambda|^2|d|(||H|| + \Delta H ) \right) \\
&\leq C( e^{-\alpha L}((e^{-\alpha L} + |G| + |\lambda|)|b| + (|D_1| + |\lambda|^2(||H|| + \Delta H ))|d| )) \\
&+ (|G| + |\lambda|)((|b| + (|D_1| + |\lambda|^2(||H|| + \Delta H ))|d|) )+ |\lambda|^2|d|(||H|| + \Delta H ) ) \\
&\leq C( (e^{-2 \alpha L} + |G| + |\lambda|)|b| + (|D_1|(e^{-\alpha L} + |G| + |\lambda|) + |\lambda|^2(||H|| + \Delta H ))|d|))
\end{align*}

Recalling that $b = x + y$, $|G| \leq \delta$, $|\lambda| \leq \delta$, and choosing $L$ sufficiently large so that $e^{-2 \alpha L} \leq \delta$, this becomes

\begin{align*}
|L_4&(\lambda)(b,d)| \leq C\delta(|x| + |y|) + C(|D_1|(e^{-\alpha L} + |G| + |\lambda|) + |\lambda|^2(||H|| + \Delta H ))|d|
\end{align*}

Since the map $J_2$ defined by
\[
J_2( (x_i^+, x_i^-),(y_i^+, y_i^-)) \rightarrow ( x_i^+, x_i^-, y_i^+ -  y_i^- )
\]

is an isomorphism, the operator

\[
S_2(x,y) = J_2(x+y) + L^4(\lambda)(x+y,0)
\]

is invertible (as in the previous lemma). Thus

\[
b = B_1(\lambda)(d) = -S_2^{-1}L_4(\lambda)(0,d)
\]

solves our problem. For the operator $B_1(\lambda)$, since $S_2^{-1}$ is a bounded linear operator, we have estimate

\begin{align*}
|B_1(\lambda)(d)| &\leq C |L_4(\lambda)(0,d)| \\
&\leq C(|D_1|(e^{-\alpha L} + |G| + |\lambda|) + |\lambda|^2(||H|| + \Delta H ))|d|
\end{align*}

Substituting this into the operators $A_1(\lambda)$ and $W_2(\lambda)$ from the previous lemma gives us operators $A_3$ and $W_3$, which have bounds

\begin{align*}
|A_3&(\lambda)(d)| \leq C (e^{-\alpha L} + |G| + |\lambda|)|B_1(\lambda)(d)| + (|D_1| + |\lambda|^2(||H|| + \Delta H ))|d| )\\
&\leq C (e^{-\alpha L} + |G| + |\lambda|)(|D_1|(e^{-\alpha L} + |G| + |\lambda|) + |\lambda|^2(||H|| + \Delta H ))|d|) + (|D_1| + |\lambda|^2(||H|| + \Delta H ))|d| ) \\
&\leq C (|D_1| + |\lambda|^2(||H|| + \Delta H ))|d|
\end{align*}

For $W_3(\lambda)$ we have the bound 

\begin{align*}
||W_3&(\lambda)(a,b,d)|| \leq C(|B_1(\lambda)(d)|  + (|D_1| + |\lambda|^2(||H|| + \Delta H ))|d|)  \\
&\leq C ((|D_1|(e^{-\alpha L} + |G| + |\lambda|) + |\lambda|^2(||H|| + \Delta H ))|d|)  + (|D_1| + |\lambda|^2(||H|| + \Delta H ))|d|) \\
&\leq C (|D_1| + |\lambda|^2(||H|| + \Delta H ))|d|
\end{align*} 

Using $A_2(\lambda)$ from the previous lemma, we can define

\[
A_4(\lambda)d = A_2(\lambda)(B_1(\lambda)d,d)
\]

which lets us write $A_3(\lambda)$ as 

\begin{align*}
(A_3(\lambda)(d))^- &= -P_0^s D_1 d + (A_4(\lambda)(d))^-\\
(A_3(\lambda)(d))^+ &= P_0^u D_1 d + (A_4(\lambda)(d))^+
\end{align*}

Plugging in the estimate for $B_1(\lambda)$ into that of $A_2(\lambda)$ from the previous lemma, we get an estimate for $A_4(\lambda)$.

\begin{align*}
|(A_4&(\lambda)d)| \\
&\leq C( (p(L) + |G| + |\lambda|)|B_1(\lambda)| + (|D_1|(p(L) + |G| + |\lambda|) + |\lambda|^2(||H|| + \Delta H ))|d| ) \\
&\leq C( (p(L) + |G| + |\lambda|)(|D_1|(e^{-\alpha L} + |G| + |\lambda|) + |\lambda|^2(||H|| + \Delta H ))|d|) \\
&+ (|D_1|(p(L) + |G| + |\lambda|) + |\lambda|^2(||H|| + \Delta H ))|d| ) \\
&\leq C(|D_1|(e^{-\alpha L} + |G| + |\lambda|) + |\lambda|^2(||H|| + \Delta H ))|d|
\end{align*}

Note that compared to (3.39) in Sandstede (1998), we have an extra $|\lambda|$ in the $D_1$ term. In that case, we had another term in $|\lambda|$ to absorb it. In this case, we have a term in $|\lambda^2|$ instead of $|\lambda|$, so we cannot absorb it. 

\end{proof}
\end{lemma}

We can use what we have done above to get a sharper estimate for $W_i^\pm$ in the unweighted norm, which we will need.

\begin{lemma}
We have the following estimates for $|W_i^\pm(x)|$

\begin{align*}
| W_i^-(x)| &\leq C ( (e^{-\alpha(x + X_{i-1})} + e^{-\alpha L} + |G| + |\lambda|)|D_1 + |\lambda|^2 (||H|| + \Delta H))|d|\\
| W_i^+(x)| &\leq C ( (e^{\alpha(x - X_{i})} + e^{-\alpha L} + |G| + |\lambda|)|D_1 + |\lambda|^2 (||H|| + \Delta H))|d|
\end{align*}
where $\alpha = \min(\alpha^s, \alpha^u)$.

\begin{proof}
Since we have 
\[
W_i^\pm = L_1(\lambda)W_i^\pm + L_2(\lambda)W_i^\pm 
\]

we can use the estimate for $L_1(\lambda)$ together with an improved estimate for $L_2(\lambda)$. We use the same estimate for $L_1(\lambda)$ as we have above. We will do the negative piece here. The positive piece is similar.

\[
| (L_1(\lambda) W_i^-)(x) | \leq C\left(|G| +|\lambda|\right)||W||
\]

For $L_2(\lambda)$ we will use the following estimate for the negative piece, which comes from the proof of Lemma \ref{L2}.

\begin{align*}
| (L_2(\lambda)(a, b, d)_i^-)(x)| &\leq C \left( e^{-\alpha^s(x + X_{i-1})} |a^-_{i-1}| + |b_i^-| + |\lambda|^2 (||H|| + \Delta H)|d_i| \right)
\end{align*}

Combining these, we have

\begin{align*}
| W_i^-(x)| &\leq C \left( e^{-\alpha^s(x + X_{i-1})} |a^-_{i-1}| + |b_i^-| + \left(|G| +|\lambda|\right)||W||_\eta + |\lambda|^2 (||H|| + \Delta H)|d_i|  \right)
\end{align*}

Substituting in $A_3(\lambda)$, $B_1(\lambda)$, and $W_3(\lambda)$, this estimate becomes

\begin{align*}
| W_i^-(x)| &\leq C \left( e^{-\alpha^s(x + X_{i-1})} |A_3(\lambda)| + |B_1(\lambda)| + \left(|G| +|\lambda|\right)||W_3(\lambda)|| + |\lambda|^2 (||H|| + \Delta H)|d_i|  \right) \\
&= C e^{-\alpha^s(x + X_{i-1})} (|D_1| + |\lambda|^2(||H|| + \Delta H ))|d|)\\
&+ C (|D_1|(e^{-\alpha L} + |G| + |\lambda|) + |\lambda|^2(||H|| + \Delta H ))|d|\\
&+ C (|G +|\lambda|)(|D_1| + |\lambda|^2(||H|| + \Delta H ))|d| + C |\lambda|^2 (||H|| + \Delta H)|d| \\
&\leq C ( (e^{-\alpha(x + X_{i-1})} + e^{-\alpha L} + |G| + |\lambda|)|D_1 + |\lambda|^2 (||H|| + \Delta H))|d|
\end{align*}

Similarly

\begin{align*}
| W_i^+(x)| \leq C ( (e^{\alpha(x - X_{i})} + e^{-\alpha L} + |G| + |\lambda|)|D_1 + |\lambda|^2 (||H|| + \Delta H))|d|
\end{align*}

\end{proof}

\end{lemma}

Next, we use this bound to estimate some integrals.

\begin{lemma}\label{intG}
We have the following bounds for these integrals involving $W_i^\pm$

\begin{align*}
\left| \int_0^{X_i} \langle \Psi(x), G_i^+(x) W_i^+(x) \rangle dx \right| &\leq C|G| \left( \left(e^{-\alpha L} + |G| + |\lambda|  \right) |D_1| + |\lambda|^2 \left(||H|| + \Delta H\right) \right)|d|\\
\left| \int_{-X_{i-1}}^0 \langle \Psi(x), G_i^-(x) W_i^-(x) \rangle dx \right| &\leq C|G| \left( \left(e^{-\alpha L} + |G| + |\lambda|  \right) |D_1| + |\lambda|^2 \left(||H|| + \Delta H\right) \right)|d|\\
\left| \int_0^{X_i} \langle \Psi(x), B W_i^+(x) \rangle dx \right| &\leq C \left( \left(e^{-\alpha L} + |G| + |\lambda|  \right) |D_1| + |\lambda|^2 \left(||H|| + \Delta H\right) \right)|d|\\
\left| \int_{-X_{i-1}}^0 \langle \Psi(x), B W_i^-(x) \rangle dx \right| &\leq C \left( \left(e^{-\alpha L} + |G| + |\lambda|  \right) |D_1| + |\lambda|^2 \left(||H|| + \Delta H\right) \right)|d|
\end{align*}

\begin{proof}

For the adjoint solution $\Psi$, we use the estimate

\[
|\Psi(x)| \leq C e^{\tilde{\alpha}|x|}
\]

Using the estimate from the previous lemma,
\begin{align*}
\left| \int_0^{X_i} \langle \Psi(x), G_i^+(x) W_i^+(x) \rangle dx \right| &\leq \int_0^{X_i} |\Psi(x)||G| |W_i^+(x)| dx \\
&\leq C|G| \int_0^{X_i} e^{-\tilde{\alpha} x} \left( (e^{-\alpha(x + X_{i-1})} + e^{-\alpha L} + |G| + |\lambda|)|D_1 + |\lambda|^2 (||H|| + \Delta H)\right) |d| dx 
\end{align*}

For the first term inside the integral,

\begin{align*}
e^{-\tilde{\alpha} x} e^{\alpha(x - X_{i})} &= e^{-\alpha X_i} e^{-(\tilde{\alpha} - \alpha)x}
\end{align*}

By our hyperbolicity assumption (CHECK THIS), we can make the constant $\tilde{\alpha}$ a little larger than $\alpha$ so that the above expression decays since $x \geq 0$. Thus we have

\begin{align*}
&\left| \int_0^{X_i} \langle \Psi(x), G_i^+(x) W_i^+(x) \rangle dx \right| \\
&\leq C|G| \int_0^{\infty} \left(  e^{-(\tilde{\alpha} - \alpha)x} e^{-\alpha X_i} |D_1| +  e^{-\tilde{\alpha} x} \left( (e^{-\alpha L} + |G| + |\lambda| ) |D_1| + |\lambda|^2 ( ||H|| + \Delta H )\right) \right) |d|dx \\
&\leq C|G| \left(  \frac{1}{\tilde{\alpha} - \alpha}e^{-\alpha X_i}|D_1| + \frac{1}{\tilde{\alpha}}\left( (e^{-\alpha L} + |G| + |\lambda| ) |D_1| + |\lambda|^2 ( ||H|| + \Delta H )\right) \right)|d| \\
&\leq C|G| \left( \left(e^{-\alpha L} + |G| + |\lambda|  \right) |D_1| + |\lambda|^2 \left(||H|| + \Delta H\right) \right)|d|
\end{align*}

where in the last line we took $X_i = L$, as is the case for the double pulse. Similarly,

\begin{align*}
\left| \int_{-X_{i-1}}^0 \langle \Psi(x), G_i^-(x) W_i^-(x) \rangle dx \right| &\leq C|G| \left( \left(e^{-\alpha L} + |G| + |\lambda|  \right) |D_1| + |\lambda|^2 \left(||H|| + \Delta H\right) \right)|d|
\end{align*}

The other two bounds are essentially identical.

\end{proof}
\end{lemma}

We now proceed to compute the jump along $\Psi(0)$ by taking $x = 0$ in the fixed point equations, analogous to (3.48) in Sandstede (1998).

\begin{align*}
\langle \Psi(0), &W_i^-(0) - W_i^+(0) \rangle \\
= &\langle \Psi(-X_{i-1}), a_{i-1}^-\rangle - \langle \Psi(X_i), a_{i}^+\rangle \\
&+\left( \int_{-X_{i-1}}^0 \langle \Psi(y), G_i^- W_i^-(y) \rangle dy - \int_{X_i}^0 \langle \Psi(y), G_i^+ W_i^+(y) \rangle dy \right) \\
&+ \lambda\left( \int_{-X_{i-1}}^0 \langle \Psi(y), B W_i^-(y) \rangle dy - \int_{X_i}^0 \langle \Psi(y), B W_i^+(y) \rangle dy \right) \\
&+\lambda^2 d_i \left( \int_{-X_{i-1}}^0 \langle \Psi(y), \tilde{H}_i^-(y) \rangle dy - \int_{X_i}^0 \langle \Psi(y), \tilde{H}_i^+(y) \rangle dy \right)
\end{align*}

where the terms involving the $b^\pm$ vanish since $b^\pm \perp \Psi(0)$.\\

To do our final estimate, we look at the terms in the jump one at a time, using the estimates we have found above. For all of these use use the estimate

\[
|\Psi(x)| \leq C e^{-\alpha|x|}
\]

\begin{enumerate}

\item For the term involving $a$, we plug in $A_3(\lambda)$ and use the estimate from $A_4(\lambda)$.

\begin{align*}
\langle \psi(L), a_i^+ \rangle &= \langle \psi(L), P_0^u D_1 d \rangle + \langle \psi(L), (A_4(\lambda)(d))^+\rangle\\
&= \langle \psi(L), P_0^u D_1 d \rangle + e^{-\alpha L} \mathcal{O} \left(|D_1|(e^{-\alpha L} + |G| + |\lambda|) + |\lambda|^2(||H|| + \Delta H )\right)|d|\\
&= \langle \psi(L), P_0^u D_1 d \rangle + \mathcal{O} \left( e^{-\alpha L} \left(|D_1|(e^{-\alpha L} + |G| + |\lambda|) + |\lambda|^2(||H|| + \Delta H )\right)\right)|d|\\
\end{align*}

Similarly,

\begin{align*}
\langle \psi(-L), a_{i-1}^- \rangle &= \langle \psi(-L), -P_0^s D_1 d \rangle + \langle \psi(-L), (A_4(\lambda)(d))^-\rangle\\
&= -\langle \psi(-L), P_0^s D_1 d \rangle + e^{-\alpha L} \mathcal{O} \left(|D_1|(e^{-\alpha L} + |G| + |\lambda|) + |\lambda|^2(||H|| + \Delta H )\right)|d|\\
&= -\langle \psi(-L), P_0^s D_1 d \rangle + \mathcal{O} \left( e^{-\alpha L} \left(|D_1|(e^{-\alpha L} + |G| + |\lambda|) + |\lambda|^2(||H|| + \Delta H )\right)\right)|d|\\
\end{align*}

Recall that $a_0^i = a_2^+ = 0$, so for the double pulse, only one of these two terms will be nonzero.

\item For the terms involving $G_1^\pm$, we use Lemma \ref{intG} to get 

\begin{align*}
&\left| \int_{-X_{i-1}}^0 \langle \Psi(y), G_1^- W_1^-(y) \rangle dy - \int_{X_i}^0 \langle \Psi(y), G_1^- W_1^-(y) \rangle dy\right| \\
&\leq C|G| \left( \left(e^{-\alpha L} + |G| + |\lambda|  \right) |D_1| + |\lambda|^2 \left(||H|| + \Delta H\right) \right)|d|
\end{align*}

\item For the terms involving $B W_1^\pm$, we use Lemma \ref{intG} again to get

\begin{align*}
\Big| \int_{-X_{i-1}}^0 &\langle \Psi(y), B W_1^-(y) \rangle dy - \int_{X_i}^0 \langle \Psi(y), B W_1^+(y) \rangle dy \Big| \\
&\leq C \left( \left(e^{-\alpha L} + |G| + |\lambda|  \right) |D_1| + |\lambda|^2 \left(||H|| + \Delta H\right) \right)|d|
\end{align*}

\item From the term involving $\tilde{H}$ we will get our higher order Melnikov integral.

\begin{align*}
\int_{-X_{i-1}}^0 &\langle \Psi(y), \tilde{H}_i^-(y) \rangle dy - \int_{X_i}^0 \langle \Psi(y), \tilde{H}_i^+(y) \rangle dy \\
&= \left( \int_{-X_{i-1}}^0 \langle \Psi(y), H(y) \rangle dy - \int_{X_i}^0 \langle \Psi(y), H(y) \rangle dy \right) \\
&+ \left( \int_{-X_{i-1}}^0 \langle \Psi(y), B (\tilde{H}_i^-(y) - H(y)) \rangle dy - \int_{X_i}^0 \langle \Psi(y), B (\tilde{H}_i^+(y) - H(y)) \rangle dy \right) 
\end{align*}

For the second pair of integrals,

\begin{align*}
\left| \int_{-X_{i-1}}^0 \langle \Psi(y), B (\tilde{H}_i^-(y) - H(y)) \rangle dy - \int_{X_i}^0 \langle \Psi(y), B (\tilde{H}_i^+(y) - H(y)) \rangle dy \right| &\leq C \frac{1}{\alpha} \Delta H = C \Delta H
\end{align*}

For the first pair of integrals,

\begin{align*}
\int_{-X_{i-1}}^0 &\langle \Psi(y), H(y) \rangle dy - \int_{X_i}^0 \langle \Psi(y), H(y) \rangle dy \\
&= \int_{-X_{i-1}}^{X_i} \langle \Psi(y), H(y) \rangle dy \\
&= \int_{-\infty}^{\infty} \langle \Psi(y), H(y) \rangle dy - \int_{-\infty}^{-X_{i-1}} \langle \Psi(y), H(y) \rangle dy - \int_{X_i}^{\infty} \langle \Psi(y), H(y) \rangle dy 
\end{align*}

The first integral on the RHS is our higher order Melnikov integral. We can bound the other two integrals easily.

\begin{align*}
\left| \int_{-\infty}^{-X_{i-1}} \langle \Psi(y), H(y) \rangle dy - \int_{X_i}^{\infty} \langle \Psi(y), H(y) \rangle dy \right| \leq C e^{-\alpha L}||H||
\end{align*}

\end{enumerate}

Now we combine all this stuff and see what we get for the jump.

\begin{align*}
\xi = \langle &\Psi(0), W_1^-(0) - W_1^+(0) \rangle = -\langle \psi(-L), P_0^s D_1 d \rangle - \langle \psi(L), P_0^u D_1 d \rangle + \lambda^2 d_i \int_{-\infty}^{\infty} \langle \Psi(y), H(y) \rangle dy + (R(\lambda)d)_i
\end{align*}

where the remainder term has bound

\begin{align*}
|(R&(\lambda)d)_i| \leq C\left( e^{-\alpha L} \left(|D_1|(e^{-\alpha L} + |G| + |\lambda|) + |\lambda|^2(||H|| + \Delta H )\right)\right)|d| \\
&+ C|G| \left( \left(e^{-\alpha L} + |G| + |\lambda|  \right) |D_1| + |\lambda|^2 \left(||H|| + \Delta H\right) \right)|d| \\
&+ C |\lambda| \left( \left(e^{-\alpha L} + |G| + |\lambda|  \right) |D_1| + |\lambda|^2 \left(||H|| + \Delta H\right) \right)|d| \\
&+ C |\lambda^2| (e^{-\alpha L}||H|| + \Delta H)|d| \\
&\leq C \left( (e^{-\alpha L} + |G| + |\lambda|)^2 |D_1| + ((||H|| + \Delta H)(e^{-\alpha L} + |G| + |\lambda|) + \Delta H )|\lambda^2| \right)|d|
\end{align*}

\subsubsection*{The Substitution}

We make the following substitutions which are appropriate to our specific case.

\begin{align*}
H &= B Q^\eta_c \\
\tilde{H} &= B \tilde{Q}^\eta_c \\
G_i^\pm &= G(\eta)_i^\pm
\end{align*}

We also need estimates for $||H||$ and $\Delta H$ for the remainder estimate. Recall that we showed that $q_c$ decays exponentially asymptotically with rate $\alpha$. We also chose $\eta$ so that $0 < \eta < \alpha$. Thus $||e^{\alpha x} q_c(x)|| \leq C$. Since the only nonzero component of $B Q^\eta c$ is $e^{\eta x} q_c(x)$, we have for $x \geq 0$

\[
\sup_{x \geq 0} |e^{\eta x} q_c(x)| \leq \sup_{x \geq 0} |e^{\alpha x} q_c(x)| = ||e^{\alpha x} q_c(x)|| \leq C
\]

and for $x \leq 0$, since $q_c$ is bounded we have

\[
\sup_{x \leq 0} |e^{\eta x} q_c(x)| \leq \sup_{x \leq 0} |q_c(x)| \leq C
\]

Thus $||H||$ really is a constant and we can take $||H|| = C$. Now we deal with $\Delta H$. We (should) have from Lemma \ref{estimates} that

\begin{align*}
| B \tilde{Q}^\eta_c(x) - B Q^\eta_c(x) | &\leq C \sup_{|x| \geq L} |e^{\eta x} q(x)| \\
&\leq C \sup_{|x| \geq L} |e^{\eta x} q(x)| \\
&\leq C e^{-(\alpha - \eta)L}
\end{align*}

Thus we can take 

\[
\Delta H = C e^{-(\alpha - \eta)L}
\]

and so the remainder term has estimate

\begin{align*}
|(R&(\lambda)d)_i| \leq C \left( (e^{-\alpha L} + |G| + |\lambda|)^2 |D_1| + (e^{-\alpha L} + |G| + |\lambda| + e^{-(\alpha - \eta)L})|\lambda^2| \right)|d| \\
&\leq C \left( (e^{-\alpha L} + |G| + |\lambda|)^2 |D_1| + (|G| + |\lambda| + e^{-(\alpha - \eta)L})|\lambda^2| \right)|d| \\
\end{align*}
From above, the higher order Melnikov integral is

\begin{align*}
\int_{-\infty}^{\infty} \langle \Psi(y), H(y) \rangle dy &= \int_{-\infty}^{\infty} e^{-\eta y} q(y) e^{\eta y} q_c(y) dy \\
&= \langle q, q_c \rangle_{L^2(\R)}
\end{align*}

which agrees with Pego and Weinstein. \\

For the terms involving $a_i^\pm$, recall from Lemma \ref{estimates} that $D_1$ is given by

\[
D_1 d = [Q^\eta_x(-L) + Q^\eta_x(L) + 
\lambda(Q^\eta_c(-L) + Q^\eta_c(L))](d_2 - d_1) + \mathcal{O}\left(e^{-\alpha L} |d| \sup_{|x| \geq L} |e^{\eta x} Q(x)| \right)
\]
The remainder term here is order $e^{(2 \alpha - \eta)L}$. When we take the inner product with $\Psi(\pm L)$, the whole thing has order $e^{(3 \alpha - \eta)L}$ and can thus be tossed into the overall remainder term, which gives us a remainder estimate of 

\begin{align*}
|(R&(\lambda)d)_i| \leq C \left( e^{(3 \alpha - \eta)L} + (e^{-\alpha L} + |G| + |\lambda|)^2 |D_1| + (|G| + |\lambda| + e^{-(\alpha - \eta)L})|\lambda^2| \right)|d| \\
\end{align*}

The highest order terms in $D_1$ are $Q^\eta_x(-L) + Q^\eta_x(L)$, which are order $e^{-(\alpha - \eta)L}$, so putting that in the remainder, we get 

\begin{align*}
|(R&(\lambda)d)_i| \leq C \left( (e^{-\alpha L} + |G| + |\lambda|)^2 e^{-(\alpha - \eta)L} + (|G| + |\lambda| + e^{-(\alpha - \eta)L})|\lambda^2| \right)|d| \\
\end{align*}

If we choose $\delta \leq e^{-\alpha L}$, the overall order of the remainder term is $e^{-(3\alpha - \eta) L}$ which is higher order than $e^{-2 \alpha L}$, the order of the higher order terms (hopefully!). \\

Now we deal with the rest of the $a_i^\pm$ terms. As in Sandstede (1998), we should be able to neglect the terms involving inner products with the same sign in front of $L$, since they are higher order terms. Similarly, we should be able to drop the projection operator in the terms involving inner products with opposite signs, since these projection are approximately the identity. This is true by how we set the problem up in the first place, I think. So we will do that for now, although we should really look at this more closely. Thus to leading order we should have

\begin{align*}
\langle \Psi(L), P_0^u [Q^\eta_x(-L) + Q^\eta_x(L) + 
\lambda(Q^\eta_c(-L) + Q^\eta_c(L))](d_2 - d_1) \rangle &= \langle \Psi(L), Q^\eta_x(-L) + \lambda Q^\eta_c(-L)\rangle (d_2 - d_1) 
\end{align*}

This is all fine and stuff, but we do not want this to involve $\lambda$. We can take care of that by noting that $\langle \Psi(L), \lambda Q^\eta_c(-L) \rangle$ has order $e^{-(2\alpha - \eta)L}|\lambda|$, thus we can add that term to our remainder term to get

\begin{align*}
|(R&(\lambda)d)_i| \leq C \left( (e^{-\alpha L} + |G| + |\lambda|)^2 e^{-(\alpha - \eta)L} + e^{-(2\alpha - \eta)L}|\lambda| + (|G| + |\lambda| + e^{-(\alpha - \eta)L})|\lambda^2| \right)|d| \\
\end{align*}

This does not change the overall order of the remainder term, so it should be legit. Thus we have

\begin{align*}
\langle \Psi(L), &P_0^u [Q^\eta_x(-L) + Q^\eta_x(L) + 
\lambda(Q^\eta_c(-L) + Q^\eta_c(L))](d_2 - d_1) \rangle \\
&= \langle \Psi(L), Q^\eta_x(-L) \rangle (d_2 - d_1) + \mathcal{O}(e^{-(\alpha - \eta) L}(e^{-\alpha L} + |\lambda|))
\end{align*}

Similary, we have

\begin{align*}
\langle \Psi(-L), &P_0^s [Q^\eta_x(-L) + Q^\eta_x(L) + 
\lambda(Q^\eta_c(-L) + Q^\eta_c(L))](d_2 - d_1) \rangle \\
&= \langle \Psi(-L), Q^\eta_x(L) \rangle (d_2 - d_1) + \mathcal{O}(e^{-(\alpha - \eta) L}(e^{-\alpha L} + |\lambda|))
\end{align*}


Putting these together, we have to solve the following system for $(d_1, d_2)$.

\begin{align*}
d_1 \lambda^2 \langle q, q_c \rangle_{L^2(\R)} - \langle \Psi(L), Q^\eta_x(-L) \rangle (d_2 - d_1)  + (R(\lambda)d)_1 = 0\\
d_2 \lambda^2 \langle q, q_c \rangle_{L^2(\R)} - \langle \Psi(-L), Q^\eta_x(L) \rangle (d_2 - d_1)  + (R(\lambda)d)_2 = 0
\end{align*}

We want this to have a solution in $d = (d_1, d_2)$ which is not the trivial solution. Thus if we write this in matrix form, that is equivalent to the matrix not being invertible.\\

First let

\begin{align*}
a &= \langle \Psi(L), Q^\eta_x(-L) \rangle \\
\tilde{a} &= \langle \Psi(-L), Q^\eta_x(L) \rangle
\end{align*}

Define the matrix

\begin{align*}
A = \begin{pmatrix}
a & -a \\ \tilde{a} & -\tilde{a} 
\end{pmatrix}
\end{align*}

Then we can define

\[
S(\lambda) = A + \lambda^2 MI + R(\lambda) 
\]

where $M = \lambda^2 \langle q, q_c \rangle_{L^2(\R)}$ is the higher-order Melnikov integral. The system above is equivalent to $S(\lambda)d = 0$. Thus our condition to have a solution, i.e. the condition to have both jumps be zero and thus a match happens, is

\[
\det S(\lambda) = 0
\]

\end{document}
