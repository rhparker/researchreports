\documentclass[12pt]{article}
\usepackage[pdfborder={0 0 0.5 [3 2]}]{hyperref}%
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}%
\usepackage[shortalphabetic]{amsrefs}%
\usepackage{amsmath}
\usepackage{enumerate}
% \usepackage{enumitem}
\usepackage{amssymb}                
\usepackage{amsmath}                
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage[table,xcdraw]{xcolor}
\usepackage{tikz}
\usepackage{float}
\usepackage{booktabs}
\usepackage{svg}
\usepackage{mathtools}
\usepackage{cool}
\usepackage{url}
\usepackage{graphicx,epsfig}
\usepackage{makecell}
\usepackage{array}

\def\noi{\noindent}
\def\T{{\mathbb T}}
\def\R{{\mathbb R}}
\def\N{{\mathbb N}}
\def\C{{\mathbb C}}
\def\Z{{\mathbb Z}}
\def\P{{\mathbb P}}
\def\E{{\mathbb E}}
\def\Q{\mathbb{Q}}
\def\ind{{\mathbb I}}

\graphicspath{ {periodic/} }

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{hypothesis}{Hypothesis}

\newtheorem{notation}{Notation}

\begin{document}

\section{KdV5 with Periodic Boundary Conditions}

\subsection{Background}

In this section we will look at multipulse solutions to KdV5 in the case where we have periodic boundary conditions. Note that we cannot use an exponential weight in this case since our domain is of finite length. A double periodic pulse is parameterized by lengths $X_1$ and $X_2$, which is shown in the diagram below

\begin{figure}[H]
\includegraphics[width=8.5cm]{dpimage}
\end{figure}

We would like to write the periodic double pulse as a small (piecewise) perturbation of the single pulse as was done in San98 (and San93). The method in those papers, however, does not give us the estimates we need. Instead, we will give another construction based on San97. This boils down to proving existence of the periodic double pulse, which is what we shall do. We assume that $c > 1/4$, since that is necessary for the existence of multipulses to KdV5. Thus the eigenvalues of the linearization about $0$ are complex and given by $\pm \alpha \pm \beta i$.\\

Since we are only interested in the existence problem here, we can use the 4th order (integrated) equation in the traveling frame

\begin{equation}\label{4thorder}
f(u) = u_{xxxx} - u_{xx} + c u - u^2 = 0
\end{equation}

This equation is Hamiltonian, with energy 

\begin{equation}\label{4thorderE}
H(u) = u_x u_{xxx} - \frac{1}{2}u_{xx}^2 - \frac{1}{2}u_x^2 + \frac{c}{2}u^2 - \frac{1}{3}u^3
\end{equation}

The Hamiltonian is conserved for any solution $u$ to \eqref{4thorder} since

\begin{align*}
\frac{d}{dx}H(u) = u_x(u_{xxxx} - u_{xx} + c u - u^2) = 0
\end{align*}

In what follows, we will write the problem as a first order system, i.e. we will take $U = (u_1, u_2, u_3, u_4) = (u, u_x, u_{xx}, u_{xxx})$. Then the problem becomes

\begin{equation}\label{4thordersystem}
U' = F(U) = \begin{pmatrix} 
u_2 \\ u_3 \\ u_4 \\ u_3 - c u_1 + u_1^2
\end{pmatrix}
\end{equation}

The Hamiltonian then takes the form

\begin{equation}
H(u_1, u_2, u_3, u_4) = u_2 u_4 - \frac{1}{2}u_3^2 - \frac{1}{2}u_2^2 + \frac{c}{2}u_1^2 - \frac{1}{3}u_1^3
\end{equation}

The gradient of the Hamiltonian is

\begin{equation}
\nabla H = (cu_1 - u_1^2, -u_2 + u_4, -u_3, u_2)
\end{equation}

It is straightforward to verify that $\langle F(U), \nabla H(U) \rangle = 0$ for all $U$.\\

The way we have written this, we can write our equation as $U' = \tilde{J} \nabla H$, where

\[
\tilde{J} = \begin{pmatrix}
0 & 0 & 0 & 1 \\
0 & 0 & -1 & 0 \\
0 & 1 & 0 & 1 \\
-1 & 0 & -1 & 0 \\
\end{pmatrix}
\]

This is not the standard symplectic matrix $J$, but $\tilde{J}$ is invertible and skew symmetric. \\

We note that 0 is an equilibrium of the system. Let $Q(x)$ be a homoclinic orbit connecting the equilibrium at 0 to itself. Such a homoclinic orbit is known to exist. Then since $Q(0)$ is not an equilibrium point of the system, we also know that $\nabla H(Q(0)) \neq 0$ since $\tilde{J}$ is invertible.

\subsection{Existence and Construction of Periodic Double Pulse}

We look at the general equation

\begin{equation}\label{generaleq}
U'(x) = F(U)
\end{equation}

where $U \in \R^4$ and $F$ is real-valued. (This should work for $\R^{2m}$ with $m \geq 2$, but it is easier to consider this case. This should also extend to the complex-valued case.)\\

We take the following assumptions.

\begin{hypothesis}\label{assumptions}
\[\]
\begin{enumerate}
	\item $U = 0$ is a hyperbolic equilibrium of \eqref{generaleq}.
	\item The spectrum of $D_U F(0)$ is $\pm \alpha \pm i \beta$, $\alpha, \beta > 0$, thus $D_U F(0)$ has two stable and two unstable directions.
	\item There exists a homoclinic orbit $Q(x)$ connecting the equilibrium at 0 to itself.
	\item We have a nondegeneracy condition
	\[
	T_{Q(0)} W^u(0) \cap T_{Q(0)} W_s(0) = \R Q'(0)
	\]
	\item The system is either reversible or Hamiltonian. In the Hamiltonian case, $\nabla H(Q(0)) \neq 0$.
\end{enumerate}
\end{hypothesis}

By the nondegeneracy hypothesis, there exists a unique, bounded solution $\Psi(x)$ to the adjoint variational equation

\begin{equation}
W' = -D_U F(Q(x))^* W
\end{equation}

In the case where the system is Hamiltonian, we actually know the specific form of $\Psi(x)$.

% Lemma : Hamiltonian case

\begin{lemma}

If $U'(x) = F(U)$ is Hamiltonian, then

\begin{equation}
\Psi(x) = \nabla H(Q(x))
\end{equation}

where $H$ is the Hamiltonian energy associated with the system.

\begin{proof}

If the system is Hamiltonian, then $\langle F(Q(x)), \nabla H(Q(x)) \rangle = 0$ for all $x$. Taking the gradient of this and using known vector calculus identities, we have

\begin{align*}
0 &= \nabla \langle F(Q(x)), \nabla H(Q(x)) \rangle \\
&= D F(Q(x))^* \nabla H(Q(x)) + D^2 H(Q(x))^* F(Q(x)) \\
&= D F(Q(x))^* \nabla H(Q(x)) + D^2 H(Q(x)) Q'(x) \\
&= D F(Q(x))^* \nabla H(Q(x)) + \frac{d}{dx} \nabla H(Q(x))
\end{align*}

since the Hessian matrix is self-adjoint. Rearranging this, we have

\begin{equation}
\frac{d}{dx} \nabla H(Q(x)) = -D F(Q(x))^* \nabla H(Q(x)) 
\end{equation}

thus $\nabla H(Q(x))$ is a solution to the adjoint variational equation. Since $\nabla H$ is continuous and $Q(x)$ is bounded (even better, it decays exponentially to 0), so is $\nabla H(Q(x))$. By the nondegeneracy condition, there is a unique such solution, so we must have $\Psi(x) = \nabla H(Q(x))$.

\end{proof}
\end{lemma}

Next, we parameterize the stable and unstable manifolds near $Q(0)$. To do that we first write

\begin{align*}
T_{q(0)}W^u(0) &= \R Q'(0) \oplus Y^- \\
T_{q(0)}W^s(0) &= \R Q'(0) \oplus Y^+ \\
Z &= \R \Psi(0)
\end{align*}

By Hypothesis \ref{assumptions},

\[
\R^4 = \R Q'(0) \oplus Y^+ \oplus Y^- \oplus Z
\]
 
We have shown before that $Z$ is perpendicular to the other three. \\

We now write $W^u(0)$ and $W^s(0)$ as graphs over their own tangent spaces near $Q(0)$. Following San97, we can parameterize the unstable and stable manifolds near $Q(0)$ by the smooth functions

\begin{align*}
Q^-(\alpha, \beta^-) \\
Q^+(\alpha, \beta^+)
\end{align*}

where $\alpha \in \R$, $\beta^\pm \in Y^\pm$ and we have set things up so that $Q^+(\alpha, 0) - Q^-(\alpha, 0) \in Z$. Note that $Q^+(0, 0) = Q^-(0, 0) = Q(0)$. The parameter $\alpha$ is a phase parameter (which we will always take to be 0); although we could ignore it in our parameterization, we include it here to indicate that both manifolds are two-dimensional. \\

Next, we define the trajectories

\begin{align*}
Q^-(\alpha, \beta^-)(x) && x \leq 0 \\
Q^+(\alpha, \beta^-)(x) && x \geq 0
\end{align*}

which are the unique solutions to $U' = F(U)$ with ICs $Q^\pm(\alpha, \beta^\pm)$ for appropriate $x$. Note that since the ICs are on the stable/unstable manifolds, these decay exponentially in the appropriate direction with rate $\alpha$.\\

Using the parameterizations of the stable and unstable manifolds, we write $U$ piecewise (in four pieces, $i = 1, 2$) as

\begin{align*}
U_i^-(x) &= Q^-(0, \beta_i^-)(x) + V_i^-(x) && x \leq 0 \\
U_i^+(x) &= Q^+(0, \beta_i^+)(x) + V_i^+(x) && x \geq 0
\end{align*}

In essence, we are writing $U$ in terms of four parameters $\beta_i^\pm$ (representing ICs on the stable/unstable manifolds) and four ``remainder'' functions $V_i^\pm(x)$.\\

We will choose $V_i^\pm(x)$ so that

\begin{align*}
V_i^-(0) &\in Z \oplus Y^- \\
V_i^+(0) &\in Z \oplus Y^+
\end{align*}

since the other directions are covered by $\beta^\pm$ (and $\alpha$, but we set that equal to 0).\\

Thus the problem we need to solve to construct the periodic double pulse is

\begin{align*}
(U_i^\pm)' - F(U_i^\pm) &= 0 \\
U_i^+(X_i) - U_{i-1}^-(-X_i) &= 0 \\
U_i^+(0) - U_i^-(0) &= 0;
\end{align*}

for $i = 1, 2$. Let $\Phi_\pm(x, y; \beta^\pm)$ be the family of evolution operators for the variational equations

\begin{equation}
(V^\pm)' = D_U F(Q^\pm(0, \beta^\pm)(x)) V^\pm
\end{equation}

where these are defined on the appropriate interval. We can decompose these evolution operators via an exponential dichotomy.

\begin{lemma}\label{dichotomy1}
There exist projections

\begin{align*}
&P_+^s(y; \beta^+) && y \geq 0 \\
&P_+^u(y; \beta^+) = I - P_+^s(y; \beta^+) && y \geq 0 \\
&P_-^u(y; \beta^-) && y \leq 0 \\
&P_-^s(y; \beta^-) = I - P_-^u(y; \beta^-) && y \leq 0 \\
\end{align*}

such that the evolution operators $\Phi_\pm(x, y; \beta^\pm)$ can be decomposed as

\begin{align*}
\Phi^s_\pm(x, y; \beta^\pm) &= \Phi_\pm(x, y; \beta^\pm) P^s_\pm(y; \beta^\pm) \\
\Phi^u_\pm(x, y; \beta^\pm) &= \Phi_\pm(x, y; \beta^\pm) P^u_\pm(y; \beta^\pm) 
\end{align*}

where we have the estimates

\begin{align*}
|\Phi^s_+(x, y, \beta^+)| &\leq C e^{-\alpha(x - y)} && 0 \leq y \leq x \\
|\Phi^u_+(x, y, \beta^+)| &\leq C e^{-\alpha(y - x)} && 0 \leq x \leq y \\
|\Phi^u_-(x, y, \beta^+)| &\leq C e^{-\alpha(y - x)} && 0 \geq y \geq x \\
|\Phi^s_-(x, y, \beta^+)| &\leq C e^{-\alpha(x - y)} && 0 \geq x \geq y \\
\end{align*}

which also hold for derivatives with respect to the ICs $\beta^\pm$. In addition, the projections satisfy 

\begin{align*}
\Phi_\pm(x, y; \beta^\pm) P^{s/u}_\pm(y; \beta^\pm) 
= P^{s/u}_\pm(x; \beta^\pm) \Phi_\pm(x, y; \beta^\pm)
\end{align*}

i.e. it does not matter if we project or evolve first. Finally, the projections can be chosen such that at $y = 0$ we have, independent of $\beta^+$ and $\beta^-$

\begin{align*}
\ker P^s_+(0; \beta^+) &= Z \oplus Y^- \\
\ker P^u_-(0; \beta^-) &= Z \oplus Y^+ \\
\textrm{ran } P^u_+(0; \beta^+) &= Z \oplus Y^- \\
\textrm{ran } P^s_-(0; \beta^-) &= Z \oplus Y^+
\end{align*}

\begin{proof}
Since $D_U F(0)$ is hyperbolic by assumption, this follows from Lemma 5.1 in San97, which follows from Lemma 1.1 in San93.
\end{proof}

\end{lemma}

Next, we rewrite the problem $U'(x) = F(U)$ in piecewise fashion in such a way as to exploit what we did above. We want to solve piecewise

\begin{align*}
(U_i^\pm(x))' &= (Q^\pm(0, \beta_i^\pm)(x))' + (V_i^\pm(x))' = F\left(Q^\pm(0, \beta_i^\pm)(x) + V_i^\pm(x) \right) \\
\end{align*}

Using the fact that (by construction) $Q^\pm(0, \beta_i^\pm)(x)$ solves $U'(x) = F(U)$ on the appropriate interval, this becomes

\begin{align*}
F(Q^\pm(0, \beta_i^\pm)(x)) + (V_i^\pm(x))' &= F(Q^\pm(0, \beta_i^\pm)(x) + V_i^\pm(x) )
\end{align*}

Solving for $(V_i^\pm(x))'$, we have

\begin{align*}
(V_i^\pm(x))' &= F(Q^\pm(0, \beta_i^\pm)(x) + V_i^\pm(x) ) - F(Q^\pm(0, \beta_i^\pm)(x) )
\end{align*}

Now we Taylor about $F(Q^\pm(0, \beta_i^\pm)(x)$ to get the equivalent formulation

\begin{equation}\label{Vpiecewise}
(V_i^\pm(x))' = D_U F(Q^\pm(0, \beta_i^\pm)(x)) V_i^\pm(x)  + G_i^\pm(\beta_i^\pm, V_i^\pm)(x)
\end{equation}

where $G_i^\pm(\beta_i^\pm, V_i^\pm)(x) = \mathcal{O}(|V_i^\pm|^2)$, independent of the parameters $\beta_i^\pm$. The fact that this is bound is independent of the $\beta_i^\pm$ follows from the fact that we have bounds on $Q^\pm(0, \beta_i^\pm)$ which are independent of $\beta_i^\pm$. Derivatives of $G_i^\pm$ with respect to the parameters $\beta_i^\pm$ are the same order in $V_i^\pm$.\\

At this point, we rewrite the (piecewise) differential equations \eqref{Vpiecewise} in integrated form as the fixed point equations

\begin{align*}
V_i^+(x) &= \Phi^u_+(x, X_i; \beta_i^+) a_i^+  \\
&+ \int_{X_i}^x \Phi_+^u(x, y; \beta_i^+) G_i^+(y, V_i^+(y),\beta_i^+)dy \\
&+ \int_0^x \Phi_+^s(x, y, \beta_i^+) G_i^+(y, V_i^+(y),\beta_i^+)dy \\ 
V_i^-(x) &= \Phi^s_-(x, -X_{i-1}, \beta_i^-) a_{i-1}^-  \\
&+ \int_{-X_{i-1}}^x \Phi_-^s(x, y, \beta_i^-) G_i^-(y, V_i^-(y),\beta_i^-)dy \\
&+ \int_0^x \Phi_-^u(x, y, \beta_i^-) G_i^-(y, V_i^-(y),\beta_i^-)dy \\
\end{align*}

where for the initial conditions $a_i^\pm$ we have

\begin{align*}
a_i^+ &\in E_0^u \\
a_i^- &\in E_0^s \\
\end{align*}

Note that we do not have initial conditions $b_i^\pm$ at $x = 0$ for the other side of the dichotomy. Since these ICs would live in $Y^\pm$, we have incorporated them into the $\beta_i^\pm$.\\

Next, we define the exponentially weighted norms

\begin{align*}
||V||_{X, +} = sup_{x \in [0, X]} e^{\alpha(X - x)}|V(x)| \\
||V||_{X, -} = sup_{x \in [-X, 0]} e^{\alpha(X + x)}|V(x)| \\
\end{align*}

Let $K_{X, +}$ be the spaces on continuous functions on the appropriate intervals equipped with these norms. These are known to be Banach spaces. Let $B_{X, \pm}(\rho)$ be the ball of radius $\rho$ in these spaces.\\

At this point, we will solve for the $V_i^\pm$ and the $\beta_i^\pm$ in a series of lemmas.\\

\begin{lemma}
There exist $\delta, \rho > 0$ such that for $|X_i| > 1/\delta$ and $|\beta_i^\pm| < \delta$, there exist unique solutions

\begin{align*}
V_i^- &\in B_{X_{i-1}, -}(\rho) \\
V_i^+ &\in B_{X_i, +}(\rho) \\
\end{align*}

These depend smoothly on the $\beta_i^\pm$, and we have the estimates

\begin{align*}
||V_i^-||_{X_{i-1}, -} &\leq C |a_{i-1}^-| \\
||V_i^+||_{X_i, +} &\leq C |a_i^+|
\end{align*}

The estimates hold as well for derivatives of $V_i^\pm$ with respect to $\beta_i^\pm$.

\begin{proof}
This should just be Lemma 5.2 in San97. The following is a mostly filled-in version of that proof.\\

We can deal with the four pieces separately, so let's do the ``positive'' pieces. The RHS of the fixed point equations defines a smooth mapping on $K_{X_i, +}$ (basically obvious since everything involved is smooth, although we could check this). We need to verify that the RHS maps to $K_{X_i, +}$, so we check the three terms individually. Using our estimates on $\Phi^{s/u}_\pm$ and $G$ and the fact that $V_i^+ \in K_{X_i, +}$, we have for the first term

\begin{align*}
e^{\alpha(X_i - x)} | \Phi^u_+(x, X_i; \beta_i^+) a_i^+ | 
&\leq C e^{\alpha(X_i - x} e^{-\alpha(X_i - x)} |a_i^+| \\
&\leq C |a_i^+|
\end{align*}

For the second term,

\begin{align*}
e^{\alpha(X_i - x)} &\left| \int_{X_i}^x \Phi_+^u(x, y; \beta_i^+) G_i^+(y, V_i^+(y),\beta_i^+)dy  \right| \\
&\leq C e^{\alpha(X_i - x)} \int_x^{X_i} e^{-\alpha(y - x)}|V_i^+(y)|^2 dy \\
&\leq C e^{\alpha(X_i - x)} \int_x^{X_i} 
e^{-\alpha(y - x)}(e^{-\alpha(X_i - y)})^2|e^{\alpha(X_i - y)} V_i^+(y)|^2 dy \\
&\leq C e^{\alpha(X_i - x)} \int_x^{X_i} 
e^{-\alpha(y - x)}(e^{-\alpha(X_i - y)})^2 dy \\
&\leq C \int_0^{X_i} e^{-\alpha (X_i - y)} dy \\
&\leq C
\end{align*}

The third term is similar. Thus the RHS of the fixed point equations does indeed map $K_{X_i, +}$ to itself.\\

Now that we know what spaces things live in, define 
$H: K_{X_i, +} \times E_0^s \rightarrow K_{X_i, +}$ by

\begin{align*}
H(V_i^+(x), a_i^+, \beta_i^+) &= V_i^+(x) - \Phi^u_+(x, X_i; \beta_i^+) a_i^+  \\
&- \int_{X_i}^x \Phi_+^u(x, y; \beta_i^+) G_i^+(y, V_i^+(y),\beta_i^+)dy \\
&- \int_0^x \Phi_+^s(x, y; \beta_i^+) G_i^+(y, V_i^+(y),\beta_i^+)dy 
\end{align*}

Since $Q(x)$ satisfies the original ODE, we know that $H(0, 0, 0) = 0$. We can check that the Frechet derivative of $H$ with respect to $V_i^+$ at $(V_i^+(x), a_i^+, \beta_i^+) = (0, 0, 0)$ is a Banach space isomorphism on $K_{X_i, +}$. Then using the implicit function theorem, we can solve for $V_i^+(x)$ in terms of $(a_i^+, \beta_i^+)$. This dependence is smooth, since the map $H$ is smooth. \\

The estimate on $V_i^\pm$ comes from the estimate of the first term on the RHS of the fixed point equations, since the other terms (those involving the integrals) are quadratic in $V_i^\pm$. 

\end{proof}
\end{lemma}

Next, we look at the conditions $U_i^+(X_i) - U_{i-1}^-(-X_i) = 0$.

% Lemma : solve for a_i^\pm

\begin{lemma}\label{solvefora}
For any $X_i$ and $\beta_i^\pm$ chosen as in the previous lemma, there is a unique pair of initial conditions $(a_i^+, a_i^-) \in E_0^s \times E_0^u$ such that $U_i^+(X_i) - U_{i-1}^-(-X_i) = 0$. $(a_i^+, a_i^-)$ depends smoothly on $(\beta_i^+, \beta_i^-)$, and we have the following estimate which is independent of the $\beta_i^\pm$

\begin{equation}
|a_i^\pm| \leq C e^{-\alpha X_i}
\end{equation}

which holds for $a_i^\pm$ as well as derivatives with respect to $\beta_i^\pm$.

We also have the following expressions for the $a_i^\pm$

\begin{align*}
a_i^+ &= P^s_0 \left( Q^+(0, \beta_i^+)(X_i) - Q^-(0, \beta_i^-)(-X_i) \right) 
+ \mathcal{O}( e^{-2 \alpha X_i} )\\
a_i^- &= P^u_0 \left( Q^+(0, \beta_i^+)(X_i) - Q^-(0, \beta_i^-)(-X_i) \right) 
+ \mathcal{O}( e^{-2 \alpha X_i} )
\end{align*}

\begin{proof}

First, plug in the fixed point equations and evaluate them at $\pm X_i$ to get
\begin{align*}
U_i^+(X_i) &= Q^+(0, \beta_i^+)(X_i) + P^u_+(X_i; \beta_i^+) a_i^+ + \int_0^{X_i} \Phi_+^s(X_i, y; \beta_i^+) G_i^+(y, V_i^+(y),\beta_i^+)dy \\ 
U_{i-1}^-(-X_i) &= Q^-(0, \beta_i^-)(-X_i) + P^s_-(-X_i; \beta_i^-) a_i^- + \int_0^{-X_i} \Phi_-^u(-X_i, y; \beta_i^-) G_i^-(y, V_i^-(y),\beta_i^-)dy \\
\end{align*}

Adding and subtracting $P_0^{s/u}$ and recalling where the $a_i^\pm$ live, this becomes

\begin{align*}
U_i^+(X_i) &= Q^+(0, \beta_i^+)(X_i) + a_i^+ + (P^u_+(X_i; \beta_i^+) -  P^u_0)a_i^+ + \int_0^{X_i} \Phi_+^s(X_i, y; \beta_i^+) G_i^+(y, V_i^+(y),\beta_i^+)dy \\ 
U_{i-1}^-(-X_i) &= Q^-(0, \beta_i^-)(-X_i) + a_i^- + (P^s_-(-X_i; \beta_i^-) - P^s_0) a_i^- + \int_0^{-X_i} \Phi_-^u(-X_i, y; \beta_i^-) G_i^-(y, V_i^-(y),\beta_i^-)dy \\
\end{align*}

Subtracting these, we obtain the equation we need to solve

\begin{align*}
0 &= H(a_i^+, a_i^-, \beta_i^+, \beta_i^-) \\
&= a_i^+ - a_i^- + (P^u_+(X_i; \beta_i^+) -  P^u_0)a_i^+ - (P^s_-(-X_i; \beta_i^-) - P^s_0) a_i^-  \\
&+ Q^+(0, \beta_i^+)(X_i) - Q^-(0, \beta_i^-)(-X_i)\\
&+ \int_0^{X_i} \Phi_+^s(X_i, y; \beta_i^+) G_i^+(y, V_i^+(y),\beta_i^+)dy
- \int_0^{-X_i} \Phi_-^u(-X_i, y; \beta_i^-) G_i^-(y, V_i^-(y),\beta_i^-)dy 
\end{align*}

where $H: E_0^s \times E_0^u \times \R \times \R \rightarrow \R^4$. In the previous lemma, we solved for $V_i^\pm$ in terms of the $a_i^\pm$ and $\beta_i^\pm$, so we rewrite the above to indicate that dependence so that $H$ is only a function of its four arguments.

\begin{align*}
H(a_i^+, &a_i^-, \beta_i^+, \beta_i^-) \\
&= a_i^+ - a_i^- + (P^u_+(X_i; \beta_i^+) -  P^u_0)a_i^+ - (P^s_-(-X_i; \beta_i^-) - P^s_0) a_i^-  \\
&+ Q^+(0, \beta_i^+)(X_i) - Q^-(0, \beta_i^-)(-X_i)\\
&+ \int_0^{X_i} \Phi_+^s(X_i, y; \beta_i^+) G_i^+(y, V_i^+(a_i^+, \beta_i^+)(y),\beta_i^+)dy \\
&- \int_0^{-X_i} \Phi_-^u(-X_i, y; \beta_i^-) G_i^-(y, V_i^-(a_i^-, \beta_i^-)(y),\beta_i^-)dy 
\end{align*}

Again, note that $H(0, 0, 0, 0) = 0$. We want to solve for $a_i^\pm$ using the implicit function theorem, so we need to look at $D_{a_i^\pm} H(0, 0, 0, 0)$. First, define

\begin{equation}
p_1(X; \beta^+, \beta^-) = \sup_{x \geq X} (|P^u_+(x; \beta^+) - P_0^u| + |P^s_-(-x; \beta^-) - P_0^s|)
\end{equation}

Based on Lemma \ref{dichotomy1}, we hypothesize the following bound for $p_1$.

\begin{hypothesis}\label{p1existence}
\begin{equation}
p_1(X; \beta^+, \beta^-) = \mathcal{O}(e^{-\alpha X})
\end{equation} 
\end{hypothesis}

When we differentiate with respect to $a_i^\pm$ at $a_i^\pm = 0$, we note that the the derivatives of the integral terms in $H$ will be 0 since $G_i^\pm$ is quadratic order in $V_i^\pm$, thus quadratic order in $a_i^\pm$ by the previous lemma. The $Q^\pm$ terms do not involve $a_i^\pm$.\\

Thus, taking the partial derivative with respect to $a_i^\pm = 0$, we get

\[
\frac{\partial}{\partial a_i^\pm} H(0, 0, 0, 0) = \pm 1 + \mathcal{O} (e^{-\alpha X_i})
\]

Thus, for sufficiently large $X_i$, $D_{a_i^\pm} H(0, 0, 0, 0)$ is invertible in a neighborhood of $(0, 0, 0, 0)$, so we can use the IFT to solve for $a_i\pm$ in terms of $\beta_i^\pm$. \\

Now that we have found $a_i^\pm$ which satisfies the $H(a_i^+, a_i^-, \beta_i^+, \beta_i^-) = 0$, we can get an expression and estimates for the $a_i^\pm$. Recalling the spaces $a_i^\pm$ live in, we apply the projection $P^s_0$ to get

\begin{align*}
0 &= a_i^+ + P^s_0(P^u_+(X_i; \beta_i^+) -  P^u_0)a_i^+ 
+ P^s_0 \left( Q^+(0, \beta_i^+)(X_i) - Q^-(0, \beta_i^-)(-X_i) \right)\\
&+ P^s_0 \left( \int_0^{X_i} \Phi_+^s(X_i, y; \beta_i^+) G_i^+(y, V_i^+(a_i^+, \beta_i^+)(y),\beta_i^+)dy \right) \\
&- P^s_0 \left( \int_0^{-X_i} \Phi_-^u(-X_i, y; \beta_i^-) G_i^-(y, V_i^-(a_i^-, \beta_i^-)(y),\beta_i^-) dy \right)
\end{align*}

We have estimates for all of these terms. For the first two terms, we have

\begin{align*}
|P^s_0(P^u_+(X_i; \beta_i^+) -  P^u_0)a_i^+ | &\leq C e^{-\alpha X_i} |a_i^+| \\
|P^s_0 \left( Q^+(0, \beta_i^+)(X_i) - Q^-(0, \beta_i^-)(-X_i) \right)| &\leq C e^{-\alpha X_i}.
\end{align*}

using our hypothesis above. For the integral terms, we have

\begin{align*}
\left| \int_0^{X_i} \Phi_+^s(X_i, y; \beta_i^+) G_i^+(y, V_i^+(a_i^+, \beta_i^+)(y),\beta_i^+)dy \right| &\leq C \int_0^{X_i}e^{-\alpha(X_i - y)} |V_i^+(y)|^2 dy \\
&\leq C \int_0^{X_i}e^{-3 \alpha (X_i - y)} |e^{\alpha(X_i - y)} V_i^+(y)|^2 dy \\
&\leq C \int_0^{X_i}e^{-3 \alpha (X_i - y)} (||V_i^+||_{X_i, +})^2 dy \\
&\leq C |a_i^+|^2
\end{align*}

The other integral term is similar. In a hand-wavey fashion, we have something which looks like

\[
|a_i^+|(1 + \mathcal{O}(e^{-\alpha X_i}) + \mathcal{O}(|a_i^+|)) = 
\mathcal{O}( e^{-\alpha X_i} )
\]

which should give us the estimate we want

\begin{align*}
|a_i^\pm| \leq C e^{-\alpha X_i}
\end{align*}

To get an actual expression for $a_i^\pm$, we plug in all our estimates to get

\begin{align*}
a_i^+ = P^s_0 \left( Q^+(0, \beta_i^+)(X_i) - Q^-(0, \beta_i^-)(-X_i) \right) 
+ \mathcal{O}( e^{-2 \alpha X_i} )
\end{align*}

Similarly, we have

\begin{align*}
a_i^- = P^u_0 \left( Q^+(0, \beta_i^+)(X_i) - Q^-(0, \beta_i^-)(-X_i) \right) 
+ \mathcal{O}( e^{-2 \alpha X_i} )
\end{align*}

\end{proof}
\end{lemma}

Finally, we need the pieces to match at $x = 0$. In other words, we need to find $\beta_i^\pm$ such that $U_i^+(0) = U_i^-(0)$, i.e. 

\begin{align*}
Q^+(0, \beta^+) + V_i^+(0) = Q^-(0, \beta^-) + V_i^-(0)
\end{align*}

Before we do that, we will need to make a smooth change of coordinates near $Q(0)$. We will do that using a variation of the ``flow-box'' method. What we want to do is straighten out the stable and unstable manifolds near $Q(0)$ so that their non-intersecting directions lie along $Y^+$ and $Y^-$.

% lemma : straighten out manifolds

\begin{lemma}\label{straightenW}

There exists a differentiable map 

\[
H: \R \times Y^- \times Y^+ \times Z \rightarrow \R^4
\]

such that $H(0, 0, 0, 0) = Q(0)$, $H$ is invertible in a neighborhood of $Q(0)$, and

\begin{align*}
H^{-1}(Q^-(0, \beta^-) &= \beta^- \in Y^- \\
H^{-1}(Q^+(0, \beta^+) &= \beta^+ \in Y^+ \\
\end{align*}

for sufficiently small $\beta^\pm$.

\begin{proof}
We know that $Q'(0) = F(Q(0)) \neq 0$, i.e. the center of the pulse is not an equilibrium of the vector field $F$. Recall that $\R^4 = Z \oplus \R Q'(0) \oplus Y^+ \oplus Y^-$.\\

Let $S_y(P)$ be the solution operator for the ODE $U' = F(U)$, i.e. the operator which takes an initial condition $P$ and maps it to the point $H(y)$, where $H$ is the unique solution to $U' = F(U)$ such that $H(0) = P$.\\

Define the map $H: \R \times Y^- \times Y^+ \times Z \rightarrow \R^4$ by 

\begin{equation}
H(y; \beta^-, \beta^+, \gamma) = S_y(Q(0) + Q^-(0, \beta^-) + Q^-(0, \beta^+) + \gamma \Psi(0))
\end{equation}

In other words, $H(y; \beta^-, \beta^+, \gamma)$ is a trajectory of $U' = F(U)$ with initial condition $Q(0) + Q^-(\beta^-,0) + Q^-(\beta^+,0) + \gamma \Psi(0)$.\\ 

Near $Q(0)$, i.e. for small $x, \beta^\pm$ the stable and unstable manifolds are given by the curves

\begin{align*}
W^u &= H(x; \beta^-, 0, 0) \\
W^s &= H(x; 0, \beta^+, 0) 
\end{align*}

Their one-dimensional intersection is the homoclinic orbit $Q(x) = H(x; 0, 0, 0)$, whose tangent space is $Q'(x)$.\\

To complete the proof, we need to show that $H$ is invertible in a neighborhood of the origin. To compute the Jacobian of $H$ at the origin, we note the following.

\begin{align*}
H_y(0, 0, 0, 0) &= F(Q(0)) = Q'(0) \\
H_{\beta^-}(0, 0, 0, 0) &= (Q^-)_{\beta^-}(0,0) = Y^- \\
H_{\beta^+}(0, 0, 0, 0) &= (Q^+)_{\beta^+}(0,0) = Y^+ \\
H_{\gamma}(0, 0, 0, 0) &= Z
\end{align*}

Since $\R^4 = Z \oplus \R Q'(0) \oplus Y^+ \oplus Y^-$, the four partial derivatives of $H$ at the origin are linearly independent, thus the Jacobian of $H$ at the origin is invertible. Note that $H_y(0, 0, 0, 0) = Q(0)$. By the inverse function theorem, there exists open neighborhoods $N_1$ of 0 and $N_2$ of $Q(0)$ such that $H(N_1) = N_2$ and $H$ is invertible on $N_2$. \\

In particular, we have for sufficiently small $\beta^\pm$

\begin{align*}
H^{-1}(Q^-(0, \beta^-) &= \beta^- \in Y^- \\
H^{-1}(Q^+(0, \beta^+) &= \beta^+ \in Y^+ \\
\end{align*}

\end{proof}
\end{lemma}

Now we are ready to perform the match at $x = 0$ to solve for $\beta_i^\pm$. Before we do that, we change coordinates according to the prior lemma. In the new coordinates, near $Q(0)$ our ODE becomes

\begin{equation}
\tilde{U}' = DH(H^{-1}(\tilde{U})) F( H^{-1}(\tilde{U}) )
\end{equation}

where $H$ is defined in the previous lemma. If we match the $\tilde{U}_i^\pm$ at $x = 0$, then since $H^{-1}$ is a homeomorphism near $Q(0)$, we have also matched the $U_i^\pm$ at $x = 0$. For convenience, we will continue to write the ODE as $U' = F(U)$ after the coordinate change.\\

Note that because of our coordinate change, we have

\begin{align*}
P_Z(Q^\pm(0, \beta^\pm) &= 0 \\
P_{\R Q'(0)}(Q^\pm(0, \beta^\pm) &= 0 \\
P_{Y^\pm}(Q^\pm(\beta^\pm, 0)) &= \beta^\pm
\end{align*}

We wish to solve $U_i^+(0) - U_i^-(0)) = 0$. Since $\R^4 = Z \oplus \R Q'(0) \oplus Y^+ \oplus Y^-$, it suffices to project this onto these four subspaces and solve separately on each subspace. Since $V_i^\pm(0) \in Z \oplus Y^\pm$, the projection on $\R Q'(0)$ is automatically 0. Thus we only need to solve the equations

\begin{align*}
P_{Y^\pm}(U_i^+(0) - U_i^-(0)) &= 0 \\
P_Z(U_i^+(0) - U_i^-(0)) &= 0 \\
\end{align*}

We will do each of these in turn.

\begin{lemma}\label{solveforbeta}
There exist $\beta_i^\pm$ such that $P_{Y^\pm}(U_i^+(0) - U_i^-(0)) = 0$. We have the estimate

\begin{equation}
| (\beta_i^+, \beta_i^-) | \leq C (e^{-2 \alpha X_i} + e^{-2 \alpha X_{i-1}})
\end{equation}

\begin{proof}
Recall that $U_i^\pm = Q^\pm(0, \beta_i^\pm) + V_i^\pm$. Then since we have applied the change of coordinates above, we have

\begin{align*}
P_{Y^\pm}(Q^+(0, \beta_i^+) - Q^-(0, \beta_i^-)) &= \beta_i^+ - \beta_i^-
\end{align*}

Then the equation we want to solve becomes

\begin{equation}
H(\beta_i^+, \beta_i^-) = \beta_i^+ - \beta_i^- 
+ P_{Y^\pm}(V_i^+(0) - V_i^-(0)) = 0
\end{equation}

Substituting the fixed point equations evaluated at $x = 0$, we get

\begin{align*}
H(\beta_i^+, \beta_i^-) &= \beta_i^+ - \beta_i^- \\
&+ P_{Y^\pm} \Big( \Phi^u_+(0, X_i; \beta_i^+) a_i^+ 
+ \int_{X_i}^0 \Phi_+^u(0, y; \beta_i^+) G_i^+(y, V_i^+(y),\beta_i^+)dy \\
&- \Phi^s_-(0, -X_{i-1}, \beta_i^-) a_{i-1}^- 
- \int_{-X_{i-1}}^0 \Phi_-^s(0, y, \beta_i^-) G_i^-(y, V_i^-(y),\beta_i^-) dy \Big) 
\end{align*}

Next, since $\text{Ran }P_+^u(0; \beta^+) = Z \oplus Y^-$, the terms involving $\Phi^u_+$ can have no component in $Y^+$. Similarly, since $\text{Ran }P_-^s(0; \beta^-) = Z \oplus Y^+$, the terms involving $\Phi^s_-$ can have no component in $Y^-$. Thus we can separate our equation into the individual projections on $Y^-$ and $Y^+$. Redefining $H$ this way gives us

\begin{equation}
H(\beta^+, \beta^-) = 
\begin{pmatrix}
\beta_i^+ - P_{Y^+}\left(\Phi^s_-(0, -X_{i-1}, \beta_i^-) a_{i-1}^- 
- \int_{-X_{i-1}}^0 \Phi_-^s(0, y, \beta_i^-) G_i^-(y, V_i^-(y),\beta_i^-) dy\right) \\
\beta_i^- - P_{Y^-}\left( \Phi^u_+(0, X_i; \beta_i^+) a_i^+ 
+ \int_{X_i}^0 \Phi_+^u(0, y; \beta_i^+) G_i^+(y, V_i^+(y),\beta_i^+)dy \right)
\end{pmatrix}
\end{equation}

Note the the $\beta_i^\pm$ ``mix''. What we would like to do now is invert this to solve for the $\beta_i^\pm$. This requires estimating the partial derivatives with respect to $\beta_i^\pm$ of the two terms. Since the estimates for $G$ and $\Phi$ also apply to the derivatives with respect to $\beta_i^\pm$, this is not hard to do.\\

For the $a_i^\pm$ terms we have

\begin{align*}
\left| \frac{\partial}{\partial \beta_i^+} \Phi^u_+(\beta_i^+, 0, X_i) a_i^+ \right| \leq C e^{-\alpha X_i}|a_i^+|
& \leq C e^{-2 \alpha X_i}
\end{align*}

where we used our estimate for $a_i^\pm$ from above. The $\beta_i^-$ term is similar, except it involves $X_{i-1}$. For the integral terms, we have

\begin{align*}
\left| \frac{\partial}{\partial \beta_i^-} \int_{-X_{i-1}}^0 \Phi_-^s(\beta_i^-, 0, y) G^-(y, V^-(y),\beta^-)dy \right| 
&\leq C \int_{-X_{i-1}}^0 e^{\alpha y} | V_i^-(y) |^2 dy \\
&\leq C \int_{-X_{i-1}}^0 e^{\alpha y} | |a_i^-|^2 dy \\
&\leq C e^{-2 \alpha X_{i-1}}
\end{align*}

where we used our estimates for $V_i^\pm$ and $a_i^\pm$ from above. The ``positive'' term is similar. Putting all this together, the Jacobian of $H$ is given by

\begin{equation}
D H(\beta_i^+, \beta_i^-) = 
\begin{pmatrix}
1 & \mathcal{O}(e^{-2 \alpha X_i} + e^{-2 \alpha X_{i-1}} ) \\
\mathcal{O}(e^{-2 \alpha X_i} + e^{-2 \alpha X_{i-1}} ) &  1 
\end{pmatrix}
\end{equation}

which has determinant $1 + \mathcal{O}(e^{-4 \alpha X_i} + e^{-4 \alpha X_{i-1}})$, thus is invertible for sufficiently large $X_i$. Using the inverse function theorem, we can invert $H$ in a neighborhood of $Q(0)$, which corresponds to $(\beta_i^+, \beta_i^-) = (0, 0)$. Thus we have 

\[
(\beta^+, \beta^-) = H^{-1}(0, 0)
\]

We would like to get an estimate on this. By the inverse function theorem, $D H^{-1}$ is bounded in a neighborhood of $(0, 0)$, thus we have 

\begin{align*}
| (\beta_i^+, \beta_i^-) | &= | H^{-1}(0, 0) | \\
&= | H^{-1}(0, 0) - (0, 0) | \\
&= | H^{-1}(0, 0) - H^{-1}(H(0, 0)) | \\
& \leq C | (0, 0) - H(0, 0) | \\
& \leq C |H(0, 0)|
\end{align*}

Since $H(0, 0) \leq C (e^{-2 \alpha X_i} + e^{-2 \alpha X_{i-1}})$, this becomes

\[
| (\beta_i^+, \beta_i^-) | \leq C (e^{-2 \alpha X_i} + e^{-2 \alpha X_{i-1}})
\]

\end{proof}
\end{lemma}

Finally, we need to look at the projection on the adjoint solution $\Psi(0)$. To do this, we will show that this condition is satisfied for certain values of the lengths $X_i$. This makes sense based on what we know about the nonperiodic case.\\

Recall that from our change of coordinates we have
\begin{align*}
P_{Z}(Q^+(0, \beta_i^+) - Q^-(0, \beta_i^-)) &= 0
\end{align*}

Thus the equation we want to solve is

\begin{equation}
P_{Z^\pm}(V_i^+(0) - V_i^-(0)) = 0
\end{equation}

which we can write as

\begin{equation}
\langle \Psi(0), V_i^+(0) - V_i^-(0) \rangle = 0
\end{equation}

First, we come up with a nice expression for $\langle \Psi(0), V_i^+(0) - V_i^-(0) \rangle$. This agrees with (3.9) on p.2093 of SanStrut, which is good.

% first lemma for jump

\begin{lemma}\label{jumplemma1}

\begin{align*}
\langle \Psi(0), V_i^+(0) - V_i^-(0) \rangle = 
\langle \Psi(X_i), Q(-X_i) \rangle - \langle \Psi(-X_{i-1}), Q(X_{i-1}) \rangle + R_i
\end{align*}

where 

\begin{align*}
|R_i| \leq C ( e^{-3 \alpha X_1} +  e^{-3 \alpha X_2} )
\end{align*}
\begin{proof}

Substituting the fixed point equations evaluated at $x = 0$, we have

\begin{align*}
\langle \Psi(0), V_i^+(0) - V_i^-(0) \rangle &= \langle \Psi(0), \Phi^u_+(0, X_i; \beta_i^+) a_i^+ \rangle
- \langle \Psi(0), \Phi^s_-(0, -X_{i-1}, \beta_i^-) a_{i-1}^- \rangle \\
&+ \int_{X_i}^0 \langle \Psi(0), \Phi_+^u(0, y; \beta_i^+) G_i^+(y, V_i^+(y),\beta_i^+) \rangle dy \\
&- \int_{-X_{i-1}}^0 \langle \Psi(0), \Phi_-^s(0, y, \beta_i^-) G_i^-(y, V_i^-(y),\beta_i^-) \rangle dy
\end{align*}

Before we continue, we make the following hypothesis (which we should be able to show). This combines the exponential dichotomy estimates with those for the parameters $\beta_i^\pm$.

\begin{hypothesis}\label{betaevolution}
\begin{align*}
|(\Phi_+^u(0, X; \beta^+) - \Phi_+^u(0, X; 0)| &\leq C |\beta^+| e^{-\alpha X} \\
|(\Phi_-^s(0, -X; \beta^+) - \Phi_-^s(0, -X; 0)| &\leq C |\beta^-| e^{-\alpha X} \\
\end{align*}
\end{hypothesis}

Using this together with the estimate on $\beta_i^\pm$ from Lemma \ref{solveforbeta}, this gives us

\begin{align*}
|(\Phi_+^u(0, X; \beta^+) - \Phi_+^u(0, X; 0)| &\leq C e^{-\alpha X} (e^{-\alpha X_1} + e^{-\alpha X_2})\\
|(\Phi_-^s(0, -X; \beta^+) - \Phi_-^s(0, -X; 0)| &\leq C e^{-\alpha X} (e^{-\alpha X_1} + e^{-\alpha X_2}) \\
\end{align*}

This implies

\begin{align*}
|Q^+(0, \beta_i^+)(X_i) - Q(X_i)| \leq C e^{-\alpha X_i} (e^{-\alpha X_1} + e^{-\alpha X_2}) \\
|Q^-(0, \beta_i^-)(-X_i) - Q(-X_i)| \leq C e^{-\alpha X_i} (e^{-\alpha X_1} + e^{-\alpha X_2}) \\
\end{align*}

We also have the following estimate from Hypothesis \ref{p1existence}

\begin{align*}
|P^u_+(X; \beta^+) - P_0^u| &\leq C e^{-\alpha X} \\
|P^s_-(-X; \beta^-) - P_0^s| &\leq C e^{-\alpha X}
\end{align*}

For the first term on the RHS, we have upon substituting for $a_i^+$ from Lemma \ref{solvefora}

\begin{align*}
\langle \Psi(0), &\Phi^u_+(0, X_i; \beta_i^+) a_i^+ \rangle \\
&= \langle \Psi(0), \Phi_+(0, X_i; \beta_i^+) P^u_+(X_i; \beta_i^+) P^s_0 \left( Q^+(0, \beta_i^+)(X_i) - Q^-(0, \beta_i^-)(-X_i) \right) 
+ \mathcal{O}( e^{-2 \alpha X_i} ) \rangle \\
&= \langle \Psi(0), \Phi_+(0, X_i; \beta_i^+) P^u_+(X_i; \beta_i^+) P^s_0 \left( Q^+(0, \beta_i^+)(X_i) - Q^-(0, \beta_i^-)(-X_i) \right) \rangle  
+ \mathcal{O}( e^{-3 \alpha X_i} ) \\
&= \langle \Psi(0), \Phi_+(0, X_i; \beta_i^+) P^u_+(X_i; \beta_i^+) P^s_0 Q^+(0, \beta_i^+)(X_i) \rangle \\
&- \langle \Psi(0), \Phi_+(0, X_i; \beta_i^+) P^u_+(X_i; \beta_i^+) P^s_0 Q^-(0, \beta_i^-)(-X_i) \rangle
+ \mathcal{O}( e^{-3 \alpha X_i} ) \\
\end{align*}

For the first term on the RHS, we have

\begin{align*}
\langle \Psi(0), &\Phi_+(0, X_i; \beta_i^+) P^u_+(X_i; \beta_i^+) P^s_0 Q^+(0, \beta_i^+)(X_i) \rangle \\
&= \langle \Psi(0), \Phi_+(0, X_i; \beta_i^+) P^u_0 P^s_0 Q^+(0, \beta_i^+)(X_i) \rangle + \mathcal{O}(e^{-3 \alpha X_i}) \\
&= \langle \Psi(0), \Phi_+(0, X_i; \beta_i^+) P^s_0 P^u_0 Q^+(0, \beta_i^+)(X_i) \rangle + \mathcal{O}(e^{-3 \alpha X_i}) \\
&= \langle \Psi(0), \Phi_+(0, X_i; \beta_i^+) P^u_0 P^u_+(X_i; \beta_i^+) Q^+(0, \beta_i^+)(X_i) \rangle + \mathcal{O}(e^{-3 \alpha X_i}) \\
&= \langle \Psi(0), \Phi_+(0, X_i; \beta_i^+) P^u_0 P^u_+(X_i; \beta_i^+) \Phi_+(X_i, 0) Q^+(0, \beta_i^+)\rangle + \mathcal{O}(e^{-3 \alpha X_i}) \\
&= \langle \Psi(0), \Phi_+(0, X_i; \beta_i^+) P^u_0 \Phi_+(X_i, 0) P^u_+(0; \beta_i^+) Q^+(0, \beta_i^+)\rangle + \mathcal{O}(e^{-3 \alpha X_i}) \\
&= \mathcal{O}(e^{-3 \alpha X_i})
\end{align*}

From our change of coordinates, $Q^+(0, \beta_i^+) \in Y^+$, and this is wiped out by $P^u_+(0; \beta_i^+)$, which projects onto the unstable part of the exponential dichotomy on $\R^+$ at $x = 0$. We also used the fact that the spectral projections $P^u_0$ and $P^s_0$ commute. \\

For the other term, we have

\begin{align*}
\langle \Psi(0), &\Phi_+(0, X_i; \beta_i^+) P^u_+(X_i; \beta_i^+) P^s_0 Q^-(0, \beta_i^-)(-X_i) \rangle \\
&= \langle \Psi(0), \Phi_+^u(0, X_i; \beta_i^+) P^s_-(-X_i; \beta^-) Q^-(0, \beta_i^-)(-X_i) \rangle + \mathcal{O}(e^{-3 \alpha X_i}) \\
&= \langle \Psi(0), \Phi_+^u(0, X_i; \beta_i^+) Q^-(0, \beta_i^-)(-X_i) \rangle + \mathcal{O}(e^{-3 \alpha X_i}) \\
&= \langle \Psi(0), \Phi(0, X_i) Q^-(0, \beta_i^-)(-X_i) \rangle + \mathcal{O}(e^{-3 \alpha X_i} + e^{-2 \alpha X_i} (e^{-\alpha X_1} + e^{-\alpha X_2})) \\
&= \langle \Psi(X_i), Q^-(0, \beta_i^-)(-X_i) \rangle + \mathcal{O}(e^{-3 \alpha X_i} + e^{-2 \alpha X_i} (e^{-\alpha X_1} + e^{-\alpha X_2})) \\
&= \langle \Psi(X_i), Q(-X_i) \rangle + \mathcal{O}(e^{-3 \alpha X_i} + e^{-2 \alpha X_i} (e^{-\alpha X_1} + e^{-\alpha X_2})) \\
\end{align*}

Thus we have

\begin{align*}
\langle \Psi(0), \Phi^u_+(0, X_i; \beta_i^+) a_i^+ \rangle
&= \langle \Psi(X_i), Q(-X_i) \rangle + \mathcal{O}(e^{-3 \alpha X_i} + e^{-2 \alpha X_i} (e^{-\alpha X_1} + e^{-\alpha X_2})) 
\end{align*}

Similarly, we have

\begin{align*}
 \langle \Psi(0), \Phi^s_-(0, -X_{i-1}, \beta_i^-) a_{i-1}^- \rangle 
&= \langle \Psi(-X_{i-1}), Q(X_{i-1}) \rangle + \mathcal{O}(e^{-3 \alpha X_{i-1}} + e^{-2 \alpha X_{i-1}} (e^{-\alpha X_1} + e^{-\alpha X_2})) 
\end{align*}

For the integral terms, we have

\begin{align*}
\left| \int_{X_i}^0 \langle \Psi(0), \Phi_+^u(0, y; \beta_i^+) G_i^+(y, V_i^+(y),\beta_i^+) \rangle \right| dy &\leq C \int_0^{X_i} e^{-\alpha y} |V_i^+(y)|^2 dy \\
&\leq C \int_0^{X_i} e^{-\alpha y} e^{-2 \alpha(X_i - y)}|e^{-\alpha (X_i - y)} V_i^+(y)|^2 dy \\
&\leq C \int_0^{X_i} e^{-\alpha y} e^{-2 \alpha(X_i - y)}(||V_i^+||_{X_i, +})^2 dy \\
&\leq C e^{-\alpha X_i} \int_0^{X_i} e^{-\alpha (X_i - y)} |a_i^+|^2 dy \\
&\leq C e^{-3 \alpha X_i}
\end{align*}

The other integral term is similar, and is of order $e^{-3 \alpha X_{i-1}}$\\

Combining these, we have

\begin{align*}
\langle \Psi(0), V_i^+(0) - V_i^-(0) \rangle = 
\langle \Psi(X_i), Q(-X_i) \rangle - \langle \Psi(-X_{i-1}), Q(X_{i-1}) \rangle + R_i
\end{align*}

where 

\begin{align*}
|R_i| \leq C ( e^{-3 \alpha X_1} +  e^{-3 \alpha X_2} )
\end{align*}

\end{proof}
\end{lemma}

The next lemma is an expression for $\langle \Psi(-x), Q(x) \rangle$

% lemma : expression for inner product

\begin{lemma}\label{IPform}
For $x > 0$ sufficiently large,
\begin{equation}
\langle \Psi(-x), Q(x) \rangle
= s_0 e^{-2 \alpha x} \sin(2 \beta x + \phi) + \mathcal{O}(e^{-(2 \alpha + \gamma) x})
\end{equation}
where $0 < \gamma \leq 1$, $s_0 > 0$, and $\phi$ are constants.
\begin{proof}
Based on our assumptions in Hypothesis \ref{assumptions}, this follows from Lemma 6.1 in San98. In our case, we do not have a parameter $\mu$, so the $\mu$-dependent terms are constant. If it turns out that $\gamma > 1$, we take $\gamma = 1$.
\end{proof}
\end{lemma}

We can then exploit symmetry properties to get an expression for $\langle \Psi(x), Q(-x) \rangle$.

% lemma : symmetry of IP

\begin{lemma}\label{otherIP}
For a reversible system, we have

\begin{equation}
\langle \Psi(x), Q(-x) \rangle = \langle \Psi(-x), Q(x) \rangle
\end{equation}

For a Hamiltonian system, we have

\begin{equation}
\langle \Psi(x), Q(-x) \rangle = \langle \Psi(-x), Q(x) \rangle
+ \mathcal{O}(e^{-3 \alpha x})
\end{equation}

\begin{proof}
The Hamiltonian case follows from proof of Lemma 3.8 in SanStrut, where we flip the inner product since we are in a real vector space. The reversibility follows from Lemma 5.3 in San98.
\end{proof}

\end{lemma}

In the next lemma, we rewrite the system of equations we are looking to solve using the above expressions for the appropriate inner products.

% Lemma : rewrite of system

\begin{lemma}

\begin{align*}
\langle \Psi(0), V_i^+(0) - V_i^-(0) \rangle = 
s_0 e^{\alpha \phi / \beta } r \left( a_i \sin \left( - \frac{\beta}{\alpha} \log a_i r \right) - a_{i-1} \sin \left( - \frac{\beta}{\alpha} \log a_{i-1} r \right) \right) + R_i(a, r)
\end{align*}

with 

\begin{align*}
R_i(a, r) &= \mathcal{O}(r^{1 + \tilde{\gamma}}) && 0 < \tilde{\gamma} < 1
\end{align*}

where 

\begin{align*}
a_i &= e^{-2 \alpha (X_i - X)} \\
r &= e^{-\alpha( 2X + \phi / \beta ) }
\end{align*}

and $X$ is chosen so that $X_i > X$ for all $i$.

\begin{proof}

Taking the log of $a_i$ and solving for $2 \alpha X_i$, we get

\[
2 \alpha X_i = -\log a_i + 2 \alpha X
\]

Taking the log of $r$ gives us

\[
\log r = -2 \alpha X - \alpha \phi / \beta
\]

which we substitute above to get

\begin{align*}
2 \alpha X_i &= -\log a_i - \log r - \alpha \phi / \beta \\
&= -\log a_i r - \alpha \phi / \beta \\
\end{align*}

Finally, we have

\begin{align*}
e^{-2 \alpha X_i} &= a_i e^{-2 \alpha X} \\
&= e^{\alpha \phi / \beta } a_i r
\end{align*}

Substituting this into our expression for $\langle \Psi(X), Q(-X) \rangle$ and using Lemmas \ref{IPform} and \ref{otherIP}, we have

\begin{align*}
\langle \Psi(X_i), Q(-X_i) \rangle 
&= s_0 e^{\alpha \phi / \beta } a_i r \sin \left( 2 \beta X_i  + \phi \right) + \mathcal{O}(e^{-(2 \alpha + \gamma) X_i}) \\
&= s_0 e^{\alpha \phi / \beta } a_i r \sin \left( \frac{\beta}{\alpha} ( -\log a_i r - \alpha \phi / \beta ) + \phi \right) + \mathcal{O}(e^{-(2 \alpha + \gamma) X_i}) \\
&= s_0 e^{\alpha \phi / \beta } a_i r \sin \left( - \frac{\beta}{\alpha} \log a_i r \right) + \mathcal{O}(e^{-(2 \alpha + \gamma) X_i}) \\
&= s_0 e^{\alpha \phi / \beta } a_i r \sin \left( - \frac{\beta}{\alpha} \log a_i r \right) + \mathcal{O}(e^{-2 \alpha X (1 + \gamma / 2 \alpha)}) \\
&= s_0 e^{\alpha \phi / \beta } a_i r \sin \left( - \frac{\beta}{\alpha} \log a_i r \right) + \mathcal{O}(r^{1 + \gamma / 2 \alpha})
\end{align*}

Similarly, we have

\begin{align*}
\langle \Psi(-X_{i-1}), Q(X_{i-1}) \rangle 
&= s_0 e^{\alpha \phi / \beta } a_{i-1} r \sin \left( - \frac{\beta}{\alpha} \log a_{i-1} r \right) + \mathcal{O}(r^{1 + \tilde{\gamma}}) \\
\end{align*}

where $\tilde{\gamma} = \min\{ 1, \gamma / 2 \alpha \}$. \\

For the remainder term $R_i$ from Lemma \ref{IPform}, we have

\begin{align*}
R_i(a, r) &= \mathcal{O}( e^{-3 \alpha X_1} +  e^{-3 \alpha X_2} ) \\
&= \mathcal{O}( e^{-2 \alpha X} e^{-\alpha X} ) \\
&= \mathcal{O}(r^{1 + \tilde{\gamma}}) 
\end{align*}

Since all remainder terms are of order $r^{1 + \tilde{\gamma}}$, the result follows. 

\end{proof}
\end{lemma}

In the next lemma, we write down the equations we need to solve for a periodic double pulse solution to exist. This is analogous to Proposition 3.8 in SanStrut.

% lemma : equations to solve

\begin{lemma}\label{systemtosolve}
A periodic double pulse solution exists if and only if for $i = 1, 2$

\begin{align*}
a_{i-1} \sin \left( - \frac{\beta}{\alpha} \log a_{i-1} r \right) - a_i \sin \left( - \frac{\beta}{\alpha} \log a_i r \right) = \tilde{R}_i(a, r)
\end{align*}

where 

\begin{align*}
R_i(a, r) &= \mathcal{O}(r^{\tilde{\gamma}}) \\
\frac{d}{d a_i} R_i(a, r) &= \mathcal{O}(r^{\tilde{\gamma}}) \\
\end{align*}

for some $\tilde{\gamma} > 0$

\begin{proof}
In the previous lemma, set $\langle \Psi(0), V_i^+(0) - V_i^-(0) \rangle = 0$ and rearrange. Then divide by $r$ and the constants out front, so that

\[
\tilde{R}_i(a, r) = \frac{1}{s_0 r} e^{-\alpha \phi / \beta } R_i(a, r)
\]

where $r > 0$. The bound on $\tilde{R}_i(a, r)$ comes from the bound on $R(a, r)$ from the previous lemma. By smoothness or somesuch, we get the bound on the derivative with respect to $a_i$.
\end{proof}
\end{lemma}

In the next lemma, we show we actually only need to solve one of these equations.

% lemma : get rid of equations

\begin{lemma}\label{reducedsystemtosolve}
If the system is Hamiltonian or reversible, a periodic double pulse solution exists if and only if

\begin{align*}
a_2 \sin \left(-\frac{\beta}{\alpha} \log a_2 r \right) - a_1 \sin \left( - \frac{\beta}{\alpha} \log a_1 r \right) = \tilde{R}_1(a, r)
\end{align*}

where $a_i$, $r$, and $R$ are as in the previous lemma.

\begin{proof} 
If the system is reversible, then by symmetry we only have to consider the equations for the left half of the double pulse. Thus we can discard half of the equations in the previous lemma, which leaves us with only one equation. To attain the desired solution, we use even symmetry to construct the right half of the double pulse.\\

If the system is Hamiltonian, the energy $H$ is conserved, thus all solutions must exist within level sets of $H$. Recall that $\Psi(0) = \nabla H(Q(0)) \neq 0$. Thus the energy $H$ must change along any jump in the direction of $\Psi(0)$. In Lemma \ref{solvefora}, we matched the tails of the pieces. Assume we have already matched $U_1^-(0) = U_1^+(0)$. In Lemma \ref{solveforbeta}, we matched the $Y^\pm$ components of $U_2^-(0) = U_2^+(0)$. All that remains is to match the $\R \Psi(0)$ component of $U_2^-(0) = U_2^+(0)$. But this is automatically 0 since the two pieces involved must have the same energy, and if this were nonzero there would be a change of energy at this point. Thus we can eliminate the final equation from the previous lemma.\\

In either case, since this is a double pulse, we are down to a single equation.
\end{proof}
\end{lemma}

In the next lemma, we (finally!) solve for the $a_i$. Since lots of periodic double pulses should be possible, there will not be a unique solution, but this will let us know which solutions are possible.

% lemma : possible solutions

\begin{lemma}

Let $r_n$ be defined by

\begin{equation}
r_n =  e^{-(2 \pi \alpha / \beta) n}
\end{equation}

Then for sufficiently large $n$, there exists a countable family of solutions $(a_1(k_1^0), a_2^0)$ to 

\begin{align*}
a_2 \sin \left( - \frac{\beta}{\alpha} \log a_2 r_n \right) - a_1 \sin \left( - \frac{\beta}{\alpha} \log a_1 r_n \right) = \tilde{R}_1(a, r_n)
\end{align*}

where

\begin{equation}
a_i^0 = e^{-\frac{\pi \alpha}{\beta} k_i^0 }
\end{equation}

for $k_i^0 \in \N_0$ and 

\begin{align*}
a_1(k_1^0, r_n) \approx a_1^0
\end{align*}

We should be able to get a bound on how close this is; it does not matter for now, but it might be nice to have.

\begin{proof}

From Lemma \ref{reducedsystemtosolve}, there is only one equation to solve.
Define the function $G: \R^3 \rightarrow \R$ by

\begin{equation}\label{arequation}
G(a_1, a_2, r) = a_2 \sin \left(-\frac{\beta}{\alpha} \log a_2 r \right) - a_1 \sin \left( - \frac{\beta}{\alpha} \log a_1 r \right) - \tilde{R}_1(a, r)
\end{equation}

We will take $r = r_n$, where
\[
r_n =  e^{-(2 \pi \alpha / \beta) n}
\]

and $n \in \N$. Noting that $(\beta / \alpha) \log r_n = 0$, the equation for $G$ becomes

\begin{equation}\label{rinremainder}
G(a_1, a_2, r_n) = a_2 \sin \left( - \frac{\beta}{\alpha} \log a_2 \right) - a_1 \sin \left( - \frac{\beta}{\alpha} \log a_1 \right) - \tilde{R}_1(a, r_n)
\end{equation}

The only dependence on $r_n$ is via the last term on the RHS, and we know that $\tilde{R}_i(a, r_n) = \mathcal{O}(r^{\tilde{\gamma}})$ from Lemma \ref{systemtosolve}. By continuity of $G$, we can send $n \rightarrow \infty$ (thus $r_n \rightarrow 0$) to get

\begin{equation}
G(a_1, a_2, 0) = a_2 \sin \left(-\frac{\beta}{\alpha} \log a_2 \right) - a_1 \sin \left(-\frac{\beta}{\alpha} \log a_1 \right)
\end{equation}

Now let

\begin{equation}
a_i^0 = e^{-\frac{\pi \alpha}{\beta} k_i^0 }
\end{equation}

for $k_i^0 \in \N_0$. Then we have $G(a_1^0, a_2^0, 0) = 0$. What we would like to do now is to use the implicit function theorem to solve for both $a_i$ in terms of $r$ near $(a_1^0, a_2^0, 0)$. But we only have one equation, and there are three unknowns!\\

Fortunately there is a way around this. First, note that for $a_i = a_i^0$,

\begin{align*}
\sin\left( -\frac{\beta}{\alpha} \log a_i^0 r \right)
&= \sin \left( -\frac{\beta}{\alpha} \log a_i^0 - \frac{\beta}{\alpha} \log r \right) \\
&= \sin \left( \pi k_i^0 - \frac{\beta}{\alpha} \log r \right) \\
&= (-1)^{k_i^0 + 1} \sin \left(- \frac{\beta}{\alpha} \log r \right)
\end{align*} 

Take $a_2 = a_2^0$ in \eqref{arequation}.
Then let $\tilde{G}(a_1, r) = G(a_1, a_2^0, r)$, i.e.  

\begin{equation}\label{arequation}
\tilde{G}(a_1, r) = a_2^0 (-1)^{k_2^0 + 1} \sin \left(- \frac{\beta}{\alpha} \log r \right) - a_1 \sin \left( - \frac{\beta}{\alpha} \log a_1 r \right) - \tilde{R}_1(a, r)
\end{equation}

Taking $r = r_n$ and sending $n \rightarrow \infty$ gives us

\begin{equation}\label{arequation}
\tilde{G}(a_1, 0) = - a_1 \sin \left( - \frac{\beta}{\alpha} \log a_1 r \right)
\end{equation}

In particular, we have $\tilde{G}(a_1^0, 0) = 0$. We are now in position to use the implicit function theorem.\\

First, we note that using the bound on the derivative of $\tilde{R}_i(a, r)$ from Lemma \ref{systemtosolve}, the derivative of $\tilde{R}_i(a, r)$ with respect to $a_i$ at $r = 0$ is always 0, thus this term will not contribute. We also note that

\begin{align*}
\frac{\partial}{\partial a_i} a_i \sin \left( - \frac{\beta}{\alpha} \log a_i r \right)\Big|_{a_i = a_i^0, r = 0}
&= \sin \left( - \frac{\beta}{\alpha} \log a_i r \right)\Big|_{a_i = a_i^0, r = 0} + a_i \cos \left( - \frac{\beta}{\alpha} \log a_i r \right)\left( -\frac{\beta}{\alpha} \frac{1}{a_i} \right) \Big|_{a_i = a_i^0, r = 0}\\
&=(-1)^{k_i^0 + 1}\frac{\beta}{\alpha}
\end{align*}

where we took the limit as $r \rightarrow 0$ as we did above. Since

\[
\frac{\partial}{\partial a_1} \tilde{G}(a_1^0, 0) = -(-1)^{k_1^0 + 1}\frac{\beta}{\alpha} \neq 0
\]

we can use the implicit function theorem to solve for $a_1$ in terms of $r$ near $(a_1^0, 0)$, i.e. there exists $r_0 > 0$ so that for $0 < r < r_0$ and $a_1$ close to $a_1^0$ we have a function $a_1(r)$ such that $a_1(0) = a_1^0$ and $\tilde{G}(a_1(r),r) = 0$.\\

Now take $n \in \N$ so that $r_n < r_0$. Then

\[
G(a_1(r_n), a_2^0, r_n) = \tilde{G}(a_1(r_n),r_n) = 0
\] 

which proves the lemma.

\end{proof}
\end{lemma}

What we really would like is to have all of this in terms of the lengths $X_i$, since those are the physically meaningful parameters. We accomplish this in the next lemma.

% lemma : write back in terms of the X_i

\begin{lemma}
For sufficiently large $n \in \N$, there exists a countable family of lengths $(X_1(k_1^0; n), X_2(k_1^0; n))$ with $k_i^0 \in \N_0$ such that

\[
\langle \Psi(X_i), Q(-X_i) \rangle - \langle \Psi(-X_{i-1}), Q(X_{i-1}) \rangle + R_i = 0
\]

The lengths $(X_i(k_i^0; n)$ are given by

\begin{equation}
X_i(k_i^0; n) \approx \frac{1}{4} \left( \frac{2 \pi}{\beta} \right) k_i^0 + \frac{2 n \pi - \phi}{2 \beta}
\end{equation}

where the formula is exact for $X_2$.

\begin{proof}
Let $n \in \N$ be sufficiently large so that the previous lemma applies, and let $a_i^0$ be as in that lemma. Solving the expression for $a_i$ for $X_i$ yields

\[
X_i = -\frac{1}{2 \alpha}\log(a_i) + X(r_n)
\]

Solving for $X$ in terms of $r_n$ and substituting the expression for $r_n$, we get

\begin{align*}
X(r_n) &= -\frac{1}{2 \alpha}\log\left( r_n \right) - \frac{\phi}{2 \beta} \\
&= -\frac{1}{2 \alpha}\log\left( e^{-(2 \pi \alpha / \beta) n} \right) - \frac{\phi}{2 \beta} \\
&= \frac{n \pi}{\beta} - \frac{\phi}{2 \beta} \\
&= \frac{2 n \pi - \phi}{2 \beta} 
\end{align*}

By the previous lemma, $a_2$ is exactly $a_2^0$ and $a_1 \approx a_1^0$. Thus we have 

\begin{align*}
\log( a_i ) \approx -\frac{\pi \alpha}{\beta} k_i^0
\end{align*}

\[
X_i(k_i^0; n) \approx \frac{\pi}{2 \beta} k_i^0 + \frac{2 n \pi - \phi}{2 \beta} 
\]

We can rewrite this as

\[
X_i(k_i^0; n) \approx \frac{1}{4} \left( \frac{2 \pi}{\beta} \right) k_i^0 + \frac{2 n \pi - \phi}{2 \beta}
\]

where the expression for $X_2$ is exact.\\

Since $\frac{2 \pi}{\beta}$ is the period of ``twisting'' of the stable and unstable manifolds about each other near the equilibrium point, this agrees with our intuition of a join being possible every ``quarter-twist''.

\end{proof}
\end{lemma}

We have successfully constructed a periodic double pulse. We now combine these results in the following theorem

% put it all together

\begin{theorem}\label{2pconstruction}
Given the assumptions in Hypothesis \ref{assumptions}, a periodic double pulse solution $Q_{2p}(x)$ of $U' = F(U)$ exists. $Q_{2p}(x)$ can be written piecewise as

\begin{align*}
\begin{cases}
Q^-(0, \beta_i^-)(x) + U_i^-(x) \\
Q^+(0, \beta_i^+)(x) + U_i^+(x)
\end{cases}
\end{align*}

where $Q^-(\alpha, \beta^-)(0)$ parameterizes the unstable manifold near $Q(0)$ and $Q^+(\alpha, \beta^+)(0)$ parameterizes the stable manifold near $Q(0)$. $Q^\pm(0, \beta_i^\pm)(x)$ lies along the stable (unstable) manifold, and $U_i^\pm(x)$ is a remainder term.\\

For the remainder term $U_i^\pm(x)$, we have bounds

\begin{align*}
|U_i^-(x)| &\leq C e^{-\alpha X_{i-1}} e^{-\alpha(X_{i-1} + x)} \\
|U_i^+(x)| &\leq C e^{-\alpha X_i} e^{-\alpha(X_i - x)} \\
\end{align*}

For the initial conditions $\beta_i^\pm$, we have bounds

\begin{equation}
| \beta_i^\pm | \leq C (e^{-2 \alpha X_1} + e^{-2 \alpha X_2})
\end{equation}

This gives us bounds

\begin{align*}
|Q^\pm(0, \beta_i^\pm)(0) - Q(0)| = |\beta_i^\pm| \leq 
C \left( e^{-\alpha X_1} + e^{-\alpha X_2} \right)
\end{align*}

\end{theorem}

% corollary : our problem

\begin{corollary}
For KdV5, if $c > 0$ there exists a double pulse solution $Q_{2p}(x)$. This solution can be written piecewise as in Theorem \ref{2pconstruction}, and bounds on the pieces are given in that theorem.

\begin{proof}
All we need to do is verify the assumptions in Hypothesis \ref{assumptions} for KdV5. If $c > 0$, the eigenvalues of $DF(0)$ are the complex quartet $\pm \alpha \pm \beta$. The remaining assumptions are discussed in the opening section, except for the transversality condition, which is a standard assumption in the literature.
\end{proof}

\end{corollary}



\subsection{Setup of Eigenvalue Problem}

To assess the stability of such double pulse solutions, we will look at the spectrum of the linearization of KdV5 about these periodic, double pulse solutions. As in the case on the unbounded domain with an exponential weight, we first write the eigenvalue problem as a first order system. We know that there are two eigenvalues at 0. We also know there will be two or four small eigenvalues from the pulse interactions. To locate those eigenvalues, we will follow the same procedure as in the unbounded case and as in San98. Thus, we will write the eigenfunction $V$ piecewise as

\[
V_i^\pm(x) = d_i (Q_{2p}'(x) + \lambda (Q_{2p})_c(x)) + W_i^\pm 
\]

Plugging this into the eigenvalue problem, we want to a system of the following form

\begin{enumerate}[(i)]
\item $(W_i^\pm)' = A(q_{2p}(x)) W_i^\pm + \lambda B W_i^\pm + \lambda^2 d_i \tilde{H}_i^\pm$
\item $W_1^+(X_1) - W_2^-(-X_1) = D_1 d$
\item $W_2^+(X_2) - W_1^-(-X_2) = D_2 d$
\end{enumerate}

where $A(q_{2p}(x))$ is the Jacobian of the linearization about the periodic double pulse, $B$ is the matrix defined previously, and 

\begin{align*}
D_1 d &= d_2(Q_{2p}'(-X_1) + \lambda (Q_{2p})_c(-X_1))
- d_1 ( Q_{2p}'(X_1) + \lambda (Q_{2p})_c(X_1) ) \\
D_2 d &= d_1(Q_{2p}'(-X_2) + \lambda (Q_{2p})_c(-X_2))
- d_2 ( Q_{2p}'(X_2) + \lambda (Q_{2p})_c(X_2) ) \\
\tilde{H} &= -B(Q_{2p})_c \\
H &= -B Q_c \\
\Delta H &= \tilde{H} - H
\end{align*}

We would like to exploit the construction of the periodic double pulse in Theorem \ref{2pconstruction}. To do this, we note that the Jacobian $A(q(x))$ depends linearly on $q(x)$ and its derivatives. Thus we have (piecewise)

\begin{equation}
A(q_{2p}(x)) = A(q^\pm(0, \beta_i^\pm)(x)) + A(u^\pm(x)) 
\end{equation}

For convenience, we let

\begin{align*}
G_i^\pm &= A(u^\pm(x))
\end{align*}

Then our system of equations to solve becomes

\begin{enumerate}[(i)]
\item $(W_i^\pm)' = A(q^\pm(0, \beta_i^\pm)(x)) W_i^\pm + \lambda B W_i^\pm + G_i^\pm W_i^\pm \lambda^2 d_i \tilde{H}_i^\pm$
\item $W_1^+(X_1) - W_2^-(-X_1) = D_1 d$
\item $W_2^+(X_2) - W_1^-(-X_2) = D_2 d$
\end{enumerate}

An important difference between this and the exponentially weighted case on the full line is that instead of a single term $A W_i^\pm$ we have the piecewise terms $A(q^\pm(0, \beta_i^\pm)(x)) $. We should be okay, since in all cases the functions $q^\pm(0, \beta_i^\pm)$ decay in the appropriate directions along the appropriate manifold. In San98, we have two piecewise terms $A^\pm W_i^\pm$ which depend on the parameter $\mu$. We can think of our case as similar, except we have four (fixed) parameters $\beta_i^\pm$, which give initial conditions on the appropriate manifolds.\\

We now need to formulate the other conditions for the problem. Recall that for the exponentially weighted case (or the hyperbolic case) the other conditions are given by

\begin{enumerate}[(i)]
\item $W_i^\pm(0) \in \C \psi(0) \oplus Y^+ \oplus Y^-$
\item $W_i^+(0) - W_i^-(0) \in \C \psi(0)$
\end{enumerate}

Let us briefly recall what the various spaces are involved in these conditions. Note at that all of the conditions are specified at $x = 0$, which is at the peak of the two pulses.

\begin{enumerate}
	\item $\text{span }\{Q'(0)\}$ is the span of the derivative of the pulse at $x = 0$. This is in both the stable and unstable manifolds of 0.
	\item $Y^+$ is the remaining dimension of the two-dimensional stable manifold.
	\item $Y^-$ is the remaining dimension of the two-dimensional unstable manifold.
	\item $\C \Psi(0)$ is the span of the solution to the adjoint variational equation, and since it is perpendicular to the unstable and stable manifolds, it fills out $\C^4$ when combined with the three things above.
\end{enumerate}

In this case, we will have an additional center space at $x = 0$, which gives us one more dimension. Let $Y^0$ be this center space. We will characterize this more fully later, but what we need to do now is figure out what role this plays in our conditions. Elements in the center space may blow up (not necessarily exponentially) at one of the ends. Thus if seek a localized eigenfunction, it is important that the initial condition at $x = 0$ not have a component in this center space. \\

This condition tells us where our perturbation must live. Since we are specifying our eigenfunction as $V_i^\pm(x) = d_i(q_{2p}'(x) + \lambda (q_{2p})_c(x)) + W_i^\pm $, and we know that $q_{2p}(x)$ and its derivatives have no component in the $Y^0$ since they both decay exponentially at both ends, this implies that $W_i^\pm(0)$ cannot have a component in $Y^0$. This also implies that $W_i^+(0) - W_i^-(0)$ cannot have a component in $Y^0$. Thus the conditions are exactly the same as for the version without the center space.\\

\begin{enumerate}
\item $W^\pm(0) \in \C \psi(0) \oplus Y^+ \oplus Y^-$
\item $W^+(0) - W^-(0) \in \C \psi(0) $
\end{enumerate}

Before we continue, we will rewrite the problem slightly. The idea is the following. The matrix $A(0)$ has an eigenvalue at 0, which gives it a center subspace. What we would like to do is combine the matrices $A(q^\pm(0, \beta_i^\pm)(x)) $ and $\lambda B$ to get a new matrix (depending on $\lambda$). In this new, $\lambda$-dependent matrix, the eigenvalue at 0 will perturb slightly to a small eigenvalue in a $\lambda$-dependent fashion.\\

We will still call the eigenspace corresponding to the small eigenvalue a center space, even though it might not technically be one. Thus we define

\[
\tilde{A}(q^\pm(0, \beta_i^\pm)(x); \lambda) = A(q^\pm(0, \beta_i^\pm)(x)) + \lambda B
\]

The first equation in the problem then becomes

\[
(W_i^\pm)' = \tilde{A}(q^\pm(0, \beta_i^\pm)(x); \lambda) W_i^\pm + G_i^\pm W_i^\pm + \lambda B W_i^\pm + \lambda^2 d_i \tilde{H}_i^\pm
\]

At this point, we drop the tilde over $A$ and the final version of the problem becomes

\begin{enumerate}[(i)]
\item $(W_i^\pm)' = A(q^\pm(0, \beta_i^\pm)(x); \lambda) W_i^\pm + G_i^\pm W_i^\pm + \lambda^2 d_i \tilde{H}_i^\pm$
\item $W_1^+(X_1) - W_2^-(-X_1) = D_1 d$
\item $W_2^+(X_2) - W_1^-(-X_2) = D_2 d$
\item $W^\pm(0) \in \C \psi(0) \oplus Y^+ \oplus Y^-$
\item $W^+(0) - W^-(0) \in \C \psi(0) $
\end{enumerate}

As a justification for the $\lambda$-dependent perturbation of the center eigenvalue of $A(q^\pm(0, \beta_i^\pm)(x); \lambda)$, we give the following heuristic argument. The matrix $A(q^\pm(0, \beta_i^\pm)(x); \lambda)$ is exponentially asymptotic (due to the exponential decay properties of $q^\pm(0, \beta_i^\pm)(x)$ and its derivatives in the appropriate directions). Let 

\[
A(\lambda) = \lim_{x \rightarrow \infty} A(q^+(0, \beta_i^+)(x); \lambda) 
= \lim_{x \rightarrow -\infty} A(q^-(0, \beta_i^-)(x); \lambda) = A(0; \lambda)
\]

Note that the limits do not depend on $i$ and that the two limits are equal due to decay properties of $q^\pm(0, \beta_i^\pm)(x)$ and its derivatives.\\

For $\lambda = 0$, $A(0)$ has an eigenvalue at 0. The characteristic polynomial for $A(\lambda)$ is $f(\nu; \lambda) = \lambda - c \nu + \nu^3 - \nu^5$, which to leading order is $f(\nu; \lambda) = \lambda - c \nu + \mathcal{O}(\nu^3)$. Thus, to leading order, this polynomial has a zero when $t = \lambda / c$, which is approximately the value of the spatial eigenvalue closest to 0. For small $\lambda$, numerics shows this is very close.\\

Before we continue, we will hypothesize the following bounds on the elements of our problem. We will eventually prove these, of course. The idea behind these hypothesized bounds is that deviation from the single pulse, nonperiodic case decreases exponentially with distance from the center, so we are letting the ``worst offender'' dictate the behavior. Since the $D_i$ equations only depend on $X_i$, the bound only involves that length parameter.

\begin{hypothesis}\label{problembounds}
We have the following estimates/bounds on terms in our problem
\begin{align*}
\Delta H &= \mathcal{O}(e^{-\alpha X_1} + e^{-\alpha X_2} ) \\
D_i &= ( Q'(X_i) + Q'(-X_i))(d_{i+1} - d_i ) + \mathcal{O} \left( e^{-\alpha X_i} \left( |\lambda| +  e^{-\alpha X_i}  \right) |d| \right)
\end{align*}
\end{hypothesis}

From Theorem \ref{2pconstruction}, we have the following piecewise bound for $G_i^\pm$

\begin{align*}
|G_i^-(x)| &\leq C e^{-\alpha X_{i-1}} e^{-\alpha(X_{i-1} + x) } \\
|G_i^+(x)| &\leq C e^{-\alpha X_i} e^{-\alpha(X_i - x) } \\
\end{align*}

Note that the bound is strongest at $x = 0$ and is weaker at $\pm X_i$. We also have the uniform bound for $G$

\begin{equation}\label{uniformG}  
||G|| = \sup |G_i^\pm(x)| \leq C (e^{-\alpha X_1} + e^{-\alpha X_2})
\end{equation}

\subsection{Characterization of Center Subspace}

Consider the first order linear system on $[0, \infty)$

\begin{equation}\label{justA}
V' = A(q^+(0, \beta^+)(x); \lambda) V
\end{equation}

Note that this paramaterized by both $\beta^+$ (the IC on the stable manifold) and by $\lambda$.\\

Let $\Phi(y, x; \beta^+, \lambda)$ be the evolution operator for \eqref{justA}. For $\lambda = 0$, as mentioned above, the matrix $A$ has a center subspace. As we perturb $\lambda$ slightly, this center subspace may peturb into a subspace which grows (or decays) exponentially, albeit at a slow rate since the spatial eigenvalue responsible for this is small. Thus instead of an exponential dichotomy, as in the hyperbolic case, we have an exponential trichotomy. This trichotomy will occur separately on $\R^+$ and $\R^-$.\\

Following HaleLin85, we will write this exponential trichotomy as follows. We have $\lambda$-dependent projections $P^s_\pm(x; \beta^+, \lambda)$, $P^u_\pm(x; \beta^+, \lambda)$ and $P^c_\pm(x; \beta^+, \lambda) = I - P^s_\pm(x; \beta^+, \lambda) - P^u_\pm(x; \beta^+, \lambda)$ (where the subscripts designate whether the trichotomy is on $\R^+$ or $\R^-$) such that

\begin{align*}
\Phi(y, x; \beta^+, \lambda)P^s_\pm(x; \beta^+, \lambda) &= P^s_\pm(y; \beta^+, \lambda)\Phi(y, x; \beta^+, \lambda) \\
\Phi(y, x; \beta^+, \lambda)P^u_\pm(x; \beta^+, \lambda) &= P^u_\pm(y; \beta^+, \lambda)\Phi(y, x; \beta^+, \lambda) \\
\Phi(y, x; \beta^+, \lambda)P^c_\pm(x; \beta^+, \lambda) &= P^c_\pm(y; \beta^+, \lambda)\Phi(y, x; \beta^+, \lambda) \\
\end{align*}

In other words, the projections commute with the evolution, so it does not matter if you project or evolve first. For $\lambda = 0$ the superscript $c$ is an actual center subspace, and for small $\lambda$, this is the subspace that the center subspace perturbs to. Using these projections, we can split the evolution up into evolution on the three subspaces by defining

\begin{align*}
\Phi^s_\pm(y, x; \beta^+, \lambda) &= \Phi(y, x; \beta^+, \lambda)P^s_\pm(x; \beta^+, \lambda) \\
\Phi^u_\pm(y, x; \beta^+, \lambda) &= \Phi(y, x; \beta^+, \lambda)P^u_\pm(x; \beta^+, \lambda) \\
\Phi^c_\pm(y, x; \beta^+, \lambda) &= \Phi(y, x; \beta^+, \lambda)P^c_\pm(x; \beta^+, \lambda) \\
\end{align*}

For the stable and unstable subspaces, we know what the eigenvalues of $A(0, 0)$ are. In particular, we have a constant $\alpha$ for which $\alpha$ is the smallest real part of the positive eigenvalues of $A(0)$ and $-\alpha$ is the largest real part of the negative eigenvalues of $A(0)$. For small $\lambda$ and $\beta^+$, the spatial eigenvalues will not perturb much, so for stable and unstable subspaces we will still have the bounds

\begin{align*}
|\Phi^s(y, x; \beta^+, \lambda)| \leq C e^{-\alpha(y-x)} \\
|\Phi^u(x, y; \beta^+, \lambda)| \leq C e^{-\alpha(y-x)}
\end{align*}

Technically, we should probably replace $\alpha$ by $\alpha - \delta$ for small $\delta$ to account for the perturbation, but it does not matter for now. Also, since $\beta^+$ is just the IC on the stable manifold, the decay rate does not change when this is altered.\\

For the center subspace, there is a small eigenvalue which is approximately $\nu = \lambda / c$. Thus we expect to have a $\lambda$-dependent bound for the center evolution resembling.

\begin{align*}
|\Phi^c_+(y, x; \beta^+, \lambda)| &\leq C e^{(\text{Re }\lambda)(y-x)/c} && y \geq x \geq 0 \\
|\Phi^c_-(x, y; \beta^+, \lambda)| &\leq C e^{(\text{Re }\lambda)/c} && x \leq y \leq 0 \\
\end{align*}

At this point, we want to make all of this precise. We start by proving the following lemma, based on Exercise 29 on p. 104 of Coddington and Levinson (1955).

\begin{lemma}Consider the eigenvalue problem on $\R^+$

\begin{equation}\label{veigproblem}
V(x)' = AV(x) + R(x)V(x)
\end{equation}

where $V(x) \in R^n$, $A$ is a constant, diagonalizable $n \times n$ matrix, and $R(x): \R \rightarrow \R^n$ is an integrable function which is globally Lipschitz continuous in $x$. Let $\nu$ be any eigenvalue of $A$ with corresponding eigenvector $p$, i.e. $A p = \nu p$. Then there is a unique solution $\phi(x)$ to \eqref{veigproblem} such that 

\[
\lim_{x\rightarrow\infty} \phi(x) e^{-\nu x} = p
\]

In other words, for large $x$, the solution $\phi(x)$ resembles that of the constant coefficient eigenvalue problem $V(x)' = AV(x)$.

\begin{proof}
Let $\sigma = \text{Re} \nu$. Let $\nu_1, \dots, \nu_n$ be the $n$ eigenvalues of $A$ with corresponding eigenvectors $p_1, \dots, p_n$. Since we are assuming that $A$ is diagonalizable, we have a complete set of $n$ of these. Order the eigenvalues by increasing real part; if more than one eigenvalue has the same real part, any order is fine, as long as we make sure that any eigenvalue besides $\nu$ with real part $\sigma$ occurs after $\nu$ in the list. Then $\nu = \nu_k$ for some $k$, $\text{Re} \nu_j < \sigma$ for $j < k$ (as long as $k \neq 1$), and $\text{Re} \nu_j \geq \sigma$ for $j \geq k$. \\

Let $e^{Ax}$ be the fundamental matrix solution for $U' = A U$, and split $e^{Ax}$ up into
\[
e^{Ax} = Y_1(x) + Y_2(x)
\]

where $Y_1$ involves only eigenvectors corresponding to eigenvalues $\nu_1, \dots, \nu_{k-1}$ and $Y_2$ involves only eigenvectors corresponding to eigenvalues $\nu_{k}, \dots, \nu_n$. Since $A$ is a constant-coefficient matrix, we can write down an explicit formula for $Y_1$ and $Y_2$. Essentially, all we need to do is change coordinates to the eigenbasis, evolve along the appropriate eigenvectors, and zero out the other ones. To be specific, let P be the $n \times n$ matrix with columns $p_1, \dots, p_n$. Since we are assuming $A$ is diagonalizable, this matrix is invertible, and $D = P^{-1}AP$ is diagonal with eigenvalues $\nu_1, \dots, \nu_n$ on the diagonal. Recall that the matrix exponential is given by $e^{Ax} = P^{-1}e^{Dx}P$. Starting with the matrix $D$, form the matrix $D_1$ by keeping only the eigenvalues $\nu_1, \dots, \nu_{k-1}$ on the diagonal, and form the matrix $D_2$ by keeping only the eigenvalues $\nu_{k}, \dots, \nu_n$ on the diagonal. Then we have

\begin{align*}
Y_1(x) &= P^{-1}e^{D_1x}P \\
Y_1(x) &= P^{-1}e^{D_2x}P \\
\end{align*}
 
Choose $\delta$ such that $0 < \delta < \sigma - \text{Re} \nu_{k-1}$, i.e. smaller than the spectral gap between $\nu$ and the eigenvalue with the next smallest real part. (If $k = 1$, $Y_1 = 0$, $Y_2 = e^{Ax}$, and we don't care about $\delta$). Then we can find a constant $C$ such that

\begin{align*} 
|Y_1(x)| &\leq Ce^{(\sigma - \delta)x} && x \geq 0 \\
|Y_2(x)| &\leq Ce^{\sigma x} && x \leq 0 
\end{align*}

Define the exponentially weighted function space with weight $\sigma$

\[
B_{\sigma, a} = \{ f \in C^0([a, \infty), \R^n) : \sup_{x \in [a, \infty)} |e^{-\sigma x} f(x)| < \infty 
\]

where $a$ will be chosen later. The norm on this space is given by

\[
||f||_{\sigma, a} = \sup_{x \in [a, \infty)} |e^{-\sigma x} f(x)|
\]

In other words, we allow functions in $B_{\sigma, a}$ to grow exponentially as $x \rightarrow \infty$ at a rate of $\sigma$ or slower. It is known that $B_{\sigma, a}$ is a Banach space. Define the operator $F$ on $B_{\sigma, a}$ by

\begin{align*}
F(\phi)(x) = e^{\nu x} p + \int_a^x Y_1(x - y)R(y)\phi(y)dy + \int_\infty^x Y_2(x - y)R(y)\phi(y)dy
\end{align*}

where the $a$ in the integral is the same as in $B_{\sigma, a}$ and will be chosen later. First we show that $F: B_{\sigma, a} \rightarrow B_{\sigma, a}$. Let $\phi \in B_{\sigma, a}$. For $x \geq a$ we have

\begin{align*}
|e^{-\sigma x} &F(\phi)(x)| \leq e^{(\nu - \sigma) x} |p| + \int_a^x |Y_1(x - y)||R(y)||\phi(y)| dy + \int_x^\infty |Y_2(x - y)||R(y)||\phi(y)|dy \\
&\leq |p| + C \left( e^{-\sigma x}  \int_a^x e^{(\sigma - \delta)(x - y)}|R(y)||\phi(y)| dy + e^{-\sigma x}  \int_x^\infty e^{\sigma(x - y)}|R(y)||\phi(y)|dy \right) \\
&\leq |p| +  C \left( \int_a^x e^{-\delta(x - y)}|R(y)||e^{-\sigma y}\phi(y)| dy + \int_x^\infty |R(y)||e^{-\sigma y} \phi(y)|dy \right) \\
&\leq |p| + C ||\phi||_{\sigma, a}\left( \int_a^x e^{-\delta(x - y)}|R(y)| dy + \int_x^\infty |R(y)|dy \right) \\
&\leq |p| + C ||\phi||_{\sigma, a} \int_a^\infty |R(y)| dy 
\end{align*}

Since $R$ is integrable, the RHS is finite, thus the map $F: B_{\sigma, a} \rightarrow B_{\sigma, a}$ is well defined. Now we show the map $F$ is a contraction. Let $\phi, \psi \in B_{\sigma, a}$. For $x \geq a$ we have

\begin{align*}
|e^{-\sigma x}( &F(\phi)(x) - F(\psi)(x))| \leq \int_a^x |Y_1(x - y)||R(y)||\phi(y) - \psi(y)| dy + \int_x^\infty |Y_2(x - y)||R(y)||\phi(y) - \psi(y)|dy \\
&\leq C \left( e^{-\sigma x}  \int_a^x e^{(\sigma - \delta)(x - y)}|R(y)||\phi(y) - \psi(y)| dy + e^{-\sigma x}  \int_x^\infty e^{\sigma(x - y)}|R(y)||\phi(y) - \psi(y)|dy \right) \\
&\leq C \left( \int_a^x e^{-\delta(x - y)}|R(y)||e^{-\sigma y}(\phi(y) - \psi(y))| dy + \int_x^\infty |R(y)||e^{-\sigma y} (\phi(y) - \psi(y))|dy \right) \\
&\leq C ||\phi - \psi ||_{\sigma, a}\left( \int_a^x e^{-\delta(x - y)}|R(y)| dy + \int_x^\infty |R(y)|dy \right) \\
&\leq C ||\phi - \psi ||_{\sigma, a} \int_a^\infty |R(y)| dy 
\end{align*}

Since $R$ is integrable, we can choose $a$ sufficiently large so that

\[
\int_a^\infty |R(y)| dy < \frac{1}{2C}
\]

from which we conclude that

\[
||F(\phi) - F(\psi) ||_{\sigma, a} \leq \frac{1}{2} ||\phi - \psi ||_{\sigma, a}
\]

Since $F$ is a contraction on the Banach space $B_{\sigma, a}$, by the Banach Fixed Point Theorem the map $F$ has a unique fixed point, i.e. a unique $\phi(x) \in B_{\sigma, a}$ such that $F(\phi) = \phi$. Note that by the fixed point theorem and definition of $B_{\sigma, a}$, we only have $\phi(x)$ defined for $\phi \geq a$. However, choosing the initial condition $\phi(a)$ at $x = a$, by the existence and uniqueness of solutions to \eqref{veigproblem} and the global Lipschitz condition placed on $R(x)$ (which guarantees global existence of solutions), we can extend $\phi(x)$ uniquely to all of $\R$. This extension of $\phi(x)$ to $\R$ is given by the same formula we have for $\phi(x)$ when $x \geq a$.

\begin{equation}\label{fpphi}
\phi(x) = e^{\nu x} p + \int_a^x Y_1(x - y)R(y)\phi(y)dy + \int_\infty^x Y_2(x - y)R(y)\phi(y)dy
\end{equation}

To see this, all we need to do is show that it satisfies the ODE \eqref{veigproblem}. Differentiating \eqref{fpphi}, we get

\begin{align*}
\phi'(x) &= \nu e^{\nu x} p + (Y_1(0) + Y_2(0))R(x)\phi(x) + \int_a^x Y_1'(x - y)R(y)\phi(y)dy + \int_\infty^x Y_2'(x - y)R(y)\phi(y)dy \\
&= e^{\nu x} A p + e^{0A}R(x)\phi(x) + \int_a^x A Y_1(x - y)R(y)\phi(y)dy + \int_\infty^x A Y_2(x - y)R(y)\phi(y)dy \\
&= A \left( e^{\nu x} p + \int_a^x Y_1(x - y)R(y)\phi(y)dy + \int_\infty^x Y_2(x - y)R(y)\phi(y)dy \right) + R(x) \phi(x) \\
&= A \phi(x) + R(x) \phi(x)
\end{align*}

Thus $\phi(x)$ defined in \eqref{fpphi} is the unique extension that we seek. All that remains is to show what happens when $x \rightarrow \infty$. Since we are interested in end behavior, we only need to consider what happens when $x \geq a$. Since $\phi(x) \in B_{\lambda, a}$, $||\phi||_{\sigma, a}$ is finite and independent of $x$. Thus for $x \geq a$, using what we did above, 

\begin{align*}
|e^{-\sigma x} &(\phi(x) - e^{\nu x} p)| \leq C ||\phi||_{\sigma, a}\left( \int_a^x e^{-\delta(x - y)}|R(y)| dy + \int_x^\infty |R(y)|dy \right) \\
&\leq C ||\phi||_{\sigma, a}\left( \int_a^{x/2} e^{-\delta(x - y)}|R(y)| dy + \int_{x/2}^x |R(y)|+ \int_x^\infty |R(y)|dy \right)\\
&\leq C ||\phi||_{\sigma, a}\left( e^{-\delta(x/2)} \int_a^{x/2} e^{-\delta(x/2 - y)}|R(y)| dy + \int_{x/2}^\infty |R(y)|dy \right)\\
&\leq C ||\phi||_{\sigma, a}\left( e^{-\delta(x/2)} \int_a^{\infty} |R(y)| dy + \int_{x/2}^\infty |R(y)|dy \right)\\
&\leq ||\phi||_{\sigma, a}\left(\frac{1}{2} e^{-\delta(x/2)} + \int_{x/2}^\infty |R(y)|dy \right)
\end{align*}

Since $\delta > 0$, $R(x)$ is integrable, and $||\phi||_{\sigma, a}$ is constant, both terms on the RHS go to 0 as $x \rightarrow \infty$. Thus we conclude that

\[
\lim_{x \rightarrow \infty} |e^{-\sigma x} (\phi(x) - e^{\nu x} p)| = 0
\]

Pulling out a factor of $e^{\nu x}$, this becomes 

\[
\lim_{x \rightarrow \infty} |e^{(\nu - \sigma) x}||\phi(x) e^{-\nu x} - p)| = 0
\]

since $\text{Re} \phi = \nu$, $|e^{(\nu - \sigma) x} = 1$ for all $x$. Thus we conclude that

\[
\lim_{x \rightarrow \infty} |\phi(x) e^{-\nu x} - p)| = 0
\]  

from which it follows that

\[
\lim_{x\rightarrow\infty} \phi(x) e^{-\nu x} = p
\]

\end{proof}
\end{lemma}

\begin{corollary}The same result holds on $(-\infty, 0]$ if we take $x \rightarrow -\infty$, i.e. there is a unique solution $\tilde{\phi}(x)$ to \eqref{veigproblem} such that 

\[
\lim_{x\rightarrow -\infty} \tilde{\phi}(x) e^{-\nu x} = p
\]

This function in general will not be the same as $\phi(x)$ above.

\begin{proof}
First we replace $x$ with $-x$ in \eqref{veigproblem}

\begin{align*}
V'(-x) = A V(-x) + R(-x)V(-x)
\end{align*}

Let $\tilde{V}(x) = V(-x)$ and $\tilde{R}(x) = -R(-x)$. Then since $\tilde{V}'(x) = -V'(-x)$, we get

\begin{align*}
\tilde{V}'(x) = -A \tilde{V}(x) + \tilde{R}(x)\tilde{V}(x)
\end{align*}

Now we use the above lemma on $\tilde{V}$ (for $x \geq 0$). Since $-A$ has an eigenvector $p$ with corresponding eigenvalue $-\nu$, by the above lemma we can find a unique solution $\psi(x)$ such that 

\[
\lim_{x\rightarrow \infty} \psi(x) e^{(-\nu)(-x)} = p
\]

Let $\tilde{\phi(x)} = \psi(-x)$. Then $\tilde{\phi(x)}$ solves the original problem, and

\[
\lim_{x\rightarrow -\infty} \tilde{\phi}(x) e^{-\nu x } = p
\]

\end{proof}
\end{corollary}

Using this, we can derive an expression for the evolution on the center subspace. First, consider the following eigenvalue problem and its adjoint problem.

\begin{align}
V' &= A(q^+(0, \beta^+)(x); \lambda) V \label{eig:V} \\
W' &= -A(q^+(0, \beta^+)(x); \lambda)^* W \label{eig:W}
\end{align}

Note that $V$ and $W$ will depend on $\beta^+$, but (for convenience) we suppress that notation for now and will bring it back when needed. Let $\Phi(y, x; \beta^+, \lambda)$ be the evolution operator for \eqref{eig:V}.\\

We summarize some useful facts about this problem in the following lemma. Note that since we are in the space $\C^n$, we have defined the inner product on $\C^n$ by $\langle x, y \rangle = \sum x_i \bar{y_i}$, i.e. the complex conjugation is on the second component.

\begin{lemma}\label{eigadjoint}
Consider the linear ODE $V' = A(x)V$ and the corresponding adjoint problem $W' = -A(x)^* W$, where $A$ is an $n \times n$ matrix depending on $x$. Then the following are true.
\begin{enumerate}[(i)]
\item $\frac{d}{dx}\langle V(x), W(x) \rangle = 0$, thus the inner product is constant as $x$ varies.
\item If $\Phi(y, x)$ is the evolution operator for $V' = A(x)V$, then $\Phi(x, y)^*$ is the evolution operator for the adjoint problem $W' = -W(x)^* W$.
\end{enumerate}
\begin{proof}
For (i), take the derivative of the inner product and use the expressions for $V'$ and $W'$. For (ii), take the derivative of the expression $\Phi(y, x)\Phi(x, y) = I$.
\end{proof}
\end{lemma}

Let $\nu(\lambda)$ be the small eigenvalue of the asymptotic matrix $A(\lambda)$ with corresponding eigenvector $v_0(\lambda)$. Then since $\det(A - \nu I) = 0$ implies $\det(A^* - \overline{\nu}I) = 0$, $-\overline{\nu(\lambda)}$ is the small eigenvalue of $-A(\lambda)^*$; let $w_0(\lambda)$ be the corresponding eigenvector.\\

It would be great if there were a nice relationship between $v_0(\lambda)$ and $w_0(\lambda)$ (like there is with the corresponding eigenvalues), but without additional assumptions on $A(\lambda)$, this is not the case. Failing that, we would settle for knowing $\langle v_0(\lambda), w_0(\lambda) \rangle \neq 0$. Even this is too much to ask in the generic case. As an easy counterexample, if we take

\[
M = \begin{pmatrix}1 & 1 \\ 0 & 1 \end{pmatrix}
\]

$M$ has a single eigenvector $(1, 0)$ and $M^*$ has a single eigenvector $(0, 1)$, both corresponding to the lone eigenvalue 1. These are clearly orthogonal. Since we suspect the problem might be the Jordan block, we will prove the following lemma.

\begin{lemma}\label{perpeigs}
Let $A$ be an $n \times n$ matrix, and suppose $v$ and $w$ are solutions to $Av = \lambda v$ and $A^*w = \overline{\lambda}w$, respectively. Suppose $\lambda$ is a simple eigenvalue, i.e. is has algebraic multiplicity of 1. Then $\langle v, w \rangle \neq 0$.
\begin{proof}
Since $\lambda$ is simple, $\text{span} \{w\} = \ker(A^* - \overline{\lambda}I)$. Suppose $v \perp w$. Then $v \in \ker(A^* - \overline{\lambda I})^\perp = \text{ran}(A - \lambda I)$, where the equality holds since $A$ is finite dimensional, thus has closed range. But this implies $(A - \lambda I)v_1 = v$ for some $v_1$, which cannot be the case since $\lambda$ is simple, so there cannot be such a generalized eigenvector. We conclude that $\langle v, w \rangle \neq 0$
\end{proof}
\end{lemma}

Given this lemma, we make the following hypothesis.

\begin{hypothesis}\label{simplesmalleig}
For sufficiently small $\lambda$, the small eigenvalue $\nu(\lambda)$ of the matrix $A(\lambda)$ is simple.
\end{hypothesis}

I am not actually sure we need to state this as a hypothesis, since the eigenvalues of $A(\lambda)$ are analytic in $\lambda$, $A(0)$ has a simple eigenvalue at 0, and the rest of the eigenvalues of $A(0)$ are large, but we might as well put it here for now.\\

Given Hypothesis \ref{simplesmalleig} (or the argument above), $\nu(\lambda)$ is a simple eigenvalue of $A(\lambda)$, thus by Lemma \ref{perpeigs}, $\langle v_0(\lambda), w_0(\lambda) \rangle \neq 0$. Since eigenvalues are defined up to scalar multiples, we can scale $v_0(\lambda)$ and/or $w_0(\lambda)$ such that

\[
\langle v_0(\lambda), w_0(\lambda) \rangle = 1
\]

Note that $v_0(\lambda), w_0(\lambda)$ depend on $\lambda$ but not on $\beta^+$.\\
 
Now we have all the pieces in place to derive a formula for the projection and evolution on the center space for $x \geq 0$. Using Lemma \ref{veigproblem}, let $\tilde{v}_+(x; \beta^+, \lambda)$ and $\tilde{w}_+(x; \beta^+, \lambda)$ be solutions to the eigenvalue problem \eqref{eig:V} and its adjoint problem \eqref{eig:W} such that

\begin{align*}
\lim_{x \rightarrow \infty} e^{-\nu(\lambda) x} \tilde{v}_+(x; \beta^+, \lambda) = v_0(\lambda) \\
\lim_{x \rightarrow \infty} e^{\overline{\nu(\lambda)} x} \tilde{w}_+(x; \beta^+, \lambda) = w_0(\lambda) \\
\end{align*}

We would like to scale out the exponential term, so let

\begin{align*}
\tilde{v}_+(x; \beta^+, \lambda) &= e^{\nu(\lambda) x } v_+(x; \beta^+, \lambda) \\
\tilde{w}_+(x; \beta^+, \lambda) &= e^{-\overline{\nu(\lambda)} x } w_+(x; \beta^+, \lambda) \\
\end{align*}

Then we have

\begin{align*}
\lim_{x \rightarrow \infty} v_+(x; \beta^+, \lambda) = v_0(\lambda) \\
\lim_{x \rightarrow \infty} w_+(x; \beta^+, \lambda) = w_0(\lambda) \\
\end{align*}

Note that since we scaled out any exponential term, we know that this limit exists, although we do not have a decay rate for $|v_+(x; \beta^+, \lambda) - v_0(\lambda)|$.\\

By Lemma \ref{eigadjoint}, $\langle \tilde{v}_+(x; \beta^+, \lambda), \tilde{w}_+(x; \beta^+, \lambda) \rangle$ is constant for all $x \geq 0$. Thus, taking $x \rightarrow \infty$ and using the continuity of the inner product, we conclude that for all $x \geq 0$,

\[
\langle \tilde{v}_+(x; \beta^+, \lambda), \tilde{w}_+(x; \beta^+, \lambda) \rangle = \langle v_0(\lambda), w_0(\lambda) \rangle = 1
\]

Looking at the inner product $\langle \tilde{v}_+(x; \beta^+, \lambda), \tilde{w}_+(x; \beta^+, \lambda) \rangle$, we have for all $x \geq 0$,

\begin{align*}
\langle \tilde{v}_+(x; \beta^+, \lambda), \tilde{w}_+(x; \beta^+, \lambda) \rangle
&= \langle e^{\nu(\lambda) x } v_+(x; \beta^+, \lambda), e^{-\overline{\nu(\lambda)} x} \tilde{w}_+(x; \beta^+, \lambda) \rangle \\
&= e^{\nu(\lambda) x } e^{-\nu(\lambda) x } \langle v_+(x; \beta^+, \lambda), w_+(x; \beta^+, \lambda) \rangle \\
&= \langle v_+(x; \beta^+, \lambda), w_+(x; \beta^+, \lambda) \rangle
\end{align*}

Combining these two results, it follows that $\langle v_+(x; \beta^+, \lambda), w_+(x; \beta^+, \lambda) \rangle = 1$ for all $x \geq 0$.\\

We will now write the projection $P^c_+(x; \beta^+, \lambda)$ for $x \geq 0$ in terms of the adjoint solution $\tilde{w}_+(x; \beta^+, \lambda)$. To do this, let $R^s_+(x; \beta^+, \lambda)$, $R^u_+(x; \beta^+, \lambda)$, and $R^c(x; \beta^+, \lambda)$ be the ranges of the corresponding projections (stable, unstable, and center). The dimensions of these ranges are 2, 2, and 1 (respectively). Note that since $\tilde{v}_+(x; \beta^+, \lambda)$ is in the center range $R^c(x; \beta^+, \lambda)$ and that space is one-dimensional, $\tilde{v}_+(x; \beta^+, \lambda)$ is a basis vector for that space. Since we can scale the basis vector by any constant we want, we can divide by $e^{\nu(\lambda) x}$, so that $v_+(x; \beta^+, \lambda)$ is a basis vector for $R^c(x; \beta^+, \lambda)$. Since this space is one-dimensional, every element in $R^c(x; \lambda)$ is a scalar multiple of $\tilde{v}_+(x; \beta^+, \lambda)$. \\

Next we show that the projection $P^c_+(x; \beta^+, \lambda)$ is a projection on the adjoint solution $w_+(x; \beta^+, \lambda)$. To do this, we first show that $\tilde{w}(0; \beta^+, \lambda)$ is perpendicular to $R^s_+(0; \beta^+, \lambda)$. Let $u(0) \in R^s_+(0; \beta^+, \lambda)$. Then $u(x) = \Phi(x, 0; \beta^+, \lambda)u(0) \in R^s_+(x; \beta^+, \lambda)$ for all $x \geq 0$. Since the inner product $\langle u(x), \tilde{w}_+(x; \beta^+, \lambda) \rangle$ is constant in $x$, we let $x \rightarrow \infty$ and use the continuity of the inner product to get

\begin{align*}
\lim_{x \rightarrow \infty} \langle u(x), \tilde{w}_+(x; \beta^+, \lambda) \rangle &= \lim_{x \rightarrow \infty} e^{-\nu(\lambda) x} \langle u(x), w_+(x; \beta^+, \lambda) \rangle \\
&= \langle \lim_{x \rightarrow \infty} e^{-\nu(\lambda) x} u(x), w_0(\lambda) \rangle \\
&= 0
\end{align*}

since $u(x)$ decays exponentially at a faster rate than $|\nu(\lambda)|$. Thus we have $\tilde{w}(0; \beta^+, \lambda) \perp R^s(0; \beta^+, \lambda)$. By Lemma \ref{perpeigs}, $\tilde{w}_+(x; \beta^+, \lambda) \perp R^s_+(x; \beta^+, \lambda)$ for all $ \geq 0$.\\

Note that we cannot play the same game with $R^u_+(x; \beta^+, \lambda)$, since to get decay to 0 we would have to take $x \rightarrow -\infty$. Since everything we have done so far pertains to the dichotomy/trichotomy on $\R^+$, the results above are only valid for $x \geq 0$, so we cannot take $x \rightarrow -\infty$.\\

However, we really want to have $\tilde{w}_+(x; \beta^+, \lambda) \perp R^u_+(x; \beta^+, \lambda)$ for all $x \geq 0$. Since $\tilde{w}_+(x; \beta^+, \lambda)$ is a nonzero scalar multiple of $w_+(x; \beta^+, \lambda)$, this is equivalent to having $w_+(x; \beta^+, \lambda) \perp R^u_+(x; \beta^+, \lambda)$ for $x \geq 0$ (they are the same at $x = 0$). To attain this, we will change coordinates. The idea is that if we do this at $x = 0$ to get $w_+(0; \beta^+, \lambda) \perp R^u_+(0; \beta^+, \lambda)$, the invariance of the inner product in $x$ should take care of the rest.\\

Let $\{ a^s_1(0), a^s_2(0)\}$ be a basis for $R^s_+(0; \beta^+, \lambda)$ and let $\{a^u_1(0), a^u_2(0)\}$ be a basis for $R^u_+(0; \beta^+, \lambda)$. By the discussion above, $w_+(0)$ is already perpendicular to $\{ a^s_1(0), a^s_2(0)\}$. All we need to do is change coordinates so that $w_+(0)$ is perpendicular to $\{ a^u_1(0), a^u_2(0)\}$. This is easy to accomplish by Gram-Schmidt since the set $\{ w_+(0), a^u_1(0), a^u_2(0) \}$ is linearly independent. Note that by this process, $w_+(0)$ remains unchanged, while $\{ a^u_1(0), a^u_2(0) \}$ may be modified. For convenience, we will use the same notation $\{ a^u_1(0), a^u_2(0) \}$ for the basis vectors of $R^u_+(0; \beta^+, \lambda)$ after the coordinate change.\\

Let $a^u_i(x) = \Phi(x,0; \beta^+, \lambda)a^u_i(0)$, i.e. evolve the new basis vectors forward. These, together with $w_+(0)$, will remain linearly independent after evolution. For simplicity, consider one of these, say $a^u_1(x)$. Since $\Phi(x,0; \lambda) a^u_1(0)$ solves the eigenvalue problem with initial condition $a^u_1(0)$, by Lemma \ref{perpeigs} we have

\begin{align*}
\langle a^u_1(x), \tilde{w}_+(x; \beta^+, \lambda) \rangle &= \langle \Phi(x,0; \lambda) a^u_1(0), \tilde{w}_+(x; \beta^+, \lambda) \rangle \\
&= \langle a^u_1(0), \tilde{w}(0; \beta^+, \lambda) \rangle \\
&= \langle a^u_1(0), w(0; \beta^+, \lambda) \rangle \\
&= 0
\end{align*}

Since $\tilde{w}_+(x; \beta^+, \lambda)$ is a scalar multiple of $w_+(x; \beta^+, \lambda)$, we also have for $x \geq 0$

\[
\langle a^u_1(x), w_+(x; \beta^+, \lambda) \rangle = 0
\]

Thus a single change of variables at $x = 0$ accomplishes what we want. Since $\tilde{w}_+(x; \beta^+, \lambda) \in R^c_+(x; \beta^+, \lambda)$ and is perpendicular to the other two spaces, to get the center range projection $P^c_+(x; \beta^+, \lambda)$, all we have to do is project onto $\tilde{w}_+(x; \beta^+, \lambda)$. To do this, we take the inner product with $\tilde{w}_+(x; \beta^+, \lambda)$ to get the component in the direction of the basis vector $\tilde{v}_+(x; \beta^+, \lambda)$. Thus for $x \geq 0$ and arbitrary $u$ we claim

\begin{align*}
P^c_+(x; \beta^+, \lambda)u &= \langle u, \tilde{w}_+(x; \beta^+, \lambda) \rangle \tilde{v}_+(x; \beta^+, \lambda) \\
&= e^{-\nu(\lambda)x} e^{\nu(\lambda) x }\langle u, w_+(x; \beta^+, \lambda) \rangle v_+(x; \beta^+, \lambda) \\
&= \langle u, w_+(x; \beta^+, \lambda) \rangle v_+(x; \beta^+, \lambda)
\end{align*}

The range of this is the span of $v_+(x; \beta^+, \lambda)$, which is what we want. To show that this is the projection onto the center range, all we need to do is verify that it is in fact a projection. To do this, we will show that $P^c_+(x; \beta^+, \lambda)P^c_+(x; \beta^+, \lambda) = P^c_+(x; \beta^+, \lambda)$.

\begin{align*}
P^c_+(x; \beta^+, \lambda)( P^c_+(x; \beta^+, \lambda) u ) &= \langle \langle u, w_+(x; \beta^+, \lambda) \rangle v_+(x; \beta^+, \lambda), w_+(x; \beta^+, \lambda) \rangle v_+(x; \beta^+, \lambda) \\
&= \langle u, w_+(x; \beta^+, \lambda) \rangle \langle v_+(x; \beta^+, \lambda), w_+(x; \beta^+, \lambda) \rangle v_+(x; \beta^+, \lambda) \\
&= \langle u, w_+(x; \beta^+, \lambda) \rangle v_+(x; \beta^+, \lambda) \\
&= P^c_+(x; \beta^+, \lambda) u 
\end{align*}

where $\langle v_+(x; \beta^+, \lambda), w_+(x; \beta^+, \lambda) \rangle = 1$ for all $x \geq 0$ by Lemma \ref{perpeigs}. Thus this is indeed a projection, and is in fact the projection we are looking for. From our definition of $P^c_+(x; \lambda)$ and the coordinate change we performed above, we have

\begin{align*}
\ker P^c_+(x; \beta^+, \lambda) &= \text{span }\{ w_+(x; \beta^+, \lambda) \}^\perp\\
\text{ran } P^c_+(x; \beta^+, \lambda) &= \text{span }\{ v_+(x; \beta^+, \lambda) \}
\end{align*}

If this were an orthogonal projection, that would mean that $(\ker P^c_+(x; \beta^+, \lambda))^\perp = \text{ran } P^c_+(x; \beta^+, \lambda)$, i.e. $\text{span }\{ w_+(x; \beta^+, \lambda) \}) = \text{span }\{ v_+(x; \beta^+, \lambda) \})$. This would mean that $v_+(x; \beta^+, \lambda)$ and $w_+(x; \beta^+, \lambda)$ are scalar multiples of each other. Since $\langle v_+(x; \beta^+, \lambda), w_+(x; \beta^+, \lambda) \rangle = 1$ for all $x \geq 0$, this implies $v_+(x; \beta^+, \lambda) = w_+(x; \beta^+, \lambda)$, which is likely not true. \\

We will also derive an expression for the center evolution $\Phi^c_+(x,y; \beta^+, \lambda)$ for $x \geq 0$. For arbirary $u$ and $x, y \geq 0$

\begin{align*}
\Phi^c_+(x,y; \beta^+, \lambda)u &= \Phi(x,y; \beta^+, \lambda) P^c_+(y; \beta^+, \lambda) u \\
&= \Phi(x,y; \beta^+, \lambda) \langle u, w_+(y; \beta^+, \lambda) \rangle v_+(y; \beta^+, \lambda) \\
&= \Phi(x,y; \beta^+, \lambda) \langle u, w_+(y; \beta^+, \lambda) \rangle e^{-\nu(\lambda)y} \tilde{v}_+(y; \beta^+, \lambda) \\
&= \langle u, w_+(y; \beta^+, \lambda) \rangle e^{-\nu(\lambda)y} \Phi(x,y; \beta^+, \lambda) \tilde{v}_+(y; \beta^+, \lambda) \\
&= \langle u, w_+(y; \beta^+, \lambda) \rangle e^{-\nu(\lambda)y} \tilde{v}_+(x; \beta^+, \lambda) \\
&= \langle u, w_+(y; \beta^+, \lambda) \rangle e^{-\nu(\lambda)y} e^{\nu(\lambda)x} v_+(x; \beta^+, \lambda) \\
&= e^{\nu(\lambda)(x-y)} v_+(x; \beta^+, \lambda) \langle u, w_+(y; \beta^+, \lambda) \rangle 
\end{align*}

where we used the fact that $\tilde{v}$ is a solution to the eigenvalue problem, thus under the evolution $\Phi(y, x; \beta^+, \lambda)$ we have $\Phi(y, x; \beta^+, \lambda)\tilde{v}_+(x; \beta^+, \lambda) = \tilde{v}_+(y; \beta^+, \lambda)$.\\

We can repeat we just did for the trichotomy on $\R^-$, which will give us analogous functions $\tilde{v}_-(x; \beta^-, \lambda)$, $v_-(x; \beta^-, \lambda)$, $\tilde{w}_-(x; \beta^-, \lambda)$, and $w_-(x; \beta^-, \lambda)$. We can do a similar change of variables to make $w_-(0; \beta^-, \lambda)$ perpendicular to $R^s_-(0; \beta^-, \lambda)$. Since we have two more dimensions to work with, this is always possible. The equations for the projection $P^c_-(x; \beta^-, \lambda)$ and the evolution $\Phi^c_-(x,y; \beta^-, \lambda)$ will be the same except for the subscripts. Note that these are all dependent on the IC $\beta^-$.
\\

For the stable and unstable parts of the trichotomy, we have the following bounds. (WE MIGHT HAVE TO SHOW THIS, BUT THESE SHOULD BE OBVIOUS. ALSO WE SHOULD CHECK THE SIGNS.)

\begin{align*}
\Phi^s_-(x, y; \beta^-, \lambda) &\leq C e^{-\alpha(y - x)} && y \geq x \geq 0\\
\Phi^u_-(x, y; \beta^-, \lambda) &\leq C e^{\alpha(y - x)} && x \geq y \geq 0\\
\Phi^s_+(x, y; \beta^-, \lambda) &\leq C e^{-\alpha(y - x)} && 0 \geq x \geq y\\
\Phi^u_+(x, y; \beta^-, \lambda) &\leq C e^{\alpha(y - x)} && 0 \geq y \geq x \\\\
\end{align*}


\subsection{The Inversion}

At this point, we write down the fixed point equations for the problem. To do that, we take a look at where the equations came from in the hyperbolic case. These are very similar to the variation of constants formula, except that we split the solution up into stable, unstable, and center parts, evolve them separately (each with its own initial condition), and recombine them. Thus the fixed point equations will look like those in San98 and the exponentially weighted space with the addition of a center evolution term together with an initial condition in the center subspace. The other difference will be the dependence on the ICs $\beta_i^\pm$. Note that these are fixed as they come from the construction of the periodic double pulse.
\\ 

The fixed point equations for our problem are thus

\begin{align*}
W_i^-(x) = \Phi^s_-(&x, -X_{i-1}; \beta_i^-, \lambda) a_{i-1}^- + \Phi^u_-(x, 0; \beta_i^-, \lambda) b_i^- \\
&+ e^{\nu(\lambda)(x+X_{i-1})} v_-(x; \beta_i^-, \lambda) \langle v_0(\lambda), w_-(-X_{i-1}; \beta_i^-, \lambda) \rangle c_{i-1}^- \\
&+ \int_0^x \Phi^u_-(x, y; \beta_i^-, \lambda)[ G_i^-(y)W_i^-(y) + \lambda^2 d_i \tilde{H}(y) ] dy \\
&+ \int_{-X_{i-1}}^x \Phi^s_-(x, y; \beta_i^-, \lambda) [ G_i^-(y)W_i^-(y) + \lambda^2 d_i \tilde{H}(y) ] dy \\
&+ \int_{-X_{i-1}}^x 
e^{\nu(\lambda)(x-y)} v_-(x; \beta_i^-, \lambda) \langle G_i^-(y)W_i^-(y) + \lambda^2 d_i \tilde{H}(y), w_-(y; \beta_i^-, \lambda) \rangle dy \\
W_i^+(x) = \Phi^u_+(&x, X_i; \beta_i^+, \lambda)a_i^+ + \Phi^s_+(x, 0; \beta_i^+, \lambda)b_i^+ \\
&+ e^{\nu(\lambda)(x - X_i)} v_+(x; \beta_i^+, \lambda) \langle v_0(\lambda), w_+(X_i; \beta_i^+, \lambda) \rangle c_i^+ \\
&+ \int_0^x \Phi^s_+(x, y; \beta_i^+, \lambda) [ G_i^+(y)W_i^+(y) + \lambda^2 d_i \tilde{H}(y) ] dy \\
&+ \int_{X_i}^x \Phi^u_+(x, y; \beta_i^+, \lambda) [ G_i^+(y)W_i^+(y) + \lambda^2 d_i \tilde{H}(y) ] dy \\
&+ \int_{X_i}^x e^{\nu(\lambda)(x-y)} v_+(x; \beta_i^+, \lambda) \langle G_i^+(y)W_i^+(y) + \lambda^2 d_i \tilde{H}(y), w_+(y; \beta_i^+, \lambda) \rangle dy
\end{align*}

where

\begin{align*}
(a^-, a^+) &\in E^s \oplus E^u\\
(b^-, b^+) &\in R^u_-(0; 0) \oplus R^s_+(0; 0)\\
c^\pm &\in \text{span}\{v_0(\lambda)\}
\end{align*}

Recall that the eigenspaces $E^s$, $E^u$, and $E^c$ refer to the unperturbed problem, i.e. they are the eigenspaces of $A(0)$. The projections onto $E^s$, $E^u$, and $E^c$ are given by $P_0^s$, $P_0^u$, and $P_0^c$. The initial conditions on the stable and unstable subspaces lie in the corresponding spaces for the unperturbed problem. By contrast, the initial conditions on the center subspace is in the eigenspace for the perturbed problem. We can do this (and it will prove to be very useful!) since we have an equation for the evolution along this center subspace.\\

We will take the following notational conventions, which we use for convenience. The first one makes sense since periodic boundary conditions means that things ``wrap around'', so can can consider the problem as posed on a ``loop''.

\begin{enumerate}
\item $c_2^- = c_0^-$, $a_2^- = a_0^-$, $b_0^- = b_2^-$, $d_0 = d_2$, and $W_0 = W_2$

\item If we eliminate either a subscript or a superscript (or both) in the norm, we are taking the maximum of the eliminated things. For example,
	\begin{enumerate}
		\item $|c_i| = \max(|c_i^+|, |c_i^-|)$ 
		\item $|c^+| = \max(|c_1^+|, |c_2^+|)$
		\item $|c| = \max(|c_1^-|, |c_1^+|, |c_2^-|, |c_2^+|)$
	\end{enumerate}
	This is always well defined, since the maximum is over a finite number of terms.
\item Let

\begin{align*}
\tilde{c}_i^- &= e^{\nu(\lambda)X_i} c_i^- \\
\tilde{c}_i^+ &= e^{-\nu(\lambda)X_i} c_i^+
\end{align*}

These will come in handy, as the the coefficients $c$ are often multiplied by an exponential factor.\\

\end{enumerate}

Before performing the inversion, we will define the following three useful constants.

\begin{enumerate}

	\item Let $\delta > 0$ be a small. How small will be determined later. Then choose $|\lambda|$ sufficiently small and $X_1$ sufficiently large such that

	\begin{equation}
	e^{-\alpha X_1}, ||G||, |\lambda|, |\Delta H| < \delta
	\end{equation}

	\item Fix a constant $\tilde{\alpha}$, with $0 < \tilde{\alpha} < \alpha$. The idea is that we want $\tilde{\alpha}$ to be close to $\alpha$. Then choose $X_1$ sufficiently large such that $e^{-\alpha X_1} < \delta$.

	\item Choose $\tilde{\delta} > 0$ such that $\tilde{\delta} < \alpha - \tilde{\alpha}$. Then choose $\delta$ sufficiently small such that $3 |\nu(\lambda)| < \tilde{\delta}$ for all $|\lambda| < \delta$. Thus we have $3 |\nu(\lambda)| < \alpha - \tilde{\alpha}$. Since $\nu(\lambda) = \mathcal{O}(\lambda)$, this can be accomplished. We also make sure $\tilde{\alpha}$ is sufficiently close to $\alpha$ and $\tilde{\delta}$ is sufficiently small so that $\tilde{\delta} < \tilde{\alpha}$.

\end{enumerate}


Now, as in San98 and the exponentially weighted case, we will perform the inversion of our problem in a series of lemmas. The first step is to solve for $W$ in terms of $(a, b, c, d)$.\\

First, we obtain a bound on the terms from the fixed point equations which involve $W$. 

% Inversion, lemma 1 (L1 bound)

\begin{lemma}\label{L1}

Let $L_1(\lambda): V_w \rightarrow V_w$ be the linear operator defined piecewise by

\begin{align*}
(L_1(\lambda)W)_i^-(x) &= \int_0^x \Phi^u_-(x, y; \beta_i^-, \lambda) G_i^-(y)W_i^-(y) dy + \int_{-X_{i-1}}^x \Phi^s_-(x, y; \beta_i^-, \lambda) G_i^-(y)W_i^-(y) dy \\
&+ \int_{-X_{i-1}}^x 
e^{\nu(\lambda)(x-y)} v_-(x; \beta_i^-, \lambda) \langle G_i^-(y)W_i^-(y), w_-(y; \beta_i^-, \lambda) \rangle dy \\
(L_1(\lambda)W)_i^+(x) &= \int_0^x \Phi^s_+(x, y; \beta_i^+, \lambda) G_i^+(y) W_i^+(y) dy + \int_{X_i}^x \Phi^u_+(x, y; \beta_i^+, \lambda) G_i^+(y) W_i^+(y) dy \\
&+ \int_{X_i}^x e^{\nu(\lambda)(x-y)} v_+(x; \beta_i^+, \lambda) \langle G_i^+(y)W_i^+(y), w_+(y; \beta_i^+, \lambda) \rangle dy
\end{align*}

Then $L_1(\lambda): V_w \rightarrow V_w$ is a bounded linear operator with bound

\begin{equation}\label{L1bound}
||L_1(\lambda)W|| \leq C (e^{\nu(\lambda)X_1} + e^{\nu(\lambda)X_2}) ||G|| \: ||W||
\end{equation}

\begin{proof}
The first two integrals on the RHS of $L_1$ have the same bound as in San98 and in the exponentially weighted case. For the third integral, the negative piece has bound

\begin{align*}
\Big| \int_{-X_{i-1}}^x &e^{\nu(\lambda)(x-y)} v_-(x; \beta_i^-, \lambda) \langle G_i^-(y)W_i^-(y), w_-(y; \beta_i^-, \lambda) \rangle dy \Big| \\
&\leq \int_{-X_{i-1}}^x e^{\nu(\lambda)(x-y)} |v_-(x; \beta_i^-, \lambda)| |G_i^-(y)|||W|||w_-(y; \beta_i^-, \lambda)|dy \\
&\leq C ||G||||W|| \int_{-X_{i-1}}^x e^{\nu(\lambda)(x-y)} dy \\
&= C ||G||||W|| \frac{e^{\nu(\lambda)x} - 1}{\nu(\lambda)} \\
&\leq C e^{\nu(\lambda)X_{i-1}} ||G|| \: ||W||
\end{align*}

where we used the fact that $x \leq 0$ on the negative piece. Since $v_-(x; 
\beta_i^-, \lambda)$ and $w_-(x; \beta_i^-, \lambda)$ are bounded and only depend on $\lambda$ (recall that we pulled out any exponential growth/decay in our expressions for $\tilde{v}$ and $\tilde{w}$), we incorporate those bounds into the constant $C$. The constant $C$ will depend on $\lambda$, but we suppress this dependence in the notation for simplicity. The positive piece has a similar bound with $X_{i-1}$ replaced with $X_i$. Thus we have the bound

\[
||L_1(\lambda)W|| \leq C (e^{\nu(\lambda)X_1} + e^{\nu(\lambda)X_2}) ||G|| \: ||W||
\]

\end{proof}
\end{lemma}

We would like a more useful bound than this for the following reason. Using the uniform bound $||G|$, the bound above will have a term of order $e^{\nu(\lambda)X_2} e^{-\alpha X_1}$. Suppose $\text{Re }\nu(\lambda) > 0$. If $X_2 > X_1$, $e^{\nu(\lambda)X_2}$ grows in $X_2$ and $e^{-\alpha X_1}$ decays in $X_1$. Thus, there is no telling what will happen as we vary $X_1$ and $X_2$ independently. One workaround is to make $X_1$ and $X_2$ depend on each other, e.g. $X_2 = k X_1$ for some $k \geq 0$. This does work (and we have shown it does!) but we do not want the two length parameters to be related, since we would like to send $X_2$ to $\infty$ without altering $X_1$.\\

For another (hopefully better!) way around this problem, we will use the piecewise bound on $|G_i^\pm(x)|$ from Theorem \ref{2pconstruction}.

% inversion lemma 1a - improved bound on L1

\begin{lemma}\label{L1better}

Let $L_1(\lambda): V_w \rightarrow V_w$ be the linear operator defined in the previous lemma. Then $L_1(\lambda): V_w \rightarrow V_w$ is a bounded linear operator with bound

\begin{equation}\label{L1bound2}
||L_1(\lambda)W|| \leq C ( e^{-(\alpha -|\nu(\lambda)|)X_1} + e^{-(\alpha -|\nu(\lambda)|)X_2})||W||
\end{equation}

Piecewise bounds are

\begin{align*}
||L_1(\lambda)_i^- W|| &\leq C e^{-(\alpha -|\nu(\lambda)|)X_{i-1}} ||W|| \\
||L_1(\lambda)_i^+ W|| &\leq C e^{-(\alpha -|\nu(\lambda)|)X_i} ||W||
\end{align*}

\begin{proof}
The first two integrals on the RHS of $L_1$ have the same bound as in San98 and in the exponentially weighted case. For the third integral, we will use the piecewise bound for $G$. For the ``negative'' piece (where $x \leq 0$), we have

\begin{align*}
\Big| \int_{-X_{i-1}}^x &e^{\nu(\lambda)(x-y)} v_-(x; \beta_i^-, \lambda) \langle G_i^-(y)W_i^-(y), w_-(y; \beta_i^-, \lambda) \rangle dy \Big| \\
&\leq C ||W|| \int_{-X_{i-1}}^x e^{\nu(\lambda)(x-y)} e^{-\alpha X_{i-1}}e^{-\alpha(X_{i-1} + y)}dy \\
&\leq C ||W|| \int_{-X_{i-1}}^x e^{|\nu(\lambda)| (x-y)} e^{-\alpha X_{i-1}}e^{-\alpha(X_{i-1} + y)}dy \\
&= C ||W|| e^{|\nu(\lambda)| x } e^{-2 \alpha X_{i-1}} \int_{-X_{i-1}}^x e^{-(\alpha + |\nu(\lambda)|) y} dy \\
&= C ||W|| e^{|\nu(\lambda)| x } e^{-2 \alpha X_{i-1}} \frac{1}{\alpha + \nu(\lambda)} \left( e^{-(\alpha + |\nu(\lambda)|)(-X_{i-1})} - e^{-(\alpha + |\nu(\lambda)|)x} \right) \\
&\leq C ||W|| e^{-2 \alpha X_{i-1}} \left( e^{(\alpha + |\nu(\lambda)|)X_{i-1}} + e^{-\alpha x}  \right) \\
&\leq C ||W|| e^{-2 \alpha X_{i-1}} \left( e^{\alpha X_{i-1}} e^{|\nu(\lambda)|X_{i-1}} + e^{\alpha X_{i-1}}  \right) \\
&\leq C ||W|| e^{-\alpha X_{i-1}} \left( e^{|\nu(\lambda)|X_{i-1}} + 1 \right) \\
&\leq C ||W|| e^{-(\alpha -|\nu(\lambda)|)X_{i-1}} \\
\end{align*}

This can be made arbitrarily small for sufficiently large $X_{i-1}$. Thus for this bound we have

\begin{align*}
||L_1(\lambda)_i^- W|| &\leq C e^{-(\alpha -|\nu(\lambda)|)X_{i-1}} ||W|| \\
||L_1(\lambda)_i^+ W|| &\leq C e^{-(\alpha -|\nu(\lambda)|)X_i} ||W||
\end{align*}

For a uniform bound, we have

\[
||L_1(\lambda)W|| \leq 
C (e^{-(\alpha -|\nu(\lambda)|)X_1} + e^{-(\alpha -|\nu(\lambda)|)X_2})||W||
\]

\end{proof}
\end{lemma}

Next, we obtain a bound on the terms from the fixed point equations which do not involve $W$. 

% inversion, lemma 2 (L2 bound)

\begin{lemma}\label{L2}

Let $L_2(\lambda): V_a \times V_b \times V_c \times V_d \rightarrow V_w$ be the linear operator defined piecewise by

\begin{align*}
L_2(\lambda)&(a,b,c,d)_i^-(x) = \Phi^s_-(x, -X_{i-1}; \beta_i^-, \lambda)a_{i-1}^- + \Phi^u_-(x, 0; \beta_i^-, \lambda)b_i^- \\
&+ e^{\nu(\lambda)(x+X_{i-1})} v_-(x; \beta_i^-, \lambda) \langle v_0(\lambda), w_-(-X_{i-1}; \beta_i^-, \lambda) \rangle c_{i-1}^- \\
&+ \int_0^x \Phi^u_-(x, y; \beta_i^-, \lambda)\lambda^2 d_i \tilde{H}(y) dy + \int_{-X_{i-1}}^x \Phi^s_-(x, y; \beta_i^-, \lambda) \lambda^2 d_i \tilde{H}(y) dy \\
&+ \int_{-X_{i-1}}^x
e^{\nu(\lambda)(x-y)} v_-(x; \beta_i^-, \lambda) \langle \lambda^2 d_i \tilde{H}(y), w_-(y; \beta_i^-, \lambda) \rangle dy \\
L_2(\lambda)&(a,b,c,d)_i^+(x) = \Phi^u_+(x, X_i; \beta_i^+, \lambda)a_i^+ + \Phi^s_+(x, 0; \beta_i^+, \lambda)b_i^+ \\
&+ e^{\nu(\lambda)(x - X_i)} v_+(x; \beta_i^+, \lambda) \langle v_0(\lambda), w_+(X_i; \beta_i^+, \lambda) \rangle c_i^+ \\
&+ \int_0^x \Phi^s_+(x, y; \beta_i^+, \lambda) \lambda^2 d_i \tilde{H}(y) dy + \int_{X_i}^x \Phi^u_+(x, y; \beta_i^+, \lambda) \lambda^2 d_i \tilde{H}(y) dy \\
&+ \int_{X_i}^x e^{\nu(\lambda)(x-y)} v_+(x; \beta_i^+, \lambda) \langle \lambda^2 d_i \tilde{H}(y), w_+(y; \beta_i^+, \lambda) \rangle dy
\end{align*}

Then $L_2$ is a bounded linear operator with bound

\begin{equation}\label{L2bound}
|L_2(\lambda)(a,b,c,d)| \leq C (|a| + |b| + e^{|\nu(\lambda)|X_i}|c_i| + e^{|\nu(\lambda)|X_{i-1}}|c_{i-1}| + |\lambda|^2 |d| )
\end{equation}

We also have the following piecewise, $x$-dependent bounds on $L_2$.

\begin{align*}
|L_2(\lambda)(a,b,c,d)_i^-(x)| &\leq C (e^{-\alpha(x + X_{i-1})}|a_{i-1}^-| + |b| + e^{\nu(\lambda)x} |\tilde{c}_{i-1}^-| + |\lambda|^2 |d| ) \\
|L_2(\lambda)(a,b,c,d)_i^+(x)| &\leq C (e^{-\alpha(x - X_i)}|a_i^+| + |b| + e^{\nu(\lambda)x} |\tilde{c}_i^+| + |\lambda|^2 |d| ) 
\end{align*}

where the $\tilde{c}$ are defined above.

\begin{proof}
First, we note that $L_2$ comprises the terms from the fixed point equations which do not involve $W$. Most of the bounds on the individual terms are the same as in San98 and the exponentially weighted case. We will look at the ``minus'' piece. The ``plus'' piece will be similar. First, for the $c_{i-1}^-$ term we have

\[
e^{\nu(\lambda)(x+X_{i-1})} v_-(x; \beta_i^-, \lambda) \langle v_0(\lambda), w_-(-X_{i-1}; \beta_i^-, \lambda) \rangle c_{i-1}^- \leq C e^{|\nu(\lambda)| X_{i-1} }|c_{i-1}^-|
\]

The first two integral terms are similar to San98. For the third (center) integral terms, we use the following trick involving $\tilde{\alpha}$ to eliminate any potential exponential growth.

\begin{align*}
&\left| \int_{-X_{i-1}}^x 
e^{\nu(\lambda)(x-y)} v_-(x; \beta_i^-, , \lambda) \langle \lambda^2 d_i \tilde{H}(y), w_-(y; \beta_i^-, \lambda) \rangle dy \right| \\
&\leq C |\lambda|^2 |d| e^{\tilde{\alpha}x} \int_{-X_{i-1}}^x e^{-\tilde{\alpha}x} e^{\tilde{\alpha}y} e^{\nu(\lambda)(x-y)} |e^{-\tilde{\alpha}y}\tilde{H}(y)|dy \\
&\leq C |\lambda|^2 |d| e^{\tilde{\alpha}x} \int_{-X_{i-1}}^x e^{-\tilde{\alpha}(x-y)} e^{\nu(\lambda)(x-y)} |e^{-\tilde{\alpha}y}\tilde{H}(y)| dy \\
&\leq C |\lambda|^2 |d| e^{\tilde{\alpha}x} \int_{-X_{i-1}}^x e^{-(\tilde{\alpha} - |\nu(\lambda)|)(x-y)} dy \\
&\leq C |\lambda|^2 |d|
\end{align*}

where we use the facts that $|e^{-\tilde{\alpha}y}\tilde{H}(y)|$ is bounded (since the decay rate of $\tilde{H}$ is known) and that $|\nu(\lambda)| < \tilde{\alpha}$ by our choice of $\tilde{\alpha}$. The other center integral is similar. Thus, we attain the bound 

\begin{align*}
|L_2(\lambda)(a,b,c,d)_i^-(x)| &\leq C (e^{-\alpha(X_{i-1} + x)}|a_{i-1}^-| + |b| + e^{\nu(\lambda)(X_{i-1} + x)} |c_{i-1}^-| + |\lambda|^2 |d| ) \\
&= C (e^{-\alpha(X_{i-1} + x)}|a_{i-1}^-| + |b| + e^{\nu(\lambda)x} |\tilde{c}_{i-1}^-| + |\lambda|^2 |d| ) 
\end{align*}

For the ``positive'' piece,

\begin{align*}
|L_2(\lambda)(a,b,c,d)_i^+(x)| &\leq C (e^{-\alpha(X_i - x)}|a_i^+| + |b| + e^{\nu(\lambda)(X_i - x)} |c_i^+| + |\lambda|^2 |d| ) \\
&\leq C (e^{-\alpha(X_i - x)}|a_i^+| + |b| + e^{\nu(\lambda)x} |\tilde{c}_i^+| + |\lambda|^2 |d| ) 
\end{align*}

The overall, uniform bound is

\[
|L_2(\lambda)(a,b,c,d)| \leq C (|a| + |b| + e^{|\nu(\lambda)|X_i}|c_i| + e^{|\nu(\lambda)|X_{i-1}}|c_{i-1}| + |\lambda|^2 |d| )
\]

\end{proof}
\end{lemma}

Now we can perform the inversion

% inversion lemma 3 - invert to solve for W

\begin{lemma}\label{W1}
There exists a bounded linear operator $W_1: V_\lambda \times V_a \times V_b \times V_c \times V_d \rightarrow V_w$ such that 

\[
W = W_1(\lambda)(a,b,c,d)
\]

This operator is analytic in $\lambda$ and linear in $(a, b, c, d)$. The operator $W_1$ satisfies the bound

\begin{equation}\label{W1bound}
||W_1(\lambda)(a,b,c,d)|| \leq C ( |a| + |b| + e^{|\nu(\lambda)|X_i}|c_i| + e^{|\nu(\lambda)|X_{i-1}}|c_{i-1}| + |\lambda|^2 |d| )
\end{equation}

We also have the piecewise bounds

\begin{align*}
||W_1(\lambda)(a,b,c,d)_i^-|| &\leq C ( |a_{i-1}^-| + |b| + e^{|\nu(\lambda)|X_{i-1}}|c_{i-1}^-| + |\lambda|^2 |d| ) \\
||W_1(\lambda)(a,b,c,d)_i^+|| &\leq C ( |a_i^+| + |b| + e^{|\nu(\lambda)|X_i}|c_i^+| + |\lambda|^2 |d| )
\end{align*}

\begin{proof}
Let the linear operators $L_1$ and $L_2$ be defined as in the previous two lemmas. Then we can rewrite the fixed point equation as

\[
(I - L_1(\lambda))W = L_2(\lambda)(a,b,c,d)
\]

For $L_1$ we have the estimate from Lemma \ref{L1better}

\begin{align*}
||L_1(\lambda)W|| &\leq C e^{-(\alpha -|\nu(\lambda)|)X_1}||W|| \\
&\leq C e^{-\tilde{\alpha} X_1}||W|| \\
&\leq C \delta ||W||
\end{align*}

Thus if we choose $\delta$ sufficiently small (i.e. smaller than $C$), the operator norm of $L_1$ is less than 1, which implies that the operator $(I - L_1(\lambda))$ is invertible. The inverse $(I - L_1(\lambda))^{-1}$ is analytic in $\lambda$ and has operator norm 

\[
||(I - L_1(\lambda))^{-1}|| \leq \frac{1}{1 - ||L_1||}
\]

We can then write $W$ as
\[
W = W_1(\lambda)(a,b,c,d) = (I - L_1(\lambda))^{-1} L_2(\lambda)(a,b,c,d)
\]

which depends linearly on $(a,b,c,d)$ and analytically on $\lambda$. Since the operator norm of $L_1$ is bounded by a constant (independent of the $X_i$), we have the uniform bound

\[
||W_1(\lambda)(a,b,c,d)|| \leq C (|a| + |b| + e^{|\nu(\lambda)|X_i}|c_i| + e^{|\nu(\lambda)|X_{i-1}}|c_{i-1}| + |\lambda|^2 |d| )
\]

The piecewise bounds are obtained by noting which piece of $L_2$ is involved with which piece of $W$.\\

We can obtain a piecewise, $x$-dependent bound for $W$ by substituting the $W_1$ bound into the expression $W = L_1 + L_2$ and using the bounds for $L_1$ and $L_2$ from the previous lemmas.

\begin{align*}
|W_i^-(x)| &\leq ||(L_1(\lambda)W)_i^-||\:||W_i^-|| + |L_2(\lambda)(a,b,c,d)_i^-(x)| \\
&\leq C \Big( e^{-(\alpha -|\nu(\lambda)|)X_{i-1}} ||W_i^-|| + e^{-\alpha(x + X_{i-1})}|a_{i-1}^-| + |b| + e^{\nu(\lambda)x} |\tilde{c}_{i-1}^-| + |\lambda|^2 |d| \Big) \\
&\leq \Big( e^{-(\alpha -|\nu(\lambda)|)X_{i-1}} (|a| + |b| + e^{\nu(\lambda)X_{i-1}}|c_{i-1}| + |\lambda|^2 |d|) \\
&+ e^{-\alpha(x + X_{i-1})}|a_{i-1}^-| + |b| + e^{\nu(\lambda)x} |\tilde{c}_{i-1}^-| + |\lambda|^2 |d| \Big) \\
&\leq \Big( e^{-(\alpha -|\nu(\lambda)|)X_{i-1}} |a| + e^{-\alpha(x + X_{i-1})}|a_{i-1}^-| + |b| + e^{-(\alpha - 2|\nu(\lambda)|)X_{i-1}} |c_{i-1}| + e^{\nu(\lambda)x} |\tilde{c}_{i-1}^-| + |\lambda|^2 |d| \Big)
\end{align*}

Similarly, 

\begin{align*}
|W_i^+(x)| &\leq \Big( e^{-(\alpha -|\nu(\lambda)|)X_i} |a| + e^{-\alpha(x - X_i)}|a_{i-1}^-| + |b| + e^{-(\alpha - 2|\nu(\lambda)|)X_{i-1}} |c_{i-1}| + e^{\nu(\lambda)x} |\tilde{c}_{i-1}^-| + |\lambda|^2 |d| \Big)
\end{align*}

\end{proof}
\end{lemma}

The next inversion step solves for the joins at $\pm X_i$, i.e. solves the equations

\begin{align*}
W_2^+(X_2) - W_1^-(-X_2) &= D_2 d \\
W_1^+(X_1) - W_2^-(-X_1) &= D_1 d \\
\end{align*}

For $i = 1, 2$, recalling our ``wrap-around'' notation, we can write this as

\begin{align*}
W_i^+(X_i) - W_{i-1}^-(-X_i) &= D_i d \\
\end{align*}

The next lemma performs this inversion.

% second inversion lemma

\begin{lemma}\label{inv2}
There exist operators

\begin{align*}
A_1: V_\lambda \times V_b \times V_c \times V_d \rightarrow V_a \\
W_2: V_\lambda \times V_b \times V_c \times V_d \rightarrow V_w \\
\end{align*}

such that $(a,W) = ( A_1(\lambda)(b,c,d), W_2(\lambda)(b,c,d) )$ solves our system. These operators are analytic in $\lambda$, linear in $(b,c,d)$, and bounds for them are given by

\begin{equation}
|A_1(\lambda)_i(b, c, d)| \leq C \Big( (e^{-\alpha X_i} + ||G||) |b| + ( p_4(X_i; \lambda) + e^{-(\alpha - |\nu(\lambda)|)X_i} )|c_i|
+ (e^{-\tilde{\alpha} X_i} + ||G||) |\lambda^2| |d| + |D_i||d| \Big)
\end{equation} 

and

\begin{equation}
|W_2(\lambda)(b,c,d)|| 
\leq C \Big( |b| + e^{|\nu(\lambda)|X_i}|c_i| + e^{|\nu(\lambda)|X_{i-1}}|c_{i-1}| + (|\lambda|^2 + |D|)|d| \Big)
\end{equation} 

Furthermore, 

\begin{align*}
a_i^+ &= P^u_0 D_i d + A_2(\lambda)_i^+(b, c, d) \\
a_i^- &= -P^s_0 D_i d + A_2(\lambda)_i^-(b, c, d)
\end{align*}

where $A_2$ is a bounded operator with estimate

\begin{align*}
|A_2&(\lambda)_i(b, c, d)| \\
&\leq C \Big( (e^{-\alpha X_i} + ||G||)|b| + ( p_4(X_i; \lambda) + e^{-(\alpha - |\nu(\lambda)|)X_i} )|c_i| + (e^{-\tilde{\alpha} X_i} + ||G||) |\lambda^2| |d| + (p_1(X_i; \lambda) + ||G|| )|D_i||d|) \Big)
\end{align*}

$(p_1(X_i; \lambda)$ and $p_4(X_i; \lambda)$ are defined in the proof below. 

\begin{proof}

Using the fixed point equations at $\pm X_i$, we have

\begin{align*}
W_i^+(X_i) &- W_{i-1}^-(-X_i) = P^u_+(X_i; \beta_i^+, \lambda) a_i^+ - P^s_-(-X_i; \beta_{i-1}^-, \lambda) a_i^- \\
&+ \Phi^s_+(X_i, 0; \beta_i^+, \lambda)b_i^+ - \Phi^u_-(-X_i, 0; \beta_{i-1}^-, \lambda)b_{i-1}^- \\
&+ v_+(X_i; \beta_i^+, \lambda) \langle v_0(\lambda), w_+(X_i; \beta_i^+, \lambda) \rangle c_i^+ - v_-(-X_i; \beta_{i-1}^-, \lambda) \langle v_0(\lambda), w_-(-X_i; \beta_{i-1}^-, \lambda) \rangle c_i^- \\
&+ \int_0^{X_i} \Phi^s_+(X_i, y; \beta_i^+, \lambda) [ G_i^+(y) W_i^+(y) + d_i \lambda^2 \tilde{H}(y) ] dy \\
&- \int_0^{-X_i} \Phi^u_-(-X_i, y; \beta_{i-1}^-, \lambda) [ G_{i-1}^-(y) W_{i-1}^-(y) + d_{i-1} \lambda^2 \tilde{H}(y) ] dy
\end{align*}

We now manipulate this to get it into a form we can use. First, we get the coefficients $a_i^\pm$ by themselves by adding and subtracting $P_0^u a_i^+$ and $P_0^s a_i^-$. Recalling where the various $a_i^\pm$ live and what happens when we apply the projections on $E^u$ and $E^s$, this becomes

\begin{align*}
D_i d &= a_i^+ - a_i^- \\
&+ (P^u_+(X_i; \beta_i^+, \lambda) - P_0^u)a_i^+ - (P^s_-(-X_i; \beta_{i-1}^-, \lambda) - P_0^s)a_i^- \\
&+ \Phi^s_+(X_i, 0; \beta_i^+, \lambda)b_i^+ - \Phi^u_-(-X_i, 0; \beta_{i-1}^-, \lambda)b_{i-1}^- \\
&+ v_+(X_i; \beta_i^+, \lambda) \langle v_0(\lambda), w_+(X_i; \beta_i^+, \lambda) \rangle c_i^+ - v_-(-X_i; \beta_{i-1}^-, \lambda) \langle v_0(\lambda), w_-(-X_i; \beta_{i-1}^-, \lambda) \rangle c_i^- \\
&+ \int_0^{X_i} \Phi^s_+(X_i, y; \beta_i^+, \lambda) [ G_i^+(y) W_i^+(y) + d_i \lambda^2 \tilde{H}(y) ] dy \\
&- \int_0^{-X_i} \Phi^u_-(-X_i, y; \beta_{i-1}^-, \lambda) [ G_{i-1}^-(y) W_{i-1}^-(y) + d_{i-1} \lambda^2 \tilde{H}(y) ] dy
\end{align*}

For a bound on the ``projection difference'', let

\[
p_1(X; \beta^+, \beta^-, \lambda) = \sup_{x \geq X} (|P^u_+(x; \beta^+, \lambda) - P_0^u| + |P^s_-(-x; \beta^-, \lambda) - P_0^s|)
\]

Note that this varies with $\beta^\pm$, $\lambda$ and with $X$. Based on San98, the bounds for $\beta_i^\pm$ from Theorem \ref{2pconstruction}, and intuition, we hypothesize the following bound. (WE SHOULD ACTUALLY SHOW THIS.)

\begin{hypothesis}\label{p1bound}
\begin{equation}
p_1(X; \lambda) = \mathcal{O}( e^{-\alpha X} +  e^{-2 \alpha X_1} + e^{-2 \alpha X_2} + |\lambda| )
\end{equation}
\end{hypothesis}

As long as $X$ is greater than the minimum of $X_1, X_2$, we have

\begin{equation}
p_1(X; \lambda) = \mathcal{O}( e^{-\alpha X_1} + e^{-\alpha X_2} + |\lambda| )
\end{equation}

Next we rearrange the above to get $a_i^- - a_i^+$ by itself on the LHS.

\begin{align*}
a_i^- - a_i^+ &= -D_i d  \\
&+ (P^u_+(X_i; \beta_i^+, \lambda) - P_0^u)a_i^+ - (P^s_-(-X_i; \beta_{i-1}^-,\lambda) - P_0^s)a_i^- \\
&+ \Phi^s_+(X_i, 0; \beta_i^+, \lambda)b_i^+ - \Phi^u_-(-X_i, 0; \beta_{i-1}^-, \lambda) b_{i-1}^- \\
&+ v_+(X_i; \beta_i^+, \lambda) \langle v_0(\lambda), w_+(X_i; \beta_i^+, \lambda) \rangle c_i^+ - v_-(-X_i; \beta_{i-1}^-, \lambda) \langle v_0(\lambda), w_-(-X_i; \beta_{i-1}^-, \lambda) \rangle c_i^- \\
&+ \int_0^{X_i} \Phi^s_+(X_i, y; \beta_i^+, \lambda) [ G_i^+(y) W_i^+(y) + d_i \lambda^2 \tilde{H}(y) ] dy \\
&- \int_0^{-X_i} \Phi^u_-(-X_i, y; \beta_{i-1}^-, \lambda) [ G_{i-1}^-(y) W_{i-1}^-(y) + d_{i-1} \lambda^2 \tilde{H}(y) ] dy
\end{align*}

Since $a_i^- - a_i^+ \in E^+ \oplus E^-$, the RHS must lie in $E^+ \oplus E^-$ as well. Since $\C^5 = E^+ \oplus E^- \oplus E^0$, the RHS must have no component in the (unperturbed) center space. The vectors $v_\pm(\pm X_i; \beta_{i}^\pm,\lambda)$ are close to $v_0(\lambda)$, which is close to $v_0(0) \in E^0$. Define

\begin{align*}
\Delta v_\pm(X; \beta^\pm, \lambda) &= v_\pm(\pm X; \beta^\pm, \lambda) - v_0(\lambda) \\
\Delta w_\pm(X; \beta^\pm, \lambda) &= w_\pm(\pm X; \beta^\pm, \lambda) - w_0(\lambda)
\end{align*}

Then let

\begin{align*}
p_2(X; \lambda) &= |\Delta v_\pm(X; \beta^\pm, \lambda)| + |\Delta w_\pm(X; \beta^\pm, \lambda)|\\
&= |v_\pm(\pm X; \beta^\pm, \lambda) - v_0(\lambda)| + |w_\pm(\pm X; \beta^\pm, \lambda) - w_0(\lambda)|
\end{align*}

and

\begin{equation}\label{p3}
p_3(X; \lambda) = |v_0(\lambda) - v_0(0)| 
\end{equation}

Since $v_\pm(\pm x; \beta^\pm, \lambda) \rightarrow v_0(\lambda)$ and $w_\pm(\pm x; \beta^\pm, \lambda) \rightarrow w_0(\lambda)$ as $|x| \rightarrow \infty$, $p_2(X; \lambda) \rightarrow 0$ as $|x| \rightarrow \infty$, but we do not have a rate of convergence for it, thus we do not have a bound. Fortunately, this will not matter.\\

For $p_3$, we can expand $v_0(\lambda)$ in a Taylor series in $\lambda$ to get

\[
v_0(\lambda) = v_0(0) + \lambda \frac{\partial}{\partial \lambda}v_0(\lambda)\Big|_{\lambda = 0} + \mathcal{O}(\lambda^2)
\]

Since $\frac{\partial}{\partial \lambda}v_0(\lambda)\Big|_{\lambda = 0}$ is a constant, we have

\[
p_3(X; \lambda) = \mathcal{O}(\lambda) 
\]

To eliminate most of the component of $v_\pm(\pm X_i; \beta^\pm, \lambda)$ and use these bounds, we write $a_i^- - a_i^+$ as

\begin{align*}
a_i^- &- a_i^+ = -D_i d + (P^u_+(X_i; \beta_i^+, \lambda) - P_0^u)a_i^+ - (P^s_-(-X_i; \beta_{i-1}^-, \lambda) - P_0^s)a_i^- \\
&+ v_0(0) \langle v_0(\lambda), w_+(X_i; \beta_i^+, \lambda) \rangle c_i^+ 
- v_0(0) \langle v_0(\lambda), w_-(-X_i; \beta_{i-1}^-, \lambda) \rangle c_i^- \\
&+ \Phi^s_+(X_i, 0; \beta_i^+, \lambda)b_i^+ - \Phi^u_-(-X_i, 0; \beta_{i-1}^-, \lambda)b_{i-1}^- \\
&+ (v_0(0) - v_+(X_i; \beta_i^+, \lambda)) \langle v_0(\lambda), w_+(X_i; \beta_i^+, \lambda) \rangle c_i^+ \\
&- (v_0(0) - v_-(-X_i; \beta_{i-1}^-, \lambda)) \langle v_0(\lambda), w_-(-X_i; \beta_{i-1}^-, \lambda) \rangle c_i^- \\
&+ \int_0^{X_i} \Phi^s_+(X_i, y; \beta_i^+, \lambda) [ G_i^+(y) W_i^+(y) + d_i \lambda^2 \tilde{H}(y) ] dy \\
&- \int_0^{-X_i} \Phi^u_-(-X_i, y; \beta_{i-1}^-, \lambda) [ G_{i-1}^-(y) W_{i-1}^-(y) + d_{i-1} \lambda^2 \tilde{H}(y) ] dy
\end{align*}

Let $P_0^\pm$ be the projection on $E^s \oplus E^u$. When we apply the projection, the $v_0(0)$ terms disappear, which leaves us with

\begin{align*}
a_i^- &- a_i^+ = P_0^\pm \Big(-D_i d \\
&+(v_+(X_i; \beta_i^+, \lambda) - v_0(0)) \langle v_0(\lambda), w_+(X_i; \beta_i^+, \lambda) \rangle c_i^+ \\
&- (v_-(-X_i; \beta_{i-1}^-,\lambda) - v_0(0)) \langle v_0(\lambda), w_-(-X_i; \beta_{i-1}^-, \lambda) \rangle c_i^- \\
&+ (P^u_+(X_i; \beta_i^+, \lambda) - P_0^u)a_i^+ - (P^s_-(-X_i; \beta_{i-1}^-, \lambda) - P_0^s)a_i^- \\
&+ \Phi^s_+(X_i, 0; \beta_i^+, \lambda)b_i^+ - \Phi^u_-(-X_i, 0; \beta_{i-1}^-, \lambda)b_{i-1}^- \\
&+ \int_0^{X_i} \Phi^s_+(X_i, y; \beta_i^+, \lambda) [ G_i^+(y) W_i^+(y) + d_i \lambda^2 \tilde{H}(y) ] dy \\
&- \int_0^{-X_i} \Phi^u_-(-X_i, y; \beta_{i-1}^-, \lambda) [ G_{i-1}^-(y) W_{i-1}^-(y) + d_{i-1} \lambda^2 \tilde{H}(y) ] dy \Big)
\end{align*}

Then we have

\begin{align*}
a_i^- - a_i^+ = -P_0^\pm D_i d + L_3(\lambda)_i(a, b, c, d)
\end{align*}

Where $L_3(\lambda)(a, b, c, d)_i$ is the rest of the terms on the RHS.

\begin{align*}
L_3(\lambda)&(a, b, c, d)_i = P_0^\pm \Big( \\
&(v_+(X_i; \beta_i^+, \lambda) - v_0(0)) \langle v_0(\lambda), w_+(X_i; \beta_i^+, \lambda) \rangle c_i^+ \\
&- (v_-(-X_i; \beta_{i-1}^-, \lambda) - v_0(0)) \langle v_0(\lambda), w_-(-X_i; \beta_{i-1}^-, \lambda) \rangle c_i^- \\
&+ (P^u_+(X_i; \beta_i^+, \lambda) - P_0^u)a_i^+ - (P^s_-(-X_i; \beta_{i-1}^-, \lambda) - P_0^s)a_i^- \\
&+ \Phi^s_+(X_i, 0; \beta_i^+, \lambda)b_i^+ - \Phi^u_-(-X_i, 0; \beta_{i-1}^-, \lambda)b_{i-1}^- \\
&+ \int_0^{X_i} \Phi^s_+(X_i, y; \beta_i^+, \lambda) [ G_i^+(y) W_i^+(y) + d_i \lambda^2 \tilde{H}(y) ] dy \\
&- \int_0^{-X_i} \Phi^u_-(-X_i, y; \beta_{i-1}^-, \lambda) [ G_{i-1}^-(y) W_{i-1}^-(y) + d_{i-1} \lambda^2 \tilde{H}(y) ] dy \Big)
\end{align*}

For the bound on $L_3$, we have seen most of the terms before. For the bound on the integral term involving $\tilde{H}$, we use the $\tilde{\alpha}$ trick to get a better bound. For the ``positive'' piece, this bound is

\begin{align*}
\left| \int_0^{X_i} \Phi^s_+(X_i, y; \beta_i^+, \lambda) \tilde{H}(y) dy \right| 
&\leq C \int_0^{X_i} e^{-\alpha (X_i - y)}|\tilde{H}(y)| dy \\
&= C e^{-\tilde{\alpha}X_i} \int_0^{X_i} e^{-\alpha X_i} e^{\alpha y}  e^{\tilde{\alpha}X_i} e^{-\tilde{\alpha}y} |e^{\tilde{\alpha}y} \tilde{H}(y)| \\
&= C e^{-\tilde{\alpha}X_i} \int_0^{X_i} e^{-(\alpha - \tilde{\alpha})(X_i-y)} |e^{\tilde{\alpha}y} \tilde{H}(y)|\\
&\leq C e^{-\tilde{\alpha}X_i} 
\end{align*}

where we again used the fact that $|e^{\tilde{\alpha}y} \tilde{H}(y)|$ is bounded, since $\tilde{\alpha} < \alpha$ and $\tilde{H}(y)$ decays with rate $\alpha$. The ``negative'' piece is similar.\\

For the integral term involving $W$, we use the piecewise bound on $W$ from Lemma \ref{W1bound}. For the ``positive'' piece, we have

\begin{align*}
&\left| \int_0^{X_i} \Phi^s_+(X_i, y; \beta_i^+, \lambda) G_i^+(y) W_i^+(y) \right| \\
&\leq C \int_0^{X_i} e^{-\alpha(X_i - y)}|G_i^+(y)|(|a| + |b| + e^{\nu(\lambda)X_i}|c_i^+| + |\lambda|^2 |d| ) dy \\
&\leq C \left( ||G||(|a| + |b| + |\lambda|^2 |d|) + \int_0^{X_i} e^{-\alpha(X_i - y)}|G_i^+(y)|e^{\nu(\lambda)X_i}|c_i^+| dy \right) \\
&\leq C \left( ||G||(|a| + |b| + |\lambda|^2 |d|) + \int_0^{X_i} e^{-\alpha(X_i - y)}e^{-\alpha X_i} e^{-\alpha(X_i - y)} e^{\nu(\lambda)X_i}|c_i^+| dy \right)\\
&\leq C \left( ||G||(|a| + |b| + |\lambda|^2 |d|) + e^{-(\alpha - |\nu(\lambda)|)X_i} |c_i^+| \int_0^{X_i} e^{-2\alpha(X_i - y)} dy \right) \\
&\leq C \left( ||G||(|a| + |b| + |\lambda|^2 |d|) + e^{-(\alpha - |\nu(\lambda)|)X_i} |c_i^+| \right)
\end{align*}

where we used the fact that $|\nu(\lambda)| \leq \alpha$.  

The ``negative'' piece is similar. This works out the way we want, since the ``negative'' piece involves $W_{i-1}$, but the bound for $W_{i-1}$ involves $c_i^-$.\\

Thus we have the following bound for $L_3$.

\begin{align*}
L_3(\lambda)&(a, b, c, d)_i \leq C ( p_1(X_i; \lambda)|a_i|
+ e^{-\alpha X_i}|b| + p_4(X_i; \lambda)|c_i| \\
&+ ||G||(|a| + |b| + |\lambda|^2 |d|) + e^{-(\alpha - |\nu(\lambda)|)X_i} |c_i^+| + e^{-\tilde{\alpha} X_i} |\lambda^2| |d| )
\end{align*}

where

\[
p_4(X_i; \lambda) = p_2(X_i; \lambda) + p_3(X_i; \lambda)
\]

Combining terms and simplifying, the final bound for $L_3$ is

\begin{align*}
L_3(\lambda)&(a, b, c, d)_i \leq C \Big( (p_1(X_i; \lambda) + ||G|| )|a_i|
+ (e^{-\alpha X_i} + ||G||)|b| + ( p_4(X_i; \lambda) + e^{-(\alpha - |\nu(\lambda)|)X_i} )|c_i| \\
&+ (e^{-\tilde{\alpha} X_i} + ||G||) |\lambda^2| |d| \Big)
\end{align*}

We use Hypothesis \ref{p1bound} for a bound on $p_1(X_i; \lambda)$. For $p_2(X_i; \lambda)$, we take $X_1$ sufficiently large so that $p_2(X_1; \lambda) < \delta$. $p_3(X_i; \lambda)$ is order $|\lambda|$. Thus we have

\begin{align*}
L_3(\lambda)&(a, b, c, d)_i \leq C \delta (|a_i| + |b| + |c_i| + |d|)
\end{align*}

Let $J_1: V_a \rightarrow \C^4$ be defined by $J_i(a_i) = (a_i^+ - a_i^-)$. The map $J_i$ is a linear isomorphism since $E^s \oplus E^u = \C^4$. Now consider the map

\[
S_i(a_i) = J_i (a_i) + L_3(\lambda)_i(a_i, 0, 0, 0) = J_i( I + J_i^{-1} L_3(\lambda)_i(a_i, 0, 0, 0))
\]

For sufficiently small $\delta$ we can get the operator norm 

\[
||J_i^{-1} L_3(\lambda)_i(\cdot, 0, 0, 0)|| < 1
\]

thus the map $a_i \rightarrow I + J_1^{-1} L_3(\lambda)_i(a_i, 0, 0, 0)$ is invertible and so the operator $S_i$ is invertible.\\

We can solve for $a$ to get

\[
a_i = A_1(\lambda)_i(b, c, d) = S_i^{-1}(-D_i d - L_3(\lambda)_i(0, b, c, d))
\]

Using the bound on $L_3$ together with $|D_i|$, $A_1$ will have bound

\begin{align*}
|A_1&(\lambda)_i(b, c, d)| \\
&\leq C \Big( (e^{-\alpha X_i} + ||G||) |b| + ( p_4(X_i; \lambda) + e^{-(\alpha - |\nu(\lambda)|)X_i} )|c_i|
+ (e^{-\tilde{\alpha} X_i} + ||G||) |\lambda^2| |d| + |D_i||d| \Big)
\end{align*} 

We can plug this into our expression for $W_1$ to get $W_2(\lambda)$, which has uniform bound

\begin{align*}
|W_2&(\lambda)(b,c,d)|| 
\leq C \Big( |b| + e^{|\nu(\lambda)|X_i}|c_i| + e^{|\nu(\lambda)|X_{i-1}}|c_{i-1}| + (|\lambda|^2 + |D|)|d| \Big)
\end{align*}

The piecewise bounds are

\begin{align*}
|W_2(\lambda)(b,c,d)_i^-|| 
&\leq C \Big( |b| + e^{|\nu(\lambda)|X_{i-1}}|c_{i-1}| + (|\lambda|^2 + |D_{i-1}|)|d| \Big) \\
|W_2(\lambda)(b,c,d)_i^+|| 
&\leq C \Big( |b| + e^{|\nu(\lambda)|X_i}|c_i| + (|\lambda|^2 + |D_i|)|d| \Big)
\end{align*}

With a multipulse, we will need an analogue of (3.25) in San98. The idea here is that we hit our expression for $D_i d$ with projections to kill some of the terms. We start with

\begin{align*}
a_i^- &- a_i^+ = -D_i d + (P^u_+(X_i; \beta_i^+, \lambda) - P_0^u)a_i^+ - (P^s_-(-X_i; \beta_{i-1}^-, \lambda) - P_0^s)a_i^- \\
&+ v_0(0) \langle v_0(\lambda), w_+(X_i; \beta_i^+, \lambda) \rangle c_i^+ 
- v_0(0) \langle v_0(\lambda), w_-(-X_i; \beta_{i-1}^-, \lambda) \rangle c_i^- \\
&+ \Phi^s_+(X_i, 0; \beta_i^+, \lambda)b_i^+ - \Phi^u_-(-X_i, 0; \beta_{i-1}^-, \lambda)b_{i-1}^- \\
&+ (v_0(0) - v_+(X_i; \beta_i^+, \lambda)) \langle v_0(\lambda), w_+(X_i; \beta_i^+, \lambda) \rangle c_i^+ \\
&- (v_0(0) - v_-(-X_i; \beta_{i-1}^-, \lambda)) \langle v_0(\lambda), w_-(-X_i; \beta_{i-1}^-, \lambda) \rangle c_i^- \\
&+ \int_0^{X_i} \Phi^s_+(X_i, y; \beta_i^+, \lambda) [ G_i^+(y) W_i^+(y) + d_i \lambda^2 \tilde{H}(y) ] dy \\
&- \int_0^{-X_i} \Phi^u_-(-X_i, y; \beta_{i-1}^-, \lambda) [ G_{i-1}^-(y) W_{i-1}^-(y) + d_{i-1} \lambda^2 \tilde{H}(y) ] dy 
\end{align*}

This time, we take projections $P^s_0$ and $P^u_0$ individually. Recalling where the $a_i^\pm$ and $v_0(0)$ live and that $v_0(0)$ is wiped out by both projections, this becomes 

\begin{align*}
a_i^+ &= P^u_0 D_i d - P^u_0 L_3(\lambda)_i(a, b, c, d) \\
a_i^- &= -P^s_0 D_i d + P^s_0 L_3(\lambda)_i(a, b, c, d)
\end{align*}

Define $A_2$ to be all the stuff on the RHS other than the $D_i d$ term. Thus we have 

\begin{align*}
a_i^+ &= P^u_0 D_i d + A_2(\lambda)_i^+(b, c, d) \\
a_i^- &= -P^s_0 D_i d + A_2(\lambda)_i^-(b, c, d)
\end{align*}

We then can come up with a bound for $A_2$ using the bound for $L_3$ and the bound for $A_1$.

\begin{align*}
|A_2&(\lambda)_i(b, c, d)| \\
&\leq C |L_3(\lambda)_i(a, b, c, d)| \\
&\leq C \Big( (p_1(X_i; \lambda) + ||G|| )|A_1(\lambda)_i(b, c, d)| \\
&+ (e^{-\alpha X_i} + ||G||)|b| + ( p_4(X_i; \lambda) + e^{-(\alpha - |\nu(\lambda)|)X_i} )|c_i| + (e^{-\tilde{\alpha} X_i} + ||G||) |\lambda^2| |d| \Big) \\
&\leq C \Big( (p_1(X_i; \lambda) + ||G|| )( (e^{-\alpha X_i} + ||G||)|b| + ( p_4(X_i; \lambda) + ||G|| )|c_i| + (e^{-\tilde{\alpha} X_i} + ||G||) |\lambda^2| |d| + |D_i||d|)  \\
&+ (e^{-\alpha X_i} + ||G||)|b| + ( p_4(X_i; \lambda) + ||G|| )|c_i| + (e^{-\tilde{\alpha} X_i} + ||G||) |\lambda^2| |d| \Big) \\
&\leq C \Big( (p_1(X_i; \lambda) + ||G|| + 1)((e^{-\alpha X_i} + ||G||)|b| 
+ ( p_4(X_i; \lambda) + e^{-(\alpha - |\nu(\lambda)|)X_i} )|c_i| + (e^{-\tilde{\alpha} X_i} + ||G||) |\lambda^2| |d|)\\
&+(p_1(X_i; \lambda) + ||G|| )|D_i||d|) \Big)
\end{align*}

Simplifying this expression and eliminating higher order terms, we attain our bound for $A_2$.

\begin{align*}
|A_2&(\lambda)_i(b, c, d)| \\
&\leq C \Big( (e^{-\alpha X_i} + ||G||)|b| + ( p_4(X_i; \lambda) + e^{-(\alpha - |\nu(\lambda)|)X_i} )|c_i| + (e^{-\tilde{\alpha} X_i} + ||G||) |\lambda^2| |d| +(p_1(X_i; \lambda) + ||G|| )|D_i||d|) \Big)
\end{align*}

\end{proof}
\end{lemma}

Finally, we want to satisfy the conditions

\begin{align*}
P(\C Q'(0))W_i^-(0) &= 0 \\
P(\C Q'(0))W_i^+(0) &= 0 \\
P(Y^+ \oplus Y^- \oplus Y^0) ( W_i^+(0) - W_i^-(0) ) &= 0
\end{align*}

Since the stable and unstable range spaces at $\lambda = 0$ both contain $\C Q'(0)$, we decompose $b^\pm$ uniquely as $b^\pm = x^\pm + y^\pm$, where $x^\pm \in \C Q'(0)$ and $y^\pm \in Y^\pm$. Then since

\begin{equation}\label{directsum}
\C^n = \C\Psi(0) \oplus \C Q'(0) \oplus Y^+ \oplus Y^- \oplus Y^0
\end{equation}

(each of these is 1D in this case), the conditions above are equivalent to the following projections

\begin{align*}
P(\C Q'(0) \oplus Y^0 )W^-(0) &= 0 \\
P(\C Q'(0) \oplus Y^0 )W^+(0) &= 0 \\
P(Y^+ \oplus Y^-) (W^+(0) - W^-(0) ) &= 0
\end{align*}

where the range of each projection is indicated, and the kernel of each projection is just the other elements of the direct sum \eqref{directsum}. Since the first two equations wipe out any component in $\C Q'(0) \oplus Y^0$, we don't need to put that in the range of the third projection. \\

Let $y_0 = v_\pm(0; 0)$ be a unit vector for $Y^0$. Since there is only a small order $\lambda$ perturbation when we go from $y_0$ to $v_\pm(0; \lambda)$ the following two direct sums (where we choose either of $v_\pm(0; \lambda)$ for the last component) hold as well.

\begin{equation}\label{directsum2}
\C^n = \C\Psi(0) \oplus \C Q'(0) \oplus Y^+ \oplus Y^- \oplus v_\pm(0; \lambda)
\end{equation}

Thus we can able to use the following projections instead

\begin{align*}
P(\C Q'(0) \oplus \C v_-(0; \lambda) )W^-(0) &= 0 \\
P(\C Q'(0) \oplus \C v_+(0; \lambda) )W^+(0) &= 0 \\
P(Y^+ \oplus Y^-) (W^+(0) - W^-(0) ) &= 0
\end{align*}

To separate out the coefficients nicely, we write this as five projections

\begin{align*}
P(\C Q'(0) )W^-(0) &= 0 \\
P(\C Q'(0) )W^+(0) &= 0 \\
P(\C v_-(0; \lambda))W^-(0) &= 0 \\
P(\C v_+(0; \lambda))W^+(0) &= 0 \\
P(Y^+ \oplus Y^-) (W^+(0) - W^-(0) ) &= 0
\end{align*}

% inversion 3

\begin{lemma}
There exists an operator

\begin{align*}
B_1(\lambda): V_\lambda \times V_d \rightarrow V_b \times V_c \\
\end{align*}

such that 
\[
(b, \tilde{c}) = B_1(\lambda)d
\]

where

\begin{align*}
\tilde{c}_i^- &= e^{\nu(\lambda)X_i} c_i^- \\
\tilde{c}_i^+ &= e^{-\nu(\lambda)X_i} c_i^+
\end{align*}

and

\begin{align*}
(a,b,&\tilde{c}, W) 
= (A_1(\lambda)(B_1(\lambda)d, d), B_1(\lambda)d, W_2(\lambda)(B_1(\lambda)d, d))
\end{align*}

solves our system. The operator $B_1(\lambda)$ is analytic in $\lambda$ and linear in $d$ and has bound

\begin{align*}
|B_1(\lambda)(d)| &\leq C ( (e^{-(\alpha - |\nu(\lambda)|) X_1} + e^{-(\alpha - |\nu(\lambda)|) X_2} ) |D| + |\lambda|^2 )|d|
\end{align*}

\begin{proof}

At $x = 0$, the fixed point equations become

\begin{align*}
W_i^-(0) = \Phi^s_-(&0, -X_{i-1}; \beta_i^-, \lambda)a_{i-1}^- + b_i^- + (P^u_-(0; \beta_i^-, \lambda) - P^u_-(0; 0, 0))b_i^- \\
&+ e^{\nu(\lambda)X_{i-1}} v_-(0; \beta_i^-, \lambda) \langle v_0(\lambda), w_-(-X_{i-1}; \beta_i^-, \lambda) \rangle c_{i-1}^- \\
&+ \int_{-X_{i-1}}^0 \Phi^s_-(0, y; \beta_i^-, \lambda) [ G_i^-(y)W_i^-(y) + \lambda^2 d_i \tilde{H}(y) ] dy \\
&+ \int_{-X_{i-1}}^0
e^{\nu(\lambda)y} v_-(0; \beta_i^-, \lambda) \langle G_i^-(y)W_i^-(y) + \lambda^2 d_i \tilde{H}(y), w_-(y; \beta_i^-, \lambda) \rangle dy \\
W_i^+(0) = \Phi^u_+(&0, X_i; \beta_i^+, \lambda)a_i^+ + b_i^+ + (P^s_+(0; \beta_i^+, \lambda) - P^s_-(0; 0, 0))b_i^+ \\
&+ e^{-\nu(\lambda) X_i} v_+(0; \beta_i^+, \lambda) \langle v_0(\lambda), w_+(X_i; \beta_i^+, \lambda) \rangle c_i^+ \\
&+ \int_{X_i}^0 \Phi^u_+(0, y; \beta_i^+, \lambda) [ G_i^+(y)W_i^+(y) + \lambda^2 d_i \tilde{H}(y) ] dy \\
&+ \int_{X_i}^0 e^{\nu(\lambda)y} v_+(0; \beta_i^+, \lambda) \langle G_i^+(y)W_i^+(y) + \lambda^2 d_i \tilde{H}(y), w_+(y; \beta_i^+, \lambda) \rangle dy
\end{align*}

where we have added and subtracted $P^s_-(0; 0, 0))b_i^+$ and $P^u_-(0; 0, 0))b_i^-$ since we want the $b_i$ to disappear when we take the projection. Let

\begin{equation}\label{p5}
p_5(\lambda) = |P^u_-(0;\lambda) - P^u_-(0; 0, 0)| + |P^s_+(0;\lambda) - P^s_+(0; 0, 0)|
\end{equation}

We hypothesize the following bound for $p_5$, based on intuition and the bounds for the $\beta_i^\pm$

\begin{hypothesis}\label{p5bound}
$p_5(\lambda) = \mathcal{O}(e^{-2 \alpha X_1} + e^{-2 \alpha X_2} + |\lambda|)$
\end{hypothesis}

Before we start hitting things with all sorts of projections, we need to get the $c_i^\pm$ term into a useful form.

\begin{align*}
e^{\nu(\lambda)X_{i-1}} &v_-(0; \beta_i^-, \lambda) \langle v_0(\lambda), w_-(-X_{i-1}; \beta_i^-, \lambda) \rangle c_{i-1}^- \\
&= e^{\nu(\lambda)X_{i-1}} v_-(0; \beta_i^-, \lambda) \langle v_0(\lambda), w_0(\lambda) \rangle c_{i-1}^- + e^{\nu(\lambda)X_{i-1}} v_-(0; \beta_i^-, \lambda) \langle v_0(\lambda), \Delta w_-(-X_{i-1}; \beta_i^-, \lambda) \rangle c_{i-1}^- \\
&= e^{\nu(\lambda)X_{i-1}} c_{i-1}^- v_-(0; \lambda) + e^{\nu(\lambda)X_{i-1}} c_{i-1}^- v_-(0; \beta_i^-, \lambda) \langle v_0(\lambda), \Delta w_-(-X_{i-1}; \beta_i^-, \lambda) \rangle 
\end{align*}

where $\Delta v$ and $\Delta w$ are defined in Lemma \ref{inv2} and $\langle v_0(\lambda), w_0(\lambda) \rangle = 1$. Similarly, we have

\begin{align*}
e^{-\nu(\lambda)X_i} &v_+(0; \beta_i^+, \lambda) \langle v_0(\lambda), w_+(X_i; \beta_i^+, \lambda) \rangle c_i^+ \\
&= e^{-\nu(\lambda)X_i} v_-(0; \beta_i^+, \lambda) \langle v_0(\lambda), w_0(\lambda) \rangle c_i^- + e^{-\nu(\lambda)X_i} v_+(0; \beta_i^+, \lambda) \langle v_0(\lambda), \Delta w_+(X_i; \beta_i^+, \lambda) \rangle c_i^+ \\
&= e^{-\nu(\lambda)X_i} c_i^+ v_+(0; \beta_i^+, \lambda) + e^{-\nu(\lambda)X_i} c_i^+ v_+(0; \beta_i^+, \lambda) \langle v_0(\lambda), \Delta w_+(X_i; \beta_i^+, \lambda) \rangle 
\end{align*}

Next, we let

\begin{align*}
\tilde{c}_i^- &= e^{\nu(\lambda)X_i} c_i^- \\
\tilde{c}_i^+ &= e^{-\nu(\lambda)X_i} c_i^+
\end{align*}

Then these become

\begin{align*}
e^{\nu(\lambda)X_{i-1}} &v_-(0; \beta_i^-, \lambda) \langle v_0(\lambda), w_-(-X_{i-1}; \beta_i^-, \lambda) \rangle c_{i-1}^- \\
&= \tilde{c}_{i-1}^- v_-(0; \beta_i^-, \lambda) + \tilde{c}_{i-1}^- v_-(0; \beta_i^-, \lambda) \langle v_0(\lambda), \Delta w_-(-X_{i-1}; \beta_i^-, \lambda) \rangle \\
e^{-\nu(\lambda)X_i} &v_+(0; \beta_i^+, \lambda) \langle v_0(\lambda), w_+(X_i; \beta_i^+, \lambda) \rangle c_i^+ \\
&= \tilde{c}_i^+ v_+(0; \beta_i^+, \lambda) + \tilde{c}_i^+ v_+(0; \beta_i^+, \lambda) \langle v_0(\lambda), \Delta w_+(X_i; \beta_i^+, \lambda) \rangle 
\end{align*}

Substituting these into the fixed point equations at $x = 0$, we have

\begin{align*}
W_i^-(0) &= x_i^- + y_i^- + v_-(0; \beta_i^-, \lambda) \tilde{c}_{i-1}^- v_-(0; \beta_i^-, \lambda) \\
&+\Phi^s_-(0, -X_{i-1}; \beta_i^-, \lambda)a_{i-1}^- + (P^u_-(0; \beta_i^-, \lambda) - P^u_-(0; 0, 0))b_i^- \\
&+ \tilde{c}_{i-1}^- v_-(0; \beta_i^-, \lambda) \langle v_0(\lambda), \Delta w_-(-X_{i-1}; \beta_i^-, \lambda) \rangle \\
&+ \int_{-X_{i-1}}^0 \Phi^s_-(0, y; \beta_i^-, \lambda) [ G_i^-(y)W_i^-(y) + \lambda^2 d_i \tilde{H}(y) ] dy \\
&+ \int_{-X_{i-1}}^0
e^{\nu(\lambda)y} v_-(0; \beta_i^-, \lambda) \langle G_i^-(y)W_i^-(y) + \lambda^2 d_i \tilde{H}(y), w_-(y; \beta_i^-, \lambda) \rangle dy \\
W_i^+(0) &= x_i^+ + y_i^+ + v_+(0; \beta_i^+, \lambda) \tilde{c}_i^+ v_+(0; \beta_i^+, \lambda) \\
&+\Phi^u_+(0, X_i; \beta_i^+, \lambda)a_i^+ + (P^s_+(0; \beta_i^+, \lambda) - P^s_-(0; 0, 0))b_i^+ \\
&+ \tilde{c}_i^+ v_+(0; \beta_i^+, \lambda) \langle v_0(\lambda), \Delta w_+(X_i; \beta_i^+, \lambda) \rangle \\  
&+ \int_{X_i}^0 \Phi^u_+(0, y; \beta_i^+, \lambda) [ G_i^+(y)W_i^+(y) + \lambda^2 d_i \tilde{H}(y) ] dy \\
&+ \int_{X_i}^0 e^{\nu(\lambda)y} v_+(0; \beta_i^+, \lambda) \langle G_i^+(y)W_i^+(y) + \lambda^2 d_i \tilde{H}(y), w_+(y; \beta_i^+, \lambda) \rangle dy
\end{align*}

Plugging in the fixed point equations into the above set of projections, we get the matrix equation

\[
\begin{pmatrix}x_i^- \\ x_i^+ \\ 
\tilde{c}_{i-1}^- v_-(0; \lambda) \\
\tilde{c}_i^+ v_+(0; \lambda) \\
y_i^+ - y_i^- \end{pmatrix} + L_4(\lambda)(b, \tilde{c}) = 0
\]

where $L_4(\lambda)(b, \tilde{c})$ is the rest of the terms that don't get eliminated outright by the projections. Note that so far we have written $L_4$ entirely in terms of $\tilde{c}$. We will make sure this remains the case when we perform the substitutions for $W$ and $a$.\\

For the bound on $L_4$, we need to get bounds on the integral terms involving $W$ in a similar fashion to what we did before. First, we find a bound for the center integral involving $W$ on the ``minus'' piece.\\

\begin{align*}
&\left| \int_{-X_{i-1}}^0
e^{\nu(\lambda)y} v_-(0; \beta_i^-, \lambda) \langle G_i^-(y)W_i^-(y), w_-(y; \beta_i^-, \lambda) \rangle \right| \\
&\leq C \int_{-X_{i-1}}^0 e^{\nu(\lambda)y} |G_i^-(y)| |W_i^-(y)| dy \\
&\leq C \int_{-X_{i-1}}^0 e^{\nu(\lambda)y} e^{-\alpha X_{i-1}} e^{-\alpha(X_{i-1} + y)} |W_i^-(y)| dy
\end{align*}

Substituting the piecewise bound for $W_2$, we have

\begin{align*}
&\left| \int_{-X_{i-1}}^0
e^{\nu(\lambda)y} v_-(0; \beta_i^-, \lambda) \langle G_i^-(y)W_i^-(y), w_-(y; \beta_i^-, \lambda) \rangle \right| \\
&\leq C \int_{-X_{i-1}}^0 e^{\nu(\lambda)y} e^{-\alpha X_{i-1}} e^{-\alpha(X_{i-1} + y)} ( |b| + e^{\nu(\lambda)X_{i-1}}|c_{i-1}| + (|\lambda|^2 + D_{i-1})|d| )dy \\
&\leq C e^{-\alpha X_{i-1}} \int_{-X_{i-1}}^0 e^{|\nu(\lambda)|X_{i-1}} e^{-\alpha(X_{i-1} + y)} ( |b| + e^{|\nu(\lambda)|X_{i-1}}|c_{i-1}| + (|\lambda|^2 + D_{i-1})|d| )dy \\
&\leq C e^{-(\alpha - |\nu(\lambda)|) X_{i-1}} ( |b| + e^{|\nu(\lambda)|X_{i-1}}|c_{i-1}| + (|\lambda|^2 + D_{i-1})|d| ) \int_{-X_{i-1}}^0 e^{-\alpha(X_{i-1} + y)} dy \\
&\leq C e^{-(\alpha - |\nu(\lambda)|) X_{i-1}} ( |b| + e^{|\nu(\lambda)|X_{i-1}}|c_{i-1}| + (|\lambda|^2 + D_{i-1})|d| ) \\
&\leq C ( e^{-(\alpha - |\nu(\lambda)|) X_{i-1}} |b| + e^{-(\alpha - 2 |\nu(\lambda)|) X_{i-1}}|c_{i-1}| + e^{-(\alpha - |\nu(\lambda)|) X_{i-1}} (|\lambda|^2 + D_{i-1})|d| )
\end{align*}

We can also use this bound on the noncenter integral on the ``minus'' piece, since that integral does not contain the $e^{\nu(\lambda)y}$ term, which only makes things worse. (In fact, we can get a better bound, but it does not matter.)\\

Similarly, for the center integral involving $W$ on the the ``plus'' piece, we will have a bound

\begin{align*}
&\left| \int_{X_i}^0 e^{\nu(\lambda)y} v_+(0; \beta_i^+, \lambda) \langle G_i^+(y)W_i^+(y), w_+(y; \beta_i^+, \lambda) \rangle dy \right| \\
&\leq C ( e^{-(\alpha - |\nu(\lambda)|) X_i} |b| + e^{-(\alpha - 2 |\nu(\lambda)|) X_{i-1}}|c_i| + e^{-(\alpha - |\nu(\lambda)|) X_i} (|\lambda|^2 + D_i)|d| )
\end{align*}

The integrals not involving $W$ can be evaluated using the $\tilde{\alpha}$ trick, as we have done before. Thus we have the following bound on $L_4(\lambda)$. 

\begin{align*}
|L_4&(\lambda)_i(b, c, d)|\\ 
&\leq C \Big( e^{-\alpha X_i} |a_i^+| +  e^{-\alpha X_{i-1}} |a_{i-1}^-| + p_5(\lambda) |b_i| \\
&+ p_2(X_{i-1}; \lambda) |\tilde{c}_{i-1}| + p_2(X_i; \lambda) |\tilde{c}_i| \\
&+ ( e^{-(\alpha - |\nu(\lambda)|) X_{i-1}} |b| + e^{-(\alpha - 2 |\nu(\lambda)|) X_{i-1}}|c_{i-1}| + e^{-(\alpha - |\nu(\lambda)|) X_{i-1}} (|\lambda|^2 + D_{i-1})|d| ) \\
&+ ( e^{-(\alpha - |\nu(\lambda)|) X_i} |b| + e^{-(\alpha - 2 |\nu(\lambda)|) X_{i-1}}|c_i| + e^{-(\alpha - |\nu(\lambda)|) X_i} (|\lambda|^2 + D_i)|d| )  \\
&+ |\lambda|^2 |d| \Big)\\
\end{align*}

Before we continue, we need to get everything we have so far on the RHS in terms of the $\tilde{c}$. Fortunately, this is not hard to do since we can absorb another $|\nu(\lambda)|X_i$ using $\alpha$. This gives us

\begin{align*}
|L_4&(\lambda)_i(b, c, d)|\\
&\leq C \Big( e^{-\alpha X_i} |a_i^+| +  e^{-\alpha X_{i-1}} |a_{i-1}^-| + p_5(\lambda) |b_i| \\
&+ p_2(X_{i-1}; \lambda) |\tilde{c}_{i-1}| + p_2(X_i; \lambda) |\tilde{c}_i| \\
&+ ( e^{-(\alpha - |\nu(\lambda)|) X_{i-1}} |b| + e^{-(\alpha - 3 |\nu(\lambda)|) X_{i-1}}|\tilde{c}_{i-1}| + e^{-(\alpha - |\nu(\lambda)|) X_{i-1}} (|\lambda|^2 + D_{i-1})|d| ) \\
&+ ( e^{-(\alpha - |\nu(\lambda)|) X_i} |b| + e^{-(\alpha - 3 |\nu(\lambda)|) X_{i-1}}|\tilde{c}_i| + e^{-(\alpha - |\nu(\lambda)|) X_i} (|\lambda|^2 + D_i)|d| )  \\
&+ |\lambda|^2 |d| \Big)\\
&\leq C \Big( e^{-\alpha X_i} |a_i^+| +  e^{-\alpha X_{i-1}} |a_{i-1}^-| + (p_5(\lambda) + e^{-(\alpha - |\nu(\lambda)|) X_1}) |b| \\
&+ (p_2(X_{i-1}; \lambda) + e^{-(\alpha - 3 |\nu(\lambda)|) X_{i-1}}) |\tilde{c}_{i-1}| + (p_2(X_i; \lambda) + e^{-(\alpha - 3 |\nu(\lambda)|) X_i})|\tilde{c}_i| \\
&+ (e^{-(\alpha - |\nu(\lambda)|) X_1} + e^{-(\alpha - |\nu(\lambda)|) X_2})|D||d| + |\lambda|^2|d|
\Big)
\end{align*}

where $p_2(X; \lambda)$ is defined in Lemma \ref{inv2}. Luckily, $p_2(X; \lambda)$ does not enter into the final bound, since we do not have a decay rate for it, but we know that its limit is 0 as $|X| \rightarrow \infty$, which is all we need.\\

To finish the bound, we plug in $A_1$ for $|a|$

\begin{align*}
|L_4&(\lambda)_i(b, c, d)|\\
&\leq C \Big( e^{-\alpha X_i} |A_1(\lambda)_i(b, c, d)| \\
&+  e^{-\alpha X_{i-1}} |A_1(\lambda)_{i-1}(b, c, d)| \\
&+ (p_5(\lambda) + e^{-(\alpha - |\nu(\lambda)|) X_1}) |b| \\
&+ (p_2(X_{i-1}; \lambda) + e^{-(\alpha - 3 |\nu(\lambda)|) X_{i-1}}) |\tilde{c}_{i-1}| + (p_2(X_i; \lambda) + e^{-(\alpha - 3 |\nu(\lambda)|) X_i})|\tilde{c}_i| \\
&+ (e^{-(\alpha - |\nu(\lambda)|) X_1} + e^{-(\alpha - |\nu(\lambda)|) X_2})|D||d| + |\lambda|^2|d|
\Big) \\
&\leq C \Big( e^{-\alpha X_i} ( (e^{-\alpha X_i} + ||G||) |b| + ( p_4(X_i; \lambda) + e^{-(\alpha - |\nu(\lambda)|)X_i} )|c_i|
+ (e^{-\tilde{\alpha} X_i} + ||G||) |\lambda^2| |d| + |D_i||d| ) \\
&+ e^{-\alpha X_{i-1}} ( (e^{-\alpha X_{i-1}} + ||G||) |b| + ( p_4(X_{i-1}; \lambda) + e^{-(\alpha - |\nu(\lambda)|)X_{i-1}} )|c_{i-1}|
+ (e^{-\tilde{\alpha} X_{i-1}} + ||G||) |\lambda^2| |d| + |D_{i-1}||d| ) \\
&+ (p_5(\lambda) + e^{-(\alpha - |\nu(\lambda)|) X_1}) |b| \\
&+ (p_2(X_{i-1}; \lambda) + e^{-(\alpha - 3 |\nu(\lambda)|) X_{i-1}}) |\tilde{c}_{i-1}| + (p_2(X_i; \lambda) + e^{-(\alpha - 3 |\nu(\lambda)|) X_i})|\tilde{c}_i| \\
&+ (e^{-(\alpha - |\nu(\lambda)|) X_1} + e^{-(\alpha - |\nu(\lambda)|) X_2})|D||d| + |\lambda|^2|d|
\Big) \\
\end{align*}

We can do the same thing we did above to get this in terms of the $\tilde{c}$.

\begin{align*}
|L_4&(\lambda)_i(b, c, d)|\\
&\leq C \Big( e^{-\alpha X_i} ( (e^{-\alpha X_i} + ||G||) |b| + ( p_4(X_i; \lambda) + e^{-(\alpha - 2|\nu(\lambda)|)X_i} )|\tilde{c}_i|
+ (e^{-\tilde{\alpha} X_i} + ||G||) |\lambda^2| |d| + |D_i||d| ) \\
&+ e^{-\alpha X_{i-1}} ( (e^{-\alpha X_{i-1}} + ||G||) |b| + ( p_4(X_{i-1}; \lambda) + e^{-(\alpha - 2 |\nu(\lambda)|)X_{i-1}} )|\tilde{c}_{i-1}|
+ (e^{-\tilde{\alpha} X_{i-1}} + ||G||) |\lambda^2| |d| + |D_{i-1}||d| ) \\
&+ (p_5(\lambda) + e^{-(\alpha - |\nu(\lambda)|) X_1}) |b| \\
&+ (p_2(X_{i-1}; \lambda) + e^{-(\alpha - 3 |\nu(\lambda)|) X_{i-1}}) |\tilde{c}_{i-1}| + (p_2(X_i; \lambda) + e^{-(\alpha - 3 |\nu(\lambda)|) X_i})|\tilde{c}_i| \\
&+ (e^{-(\alpha - |\nu(\lambda)|) X_1} + e^{-(\alpha - |\nu(\lambda)|) X_2})|D||d| + |\lambda|^2|d|
\Big) \\
\end{align*}

Simplifying and eliminating higher order terms, this becomes

\begin{align*}
|L_4&(\lambda)_i(b, c, d)|\\
&\leq C \Big(
(p_5(\lambda) + e^{-(\alpha - |\nu(\lambda)|) X_1}) |b| \\
&+ (p_2(X_{i-1}; \lambda) + e^{-(\alpha - 3 |\nu(\lambda)|) X_{i-1}} + e^{-\alpha X_{i-1}} p_4(X_{i-1}; \lambda) ) |\tilde{c}_{i-1}| \\
&+ (p_2(X_i; \lambda) + e^{-(\alpha - 3 |\nu(\lambda)|) X_i} + e^{-\alpha X_i} p_4(X_i; \lambda) )|\tilde{c}_i| \\
&+ (e^{-(\alpha - |\nu(\lambda)|) X_1} + e^{-(\alpha - |\nu(\lambda)|) X_2})|D||d| + |\lambda|^2|d|
\Big) \\
&\leq C \Big(
(p_5(\lambda) + e^{-\tilde{\alpha} X_1}  |b| \\
&+ (p_2(X_{i-1}; \lambda) + e^{-\tilde{\alpha} X_{i-1}} + e^{-\alpha X_{i-1}} p_4(X_{i-1}; \lambda) ) |\tilde{c}_{i-1}| \\
&+ (p_2(X_i; \lambda) + e^{-\tilde{\alpha} X_i} + e^{-\alpha X_i} p_4(X_i; \lambda) )|\tilde{c}_i| \\
&+ (e^{-(\alpha - |\nu(\lambda)|) X_1} + e^{-(\alpha - |\nu(\lambda)|) X_2})|D||d| + |\lambda|^2|d|
\Big)
\end{align*}

To do the inversion, we use Hypothesis \ref{p5bound} for $p_5(\lambda)$ and note that in the previous lemma we chose $X_1$ sufficiently large so that $p_2(X_1; \lambda) < \delta$. Thus we have

\begin{align*}
|L_4&(\lambda)_i(b, c, d)| \leq C \delta (|b| + |\tilde{c}|) + C ( (e^{-(\alpha - |\nu(\lambda)|) X_1} + e^{-(\alpha - |\nu(\lambda)|) X_2})|D||d|+ |\lambda|^2)|d|
\end{align*}

Thus for sufficiently small $\delta$, we can perform the inversion, as in the previous two lemmas.\\

Since the map $J_2$ defined by
\[
J_2( (x_i^+, x_i^-),(y_i^+, y_i^-), \tilde{c}_{i-1}^-, \tilde{c}_i^+ ) \rightarrow ( x_i^+, x_i^-, \tilde{c}_{i-1}^-, \tilde{c}_i^+, y_i^+ -  y_i^- )
\]

is an isomorphism, the operator

\[
S_2(x,y, \tilde{c}) = J_2(x+y, \tilde{c}) + L_4(\lambda)(x+y,\tilde{c}, 0)
\]

is invertible. Thus we have

\begin{equation}
(b,\tilde{c}) = B_1(\lambda)(d) = -S_2^{-1} L_4(\lambda)(0, 0, d)
\end{equation}

where we have the following bound on $B_1$

\begin{align*}
|B_1(\lambda)(d)| &\leq C ( (e^{-(\alpha - |\nu(\lambda)|) X_1} + e^{-(\alpha - |\nu(\lambda)|) X_2})|D| + |\lambda|^2 )|d|
\end{align*}
 
\end{proof}
\end{lemma}

For our final step, we estimate the jumps

\begin{equation}
\xi_i = \langle \Psi(0), W_i^+(0) - W_i^-(0) \rangle 
\end{equation}

Recall that $\Psi(0)$ is the adjoint solution for the unperturbed problem linearized about the single pulse $q(x)$, i.e. with $\lambda = 0$ and $\beta_i^\pm = 0$. The equations for $W$ contain the evolution operator $\Phi^{(s/u)}_\pm(x, y; \beta_i^\pm, \lambda)$ which are for the perturbed system with $\lambda \neq 0$ and with ICs $\beta_i^\pm$.\\

For the adjoint solution $\Psi(x)$, we have estimate 

\begin{equation}
|\Psi(x)| \leq C e^{-\alpha|x|}
\end{equation}

which holds since we know exactly what $\Psi$ is (it involves only the single pulse $q(x)$ and its derivatives, all of which decay with rate $\alpha$). Note as well that $\Psi(0)$ is just a fixed constant.\\

\begin{align*}
W_i^+(0) - W_i^-(0) &= b_i^+ - b_i^- \\
&+ \Phi^u_+(0, X_i; \beta_i^+, \lambda)a_i^+ - \Phi^s_-(0, -X_{i-1}; \beta_i^-, \lambda)a_{i-1}^- \\
&+(P^s_+(0; \beta_i^+, \lambda) - P^s_-(0; 0, 0))b_i^+  - (P^u_-(0; \beta_i^-, \lambda) - P^u_-(0; 0, 0))b_i^- \\
&+ v_+(0; \beta_i^+, \lambda) \langle v_0(\lambda), w_+(X_i; \beta_i^+, \lambda) \rangle e^{-\nu(\lambda)X_i} c_i^+ \\
&- v_-(0; \beta_i^-, \lambda) \langle v_0(\lambda), w_-(-X_{i-1}; \beta_i^-, \lambda) \rangle e^{\nu(\lambda)X_{i-1}} c_{i-1}^- \\
&+ \int_{X_i}^0 \Phi^u_+(0, y; \beta_i^+, \lambda) [ G_i^+(y)W_i^+(y) + \lambda^2 d_i \tilde{H}(y) ] dy \\
&+ \int_{X_i}^0 e^{-\nu(\lambda)y} v_+(0; \beta_i^+, \lambda) \langle G_i^+(y)(y)W_i^+(y) + \lambda^2 d_i \tilde{H}(y), w_+(y; \beta_i^+, \lambda) \rangle dy \\
&- \int_{-X_{i-1}}^0 \Phi^s_-(0, y; \beta_i^-, \lambda) [ G_i^-(y)W_i^-(y) + \lambda^2 d_i \tilde{H}(y) ] dy \\
&- \int_{-X_{i-1}}^0
e^{\nu(\lambda)y} v_-(0; \beta_i^-, \lambda) \langle G_i^-(y)(y)W_i^-(y) + \lambda^2 d_i \tilde{H}(y), w_-(y; \beta_i^-, \lambda) \rangle dy \\
\end{align*}

We will evaluate (or estimate) the terms in the jump equation in a series of lemmas.

% lemma : a terms in jump

\begin{lemma}\label{jumpa}
For the terms involving $a$ in the jump $\xi_i$, we have

\begin{align*}
\langle \Psi(0), &\Phi^u_+(0, X_i; \beta_i^+, \lambda)a_i^+ - \Phi^s_-(0, -X_{i-1}; \beta_i^-, \lambda)a_{i-1}^- \rangle \\
&= \langle \Psi(X_i), P^u_0 D_i d \rangle + \langle \Psi(-X_{i-1}), P^s_0 D_{i-1} d \rangle \\
&+ + \mathcal{O}\Big( (e^{-\alpha X_1} + e^{-\alpha X_2}) \Big( |\lambda| + e^{-\tilde{\alpha} X_1} + e^{-\tilde{\alpha} X_2} )|D| \Big)|d| \Big)
\end{align*}

\begin{proof}

Recall the following expressions for $a_i^\pm$.

\begin{align*}
a_i^+ &= P^u_0 D_i d + A_2(\lambda)_i^+(d))\\
a_i^- &= -P^s_0 D_i d + A_2(\lambda)_i^-(d))
\end{align*}

For $\langle \Psi(0), \Phi^u_+(0, X_i; \beta_i^+, \lambda)a_i^+ \rangle$, we have

\begin{align*}
\langle \Psi(0), &\Phi^u_+(0, X_i; \beta_i^+, \lambda) a_i^+ \rangle = \langle \Psi(0), \Phi^u_+(0, X_i; \beta_i^+, \lambda) P^u_0 D_i d \rangle + \langle \Psi(0), \Phi^u_+(0, X_i; \beta_i^+, \lambda) A_2(\lambda)_i^+(b, c, d) \rangle \\
\end{align*} 

Note that $\Psi(0)$ is the adjoint solution unperturbed by $\lambda$ and with $\beta_i^\pm = 0$, whereas the evolution $\Phi^u_+(0, X_i; \lambda)$ is perturbed by $\lambda$ and has $\beta_i^\pm \neq 0$. Let

\begin{equation}\label{p6}
p_6(y; \lambda) = |\Phi^u_+(0, y; \beta_i^+, \lambda) - \Phi^u_+(0, y; 0, 0)| + |\Phi^s_-(0, -y; \lambda) - \Phi^s_-(0, -y; 0, 0)| 
\end{equation}

For now, we take the following hypothesis, which should follow from the Taylor theorem and the bounds on $\beta_i^\pm$

\begin{hypothesis}\label{p6}
\begin{equation}\label{p6bound}
p_6(y; \lambda) \leq 
C (|\lambda| + e^{-\alpha X_1} + e^{-\alpha X_2} ) e^{-\alpha |y|}
\end{equation}
\end{hypothesis}

Adding and subtracting $\Phi^u_+(0, X_i; 0, 0)$, we have

\begin{align*}
\langle \Psi(0), &\Phi^u_+(0, X_i; \beta_i^+, \lambda) a_i^+ \rangle \\
&= \langle \Psi(0), \Phi^u_+(0, X_i; 0, 0) P^u_0 D_i d \rangle + \langle \Psi(0), (\Phi^u_+(0, X_i; \beta_i^+, \lambda) - \Phi^u_+(0, X_i; 0, 0)) P^u_0 D_i d \rangle \\
&+ \langle \Psi(0), \Phi^u_+(0, X_i; \beta_i^+, \lambda) A_2(\lambda)_i^+(b,c,d) \rangle \\
&= \langle \Psi(X_i), P^u_0 D_i d \rangle 
+ \mathcal{O}((|\lambda| + e^{-\alpha X_1} + e^{-\alpha X_2} ) e^{-\alpha X_i}|D_i||d|) + \langle \Psi(0), \Phi^u_+(0, X_i; \lambda) A_2(\lambda)_i^+(b,c,d) \rangle 
\end{align*}

Using the bound for $A_2$, the last term on the RHS has bound

\begin{align*}
\langle \Psi(0), &\Phi^u_+(0, X_i; \beta_i^+, \lambda) A_2(\lambda)_i^+(b,c,d) \rangle \\
&\leq C e^{-\alpha X_i} |A_2(\lambda)_i(b,c,d)| \\
&\leq C e^{-\alpha X_i}  \Big( (e^{-\alpha X_i} + ||G||)|b| + ( p_4(X_i; \lambda) + e^{-(\alpha - |\nu(\lambda)|)X_i} )|c_i| \\
&+ (e^{-\tilde{\alpha} X_i} + ||G||) |\lambda^2| |d| +(p_1(X_i; \lambda) + ||G|| )|D_i||d|) \Big)
\end{align*}

We want to plug in the $B_1$ bound here, but first we need to get this in terms of the $\tilde{c}$. This is not hard to do.

\begin{align*}
\langle \Psi(0), &\Phi^u_+(0, X_i; \lambda) A_2(\lambda)_i^+(b,c,d) \rangle \\
&\leq C e^{-\alpha X_i} ( (e^{-\alpha X_i} + ||G||)|b| + ( p_4(X_i; \lambda) + e^{-(\alpha - |2 \nu(\lambda)|)X_i} )|\tilde{c}_i| \\
&+ (e^{-\tilde{\alpha} X_i} + ||G||) |\lambda^2| |d| +(p_1(X_i; \lambda) + ||G|| )|D_i||d|))
\end{align*}

Substituting $B_1$ for $b$ and $\tilde{c}$ and using the $B_1$ bound, we have

\begin{align*}
\langle \Psi(0), &\Phi^u_+(0, X_i; \beta_i^+, \lambda) A_2(\lambda)_i^+(b,c,d) \rangle \\
&\leq C e^{-\alpha X_i}  \Big( (e^{-\alpha X_i} + ||G||)|B_1(\lambda)(d)| \\
&+ ( p_4(X_i; \lambda) + e^{-(\alpha - |2 \nu(\lambda)|)X_i} )|B_1(\lambda)(d)| \\
&+ (e^{-\tilde{\alpha} X_i} + ||G||) |\lambda^2| |d| +(p_1(X_i; \lambda) + ||G|| )|D_i||d|) \Big) \\
&\leq C e^{-\alpha X_i}  \Big( (e^{-\alpha X_i} + ||G||)( e^{-(\alpha - |\nu(\lambda)|) X_1} |D| + |\lambda|^2 )|d| \\
&+ ( p_4(X_i; \lambda) + e^{-(\alpha - |2 \nu(\lambda)|)X_i} )( e^{-(\alpha - |\nu(\lambda)|) X_1} |D| + |\lambda|^2 )|d| \\
&+ (e^{-\tilde{\alpha} X_i} + ||G||) |\lambda^2| |d| +(p_1(X_i; \lambda) + ||G|| )|D_i||d|) \Big) 
\end{align*}

Collecting like terms and dropping higher order terms, this becomes

\begin{align*}
\langle \Psi(0), &\Phi^u_+(0, X_i; \beta_i^+, \lambda) A_2(\lambda)_i^+(b,c,d) \rangle \\
&\leq C e^{-\alpha X_i} \Big( (e^{-\alpha X_i} + ||G|| + p_4(X_i; \lambda) + e^{-(\alpha - |2 \nu(\lambda)|)X_i} ) |\lambda|^2 ) \\
&+ (p_1(X_i; \lambda) + e^{-(\alpha - |\nu(\lambda)|) X_1} + |G|| )|D| \Big)|d| \\
&\leq C \Big( e^{-(\alpha - \nu(\lambda)) X_i} |\lambda|^2 
+ e^{-(\alpha - \nu(\lambda)) X_i}( e^{-\alpha X_1} + ||G|| + p_1(X_i; \lambda))|D| )|d| \Big) \\
&\leq e^{-\alpha X_i} \Big( |\lambda|^2  + (p_1(X_i; \lambda) + e^{-(\alpha - |\nu(\lambda)|) X_1} + ||G|| )|D| \Big)|d| 
\end{align*}

Thus, we conclude

\begin{align*}
\langle \Psi(0), &\Phi^u_+(0, X_i; \beta_i^+, \lambda) a_i^+ \rangle \\
&= \langle \Psi(X_i), P^u_0 D_i d \rangle + \mathcal{O}\Big(e^{-\alpha X_i} \Big( |\lambda|^2  + p_1(X_i; \lambda) + e^{-\tilde{\alpha} X_1} + e^{-\tilde{\alpha} X_2}+ ||G|| + |\lambda|)|D| \Big)|d| \Big)
\end{align*}

Similarly we have

\begin{align*}
\langle \Psi(0), &\Phi^s_-(0, -X_{i-1}; \lambda)a_{i-1}^- \rangle \\
&= -\langle \Psi(-X_{i-1}), P^s_0 D_{i-1} d \rangle
+ \mathcal{O}\Big(e^{-\alpha X_{i-1}} \Big( |\lambda|^2  + p_1(X_{i-1}; \lambda) + e^{-\tilde{\alpha} X_1} +e^{-\tilde{\alpha} X_2} + ||G|| + |\lambda|)|D| \Big)|d| \Big)
\end{align*}

Putting everything together, we have

\begin{align*}
\langle \Psi(0), &\Phi^u_+(0, X_i; \lambda)a_i^+ - \Phi^s_-(0, -X_{i-1}; \lambda)a_{i-1}^- \rangle \\
&= \langle \Psi(X_i), P^u_0 D_i d \rangle + \langle \Psi(-X_{i-1}), P^s_0 D_{i-1} d \rangle \\
&+ \mathcal{O}\Big( (e^{-\alpha X_1} + e^{-\alpha X_2}) \Big( |\lambda|^2  + p_1(X_1; \lambda) + p_1(X_2; \lambda) \\
&+ e^{-\tilde{\alpha} X_1} + e^{-\tilde{\alpha} X_2} + ||G|| + |\lambda|)|D| \Big)|d| \Big)
\end{align*}

Recalling the bounds for $p_1$ and $||G||$, since $\tilde{\alpha} < \alpha$, this can be simplified to

\begin{align*}
\langle \Psi(0), &\Phi^u_+(0, X_i; \lambda)a_i^+ - \Phi^s_-(0, -X_{i-1}; \lambda)a_{i-1}^- \rangle \\
&= \langle \Psi(X_i), P^u_0 D_i d \rangle + \langle \Psi(-X_{i-1}), P^s_0 D_{i-1} d \rangle \\
&+ \mathcal{O}\Big( (e^{-\alpha X_1} + e^{-\alpha X_2}) \Big( |\lambda| + e^{-\tilde{\alpha} X_1} + e^{-\tilde{\alpha} X_2} )|D| \Big)|d| \Big)
\end{align*}

\end{proof}
\end{lemma}

Next, we look at the terms involving $b$. Note that the terms involving $b^\pm$ by themselves will vanish, since they are in the spaces $R^u_-(0; 0) \oplus R^s_+(0; 0)$ which are perpendicular to $\Psi(0)$. For the other terms involving $b$, we have the following lemma.

% lemma : b terms in jump

\begin{lemma}\label{jumpb}

For the terms involving $b$ in the jump $\xi_i$, we have

\begin{align*}
\langle \Psi(0), (P^s_+(0; \beta_i^+, \lambda) - P^s_-(0; 0, 0))b_i^+ \rangle
&\leq C p_5(\lambda) \Big( (e^{-\tilde{\alpha} X_1}+e^{-\tilde{\alpha} X_2})|D| + |\lambda|^2 \Big)|d|\\
\langle \Psi(0), (P^u_-(0; \beta_i^-, \lambda) - P^u_-(0; 0, 0))b_i^- \rangle
&\leq C p_5(\lambda)\Big( (e^{-\tilde{\alpha} X_1}+e^{-\tilde{\alpha} X_2})|D| + |\lambda|^2 \Big)|d|
\end{align*}

where $p_5(\lambda)$ is defined above.

\begin{proof}

Using the bound for $B_1$, we have for the ``negative'' piece

\begin{align*}
|\langle \Psi(0), &(P^u_-(0; \lambda) - P^u_-(0; 0))b_i^- \rangle|
\leq |\Psi(0)| p_5(\lambda)|b_i^-| \\
&\leq |\Psi(0)| p_5(\lambda)|B_1(\lambda)(d)| \\
&\leq C p_5(\lambda) \Big( (e^{-(\alpha - |\nu(\lambda)|) X_1} 
+ e^{-(\alpha - |\nu(\lambda)|) X_2}) |D| + |\lambda|^2 \Big)|d|\\
\end{align*}

Since $\alpha - |\nu(\lambda)| > \tilde{\alpha}$, this bound becomes

\begin{align*}
|\langle \Psi(0), &(P^u_-(0; \lambda) - P^u_-(0; 0))b_i^- \rangle|
\leq C p_5(\lambda) \Big( (e^{-\tilde{\alpha} X_1} + e^{-\tilde{\alpha} X_2}) |D| + |\lambda|^2 \Big)|d|\\
\end{align*}

The ``positive'' piece is similar.

\end{proof}
\end{lemma}

Next, we look at the terms in the jump involving $c$.

% lemma : c terms in jump

\begin{lemma}\label{jumpc}
For the terms involving $c$ in the jump $\xi_i$, we have bounds

\begin{align*}
|\langle \Psi(0), &v_+(0; \beta_i^+, \lambda) \langle v_0(\lambda), w_+(X_i; \beta_i^+,\lambda) \rangle e^{-\nu(\lambda)X_i} c_i^+ \rangle| \\
&\leq C ( |\lambda| + e^{-2 \alpha X_1} + e^{-2 \alpha X_2} )
( ( e^{-\tilde{\alpha} X_1} + e^{-\tilde{\alpha} X_2}) |D| + |\lambda|^2 )|d| \\
|\langle \Psi(0), &v_-(0; \beta_i^-, \lambda) \langle v_0(\lambda), w_-(-X_{i-1}; \beta_i^-, \lambda) \rangle e^{\nu(\lambda)X_{i-1}} c_{i-1}^- \rangle | \\
&\leq C ( |\lambda| + e^{-2 \alpha X_1} + e^{-2 \alpha X_2} )
( ( e^{-\tilde{\alpha} X_1} + e^{-\tilde{\alpha} X_2}) |D| + |\lambda|^2 )|d|  \\
\end{align*}

\begin{proof}
Looking at the ``plus'' term, we have

\begin{align*}
\langle \Psi(0), v_+(0; \lambda) \langle v_0(\lambda), w_+(X_i; \lambda) \rangle e^{-\nu(\lambda)X_i} c_i^+ \rangle &= \langle \Psi(0), v_+(0; \lambda) \rangle \langle v_0(\lambda), w_+(X_i; \lambda) \rangle \tilde{c}_i^+
\end{align*}

To get our bound, we will expand $v_+(0; \beta_i^+, \lambda)$ in a Taylor series about $(\beta_i^+, \lambda) = (0, 0)$. FOR NOW WE WILL ASSUME WE CAN ACTUALLY DO THIS.

\begin{align*}
v_+(0; &\beta_i^+, \lambda) = v_+(0; 0, 0) 
+ \lambda \frac{\partial}{\partial \lambda}v_+(0; \beta_i^+, \lambda)\Big|_{\lambda, \beta_i^+ = (0, 0)} 
+ \beta_i^+ \frac{\partial}{\partial \beta_i^+}v_+(0; \beta_i^+, \lambda)\Big|_{\lambda, \beta_i^+ = (0, 0)} \\
&+ \mathcal{O}(|\lambda|^2 + |\beta_i^+||\lambda| + |\beta_i^+|^2)
\end{align*}

When we take the inner product with $\Psi(0)$, the first term will vanish since 
$\langle \Psi(0), v_+(0; 0, 0) \rangle = 0$. Since the two derivatives in the expression above are constants, we will have

\begin{align*}
\langle \Psi(0), v_+(0; \lambda) \rangle &\leq C ( |\lambda| + |\beta_1^+| ) \\
&\leq C ( |\lambda| + e^{-2 \alpha X_1} + e^{-2 \alpha X_2} )
\end{align*}

Plugging this in, together with the bound for $B_1$ (which is a bound for $e^{-\nu(\lambda)X_i} c_i^+$), we have

\begin{align*}
|\langle \Psi(0), &v_+(0; \lambda) \langle v_0(\lambda), w_+(X_i; \lambda) \rangle e^{-\nu(\lambda)X_i} c_i^+ \rangle| \\ 
&\leq C ( |\lambda| + e^{-2 \alpha X_1} + e^{-2 \alpha X_2} ) |B_1(\lambda)(d)| \\
&\leq C ( |\lambda| + e^{-2 \alpha X_1} + e^{-2 \alpha X_2} )
( ( e^{-(\alpha - |\nu(\lambda)|) X_1} + e^{-(\alpha - |\nu(\lambda)|) X_2}) |D| + |\lambda|^2 )|d| \\
\end{align*}

Since $\alpha - |\nu(\lambda)| > \tilde{\alpha}$, this becomes 

\begin{align*}
|\langle \Psi(0), &v_+(0; \lambda) \langle v_0(\lambda), w_+(X_i; \lambda) \rangle e^{-\nu(\lambda)X_i} c_i^+ \rangle| \\
&\leq C ( |\lambda| + e^{-2 \alpha X_1} + e^{-2 \alpha X_2} )
( ( e^{-\tilde{\alpha} X_1} + e^{-\tilde{\alpha} X_2}) |D| + |\lambda|^2 )|d| 
\end{align*}
 
The other one is similar.\\

\end{proof} 
\end{lemma}

Next, we look at the integral terms in the jump $\xi_i$. First, we look at the ``noncenter'' integral terms. We split these into ones involving $W$ and ones not involving $W$. Before we can do that, we will need an improved estimate for $|W|$.

\begin{lemma}
We have the following improved, piecewise, $x$-dependent estimate for $|W_i^\pm(x)$

\begin{align*}
| W_i^-(x)| &\leq C \Big(e^{-\alpha(X_{i-1} + x)}( e^{|\nu(\lambda)| X_{i-1}} |\lambda|^2 + |D|) + e^{\nu(\lambda)x} |\lambda|^2 
+ |\lambda|^2 + (e^{-\tilde{\alpha} X_1} + e^{-\tilde{\alpha} X_2}) |D| \Big) |d| \\
| W_i^+(x)| &\leq C \Big(e^{-\alpha(X_i - x)}( e^{|\nu(\lambda)| X_i} |\lambda|^2 + |D|) + e^{\nu(\lambda)x} |\lambda|^2 
+ |\lambda|^2 + (e^{-\tilde{\alpha} X_1} + e^{-\tilde{\alpha} X_2}) |D| \Big) |d| \\
\end{align*}

\begin{proof}

Since we have 
\[
W_i^\pm = L_1(\lambda)W_i^\pm + L_2(\lambda)W_i^\pm 
\]

we can use the estimate for $L_1(\lambda)$ together with an improved estimate for $L_2(\lambda)$. We use the same estimate for $L_1(\lambda)$ as we have above. We will do the negative piece here. The positive piece is similar.

\[
||L_1(\lambda)_i^- W|| \leq C e^{-(\alpha -|\nu(\lambda)|)X_{i-1}} ||W||
\]

Plugging in the piecewise bound for $W_2$ for $W$, we have

\begin{align*}
||L_1(\lambda)_i^- W|| &\leq C e^{-(\alpha -|\nu(\lambda)|)X_{i-1}} ( |b| + e^{\nu(\lambda)X_{i-1}}|c_{i-1}| + (|\lambda|^2 + |D_{i-1}|)|d| ) \\
&\leq C e^{-(\alpha - 3|\nu(\lambda)|)X_{i-1}} ( |b| + |\tilde{c}_{i-1}| + (|\lambda|^2 + |D_{i-1}|)|d| ) 
\end{align*}

For $L_2(\lambda)$ we will use the estimate for the negative piece from Lemma \ref{L2}.

\begin{align*}
|L_2(\lambda)(a,b,c,d)_i^-(x)| &\leq C (e^{-\alpha(X_{i-1} + x)}|a_{i-1}^-| + |b| + e^{\nu(\lambda)x} |\tilde{c}_{i-1}^-| + |\lambda|^2 |d| )
\end{align*}

Combining these, we have

\begin{align*}
| &W_i^-(x)| \leq C \Big(e^{-\alpha(X_{i-1} + x)}|a_{i-1}^-| + |b| + e^{\nu(\lambda)x} |\tilde{c}_{i-1}^-| + |\lambda|^2 |d| \\
&+ e^{-(\alpha - 3|\nu(\lambda)|)X_{i-1}} ( |b| + |\tilde{c}_{i-1}| + (|\lambda|^2 + |D_{i-1}|)|d| ) \Big) \\
&\leq C \Big(e^{-\alpha(X_{i-1} + x)}|a_{i-1}^-| + |b| + e^{\nu(\lambda)x} |\tilde{c}_{i-1}^-| + |\lambda|^2 |d| \\
&+ e^{-\tilde{\alpha} X_{i-1}} ( |b| + |\tilde{c}_{i-1}| + (|\lambda|^2 + |D_{i-1}|)|d| ) \Big) \\
&\leq C \Big(e^{-\alpha(X_{i-1} + x)}|a_{i-1}^-| + |b| + e^{\nu(\lambda)x} |\tilde{c}_{i-1}^-| + e^{-\tilde{\alpha} X_{i-1}} |\tilde{c}_{i-1}^-| + |\lambda|^2 |d| + e^{-\tilde{\alpha} X_{i-1}} |D_{i-1}||d| \Big) \\
\end{align*}

where we have chosen stuff so that $3|\nu(\lambda)| \leq \alpha - \tilde{\alpha}$.\\

At this point, we substitute in $A_3(\lambda)$ and $B_1(\lambda)$. First, we substitute $A_3(\lambda)$.

\begin{align*}
| &W_i^-(x)| \leq C \Big(e^{-\alpha(X_{i-1} + x)}|A_1(\lambda)_{i-1}(b, c, d)|| \\
&+ |b| + e^{\nu(\lambda)x} |\tilde{c}_{i-1}^-| + e^{-\tilde{\alpha} X_{i-1}} |\tilde{c}_{i-1}^-| + |\lambda|^2 |d| + e^{-\tilde{\alpha} X_{i-1}} |D_{i-1}||d| \Big) \\
&\leq C \Big(e^{-\alpha(X_{i-1} + x)}( (e^{-\alpha X_{i-1}} + ||G||) |b| + ( p_4(X_{i-1}; \lambda) + e^{-(\alpha - |\nu(\lambda)|)X_{i-1}} )|c_{i-1}| \\
&+ (e^{-\tilde{\alpha} X_{i-1}} + ||G||) |\lambda^2| |d| + |D_{i-1}||d| ) \\
&+ |b| + e^{\nu(\lambda)x} |\tilde{c}_{i-1}^-| + e^{-\tilde{\alpha} X_{i-1}} |\tilde{c}_{i-1}^-| + |\lambda|^2 |d| + e^{-\tilde{\alpha} X_{i-1}} |D_{i-1}||d| \Big) \\
&\leq C \Big(e^{-\alpha(X_{i-1} + x)}( e^{|\nu(\lambda)| X_{i-1}} |\tilde{c}_{i-1}| + |D_{i-1}||d| ) \\
&+ |b| + e^{\nu(\lambda)x} |\tilde{c}_{i-1}^-| + e^{-\tilde{\alpha} X_{i-1}} |\tilde{c}_{i-1}^-| + |\lambda|^2 |d| + e^{-\tilde{\alpha} X_{i-1}} |D_{i-1}||d| \Big) \\
\end{align*}

Finally, we substitute $|B_1(\lambda)(d)$ for $|b|$ and $\tilde{c}$.

\begin{align*}
| &W_i^-(x)| \leq C \Big(e^{-\alpha(X_{i-1} + x)}( e^{|\nu(\lambda)| X_{i-1}} ( e^{-(\alpha - |\nu(\lambda)|) X_1} |D| + |\lambda|^2 )|d| + |D_{i-1}||d| ) \\
&+ e^{\nu(\lambda)x} ( (e^{-(\alpha - |\nu(\lambda)|) X_1} + e^{-(\alpha - |\nu(\lambda)|) X_2})|D| + |\lambda|^2 )|d| \\ &+ ( (e^{-(\alpha - |\nu(\lambda)|) X_1} + e^{-(\alpha - |\nu(\lambda)|) X_2}) |D| + |\lambda|^2 )|d| + |\lambda|^2 |d| + e^{-\tilde{\alpha} X_{i-1}} |D_{i-1}||d| \Big) \\
&\leq C \Big(e^{-\alpha(X_{i-1} + x)}( e^{|\nu(\lambda)| X_{i-1}} |\lambda|^2 + |D|)
+ (e^{-(\alpha - 2 |\nu(\lambda)|) X_1} +e^{-(\alpha - 2 |\nu(\lambda)|) X_2})|D| 
+ e^{\nu(\lambda)x} |\lambda|^2 \\ 
&+ (e^{-(\alpha - |\nu(\lambda)|) X_1} + e^{-(\alpha - |\nu(\lambda)|) X_2} )|D| 
+ |\lambda|^2 + e^{-\tilde{\alpha} X_{i-1}} |D_{i-1}| \Big) |d| \\
&\leq C \Big(e^{-\alpha(X_{i-1} + x)}( e^{|\nu(\lambda)| X_{i-1}} |\lambda|^2 + |D|) + e^{\nu(\lambda)x} |\lambda|^2 
+ |\lambda|^2 + (e^{-\tilde{\alpha} X_1} + e^{-\tilde{\alpha} X_2}) |D| \Big) |d| \\
\end{align*}

Similarly

\begin{align*}
| &W_i^+(x)| \leq C \Big(e^{-\alpha(X_i - x)}( e^{|\nu(\lambda)| X_i} |\lambda|^2 + |D|) + e^{\nu(\lambda)x} |\lambda|^2 
+ |\lambda|^2 + (e^{-\tilde{\alpha} X_1} + e^{-\tilde{\alpha} X_2}) |D| \Big) |d| \\
\end{align*}
\end{proof}
\end{lemma}

We can now estimate the noncenter integrals which involve $W$.

\begin{lemma}\label{noncenterW}

\begin{align*}
\left| \int_{-X_{i-1}}^0 \langle \Psi(0), \Phi^s_-(0, y; \lambda) G_i^-(y) W_i^-(y) \rangle dy \right| &\leq C \Big( e^{-\alpha X_{i-1}} |\lambda|^2 + (e^{-(\alpha + \tilde{\alpha}) X_1} + e^{-(\alpha + \tilde{\alpha}) X_2})|D| \Big) |d| \\
\left| \int_{X_i}^0 \langle \Psi(0), \Phi^u_-(0, y; \lambda) G_i^+(y) W_i^+(y) \rangle dy \right| &\leq C \Big( e^{-\alpha X_i} |\lambda|^2 + (e^{-(\alpha + \tilde{\alpha}) X_1} + e^{-(\alpha + \tilde{\alpha}) X_2})|D| \Big) |d|
\end{align*}

\begin{proof}

We do the ``minus'' integral here. Substituting in our bound for $G_i^-(y)$ and the improved bound for $W_i^-(y)$ from the previous lemma,

\begin{align*}
&\left| \int_{-X_{i-1}}^0 \langle \Psi(0), \Phi^s_-(0, y; \lambda) G_i^-(y) W_i^-(y) \rangle dy \right| \\
&\leq C \int_{-X_{i-1}}^0 e^{\alpha y} |G_i^-(y)| ||W_i^-(y)|| dy \\
&\leq C \int_{-X_{i-1}}^0 e^{\alpha y} e^{-\alpha X_{i-1}} e^{-\alpha(X_{i-1} + y)} \Big(e^{-\alpha(X_{i-1} + y)}( e^{|\nu(\lambda)| X_{i-1}} |\lambda|^2 + |D|) \\
&+ e^{\nu(\lambda)y} |\lambda|^2 
+ |\lambda|^2 + (e^{-\tilde{\alpha} X_1} + e^{-\tilde{\alpha} X_2}) |D| \Big) |d| dy \\
&\leq C e^{-\alpha X_{i-1}} (|\lambda|^2 + (e^{-\tilde{\alpha} X_1} + e^{-\tilde{\alpha} X_2}) |D| ) |d| \int_{-X_{i-1}}^0 e^{\alpha y} dy \\
&+ C ( e^{(\alpha - |\nu(\lambda)|) X_{i-1}} |\lambda|^2 + e^{-\alpha X_{i-1}} |D|)|d|\int_{-X_{i-1}}^0 e^{\alpha y} e^{-\alpha(X_{i-1} + y)} e^{-\alpha(X_{i-1} + y)} dy \\
&+ C e^{-\alpha X_{i-1}} |\lambda|^2 |d| \int_{-X_{i-1}}^0 e^{\alpha y} e^{-\alpha(X_{i-1} + y)} e^{\nu(\lambda)y} dy
\end{align*}

We have three integrals to evaluate. The first one is a constant (independent of $X_{i-1}$. For the third one

\begin{align*}
\int_{-X_{i-1}}^0 e^{\alpha y} e^{-\alpha(X_{i-1} + y)} e^{\nu(\lambda)y} dy 
&\leq \int_{-X_{i-1}}^0 e^{(\alpha - |\nu(\lambda)|) y} dy \leq C
\end{align*}

which is again a constant independent of $X_{i-1}$. We take the absolute value of $\nu(\lambda)$ in the exponential to account for both possible signs of $\nu(\lambda)$. For the second integral, we need something exponentially decaying in $X_{i-1}$, rather than just a constant (that easier bound is insufficient for our needs). But that is easy to obtain.

\begin{align*}
\int_{-X_{i-1}}^0 e^{\alpha y} e^{-\alpha(X_{i-1} + y)} e^{-\alpha(X_{i-1} + y)} dy &= e^{-\alpha X_{i-1}} \int_{-X_{i-1}}^0 e^{-\alpha(X_{i-1} + y)} dy \leq C e^{-\alpha X_{i-1}}
\end{align*}

Putting this all together, we have

\begin{align*}
&\left| \int_{-X_{i-1}}^0 \langle \Psi(0), \Phi^s_-(0, y; \lambda) G_i^-(y) W_i^-(y) \rangle dy \right| \\
&\leq C \Big( e^{-\alpha X_{i-1}} (|\lambda|^2 
+ (e^{-\tilde{\alpha} X_1} + e^{-\tilde{\alpha} X_2}) |D|) + e^{-\alpha X_{i-1}} (e^{(\alpha - |\nu(\lambda)|) X_{i-1}} |\lambda|^2 + e^{-\alpha X_{i-1}} |D|) + e^{-\alpha X_{i-1}} |\lambda|^2 \Big) |d| \\
&\leq C \Big( e^{-\alpha X_{i-1}} |\lambda|^2 
+ (e^{-(\alpha + \tilde{\alpha}) X_1} + e^{-(\alpha + \tilde{\alpha}) X_2})|D| \Big) |d|
\end{align*}

Similarly, for the other integral,

\begin{align*}
&\left| \int_{X_i}^0 \langle \Psi(0), \Phi^u_-(0, y; \lambda) G_i^+(y) W_i^+(y) \rangle dy \right| \leq C \Big( e^{-\alpha X_i} |\lambda|^2 + (e^{-(\alpha + \tilde{\alpha}) X_1} + e^{-(\alpha + \tilde{\alpha}) X_2})|D| \Big) |d|
\end{align*}

\end{proof}
\end{lemma}

Next, we look at the ``noncenter'' integrals involving $H$. These will give us our higher order Melnikov integral.

% lemma : noncenter integrals involving H

\begin{lemma}\label{noncenterH}

\begin{align*}
\langle \Psi(0), &\int_{X_i}^0 \Phi^u_+(0, y; \beta_i^+, \lambda) \lambda^2 d_i \tilde{H}(y) dy - \int_{-X_{i-1}}^0 \Phi^s_-(0, y; \beta_i^-, \lambda) \lambda^2 d_i \tilde{H}(y) dy \rangle \\ 
&= -d_i \lambda^2 \int_{-\infty}^\infty \langle \Psi(y), H(y) \rangle dy + \mathcal{O}\left( (|\lambda| + e^{-\alpha X_1} + e^{-\alpha X_2} ) |\lambda|^2 |d| \right)
\end{align*}

\begin{proof}

Looking at the ``negative'' integral (which is multipled by $d_i \lambda^2$), we write it as

\begin{align*}
\langle \Psi(0)&, \int_{-X_{i-1}}^0 \Phi^s_-(0, y; \beta_i^-, \lambda) \tilde{H}(y) dy \rangle \\ 
&= \int_{-X_{i-1}}^0 \langle \Psi(0), \Phi^s_-(0, y; 0, 0) \tilde{H}(y) \rangle dy + 
\int_{-X_{i-1}}^0 \langle \Psi(0), (\Phi^s_-(0, y; \beta_i^-, \lambda) - \Phi^s_-(0, y; 0, 0)) \tilde{H}(y) \rangle dy
\end{align*}

where we use this trick in order to get the Melnikov term we want. Again, we will assume the bound from Hypothesis \ref{p6}. Using this, the second integral above is order $|\lambda| + e^{-2 \alpha X_1} + e^{-2 \alpha X_2}$.\\

For the first integral, we manipulate things to bring the limits out to $\pm \infty$ (to get the higher order Melnikov integral) and to replace $\tilde{H}$ by $H$ (so that the Melnikov integral only involves the single pulse). We also use what we know about the evolution of the adjoint solution from a previous lemma (CITE IT HERE).

\begin{align*}
\int_{-X_{i-1}}^0 \langle \Psi(0), \Phi^s_-(0, y; 0, 0) \tilde{H}(y) \rangle dy &= 
\int_{-X_{i-1}}^0 \langle \Psi(y), H(y) \rangle dy + \int_{-X_{i-1}}^0 \langle \Psi(y), \Delta H(y) \rangle dy \\
&= \int_{-\infty}^0 \langle \Psi(y), H(y) \rangle dy - \int_{-\infty}^{-X_{i-1}} \langle \Psi(y), H(y) \rangle dy \\
&+ \int_{-X_{i-1}}^0 \langle \Psi(y), \Delta H(y) \rangle dy 
\end{align*}

The first integral on the RHS is half of our Melnikov integral. The second is order $e^{-\alpha X_{i-1}}$, since we know the decay rate of $H$. The third is order $e^{-\alpha X_1} + e^{-\alpha X_2}$ (the order of $\Delta H$). When we do the ``plus'' piece, the first integral is the other half of the Melnikov integral; the second integral is order $e^{-\alpha X_i}$; and the third integral is the same order. Combining all of this gives us our desired result.
\end{proof}

\end{lemma}

Finally, we look at the ``center'' integral terms. As before, we can split this into integrals involving $W$ and integrals not involving $W$. First, we look at the integral involving $W$.

% lemma : center integrals involving W

\begin{lemma} 

\begin{align*}
\left| \langle \Psi(0), \int_{-X_{i-1}}^0
e^{\nu(\lambda)y} v_-(0; \beta_i^-, \lambda) \langle G_i^-(y)W_i^-(y), w_-(y; \beta_i^-, \lambda) \rangle dy \rangle \right| &\leq C |\lambda| (e^{-\tilde{\alpha} X_1} + e^{-\tilde{\alpha} X_2}) (|\lambda|^2 + |D| ) |d| \\
\left| \Psi(0), \int_{X_i}^0 e^{-\nu(\lambda)y} v_+(0; \beta_i^+, \lambda) \langle G_i^+(y)W_i^+(y), w_+(y; \beta_i^+, \lambda) \rangle dy  \right| &\leq C |\lambda| (e^{-\tilde{\alpha} X_1} + e^{-\tilde{\alpha} X_2}) (|\lambda|^2 + |D| ) |d|
\end{align*}

\begin{proof}

We do the ``minus'' integral here. Substituting in our bound for $G_i^-(y)$ and the improved bound for $W_i^-(y)$, we have

\begin{align*}
\langle \Psi(0) &, \int_{-X_{i-1}}^0
e^{\nu(\lambda)y} v_-(0; \beta_i^-, \lambda) \langle G_i^-(y)W_i^-(y), w_-(y; \beta_i^-, \lambda) \rangle dy \rangle \\
&= \int_{-X_{i-1}}^0
e^{\nu(\lambda)y} \langle \Psi(0), v_-(0; \beta_i^- ,\lambda) \rangle \langle G_i^-(y)W_i^-(y), w_-(y; \beta_i^- , \lambda) \rangle dy \\
&\leq C |\langle \Psi(0), v_-(0; \beta_i^-, \lambda) \rangle| 
\int_{-X_{i-1}}^0 e^{-|\nu(\lambda)|y} |G_i^-(y)||W_i^-(y)| dy 
\end{align*}

From Lemma \ref{jumpc}, $|\langle \Psi(0), v_-(0; \lambda) \rangle| = \mathcal{O}(|\lambda| + e^{-2 \alpha X_1} + e^{-2 \alpha X_2})$. Thus we have

\begin{align*}
\langle \Psi(0) &, \int_{-X_{i-1}}^0
e^{\nu(\lambda)y} v_-(0; \lambda) \langle G_i^-(y)W_i^-(y), w_-(y; \lambda) \rangle dy \rangle \\
&\leq C (|\lambda| + e^{-2 \alpha X_1} + e^{-2 \alpha X_2}) \int_{-X_{i-1}}^0 e^{-|\nu(\lambda)|y} |G_i^-(y)||W_i^-(y)| dy \\
&\leq C (|\lambda| + e^{-2 \alpha X_1} + e^{-2 \alpha X_2}) \int_{-X_{i-1}}^0 e^{-|\nu(\lambda)|y} e^{-\alpha X_{i-1}} e^{-\alpha(X_{i-1} + y)} \Big(e^{-\alpha(X_{i-1} + y)}( e^{|\nu(\lambda)| X_{i-1}} |\lambda|^2 + |D|) \\
&+ e^{\nu(\lambda)y} |\lambda|^2 
+ |\lambda|^2 + (e^{-\tilde{\alpha} X_1} + e^{-\tilde{\alpha} X_2}) |D| \Big) |d| dy \\
&\leq C (|\lambda| + e^{-2 \alpha X_1} + e^{-2 \alpha X_2}) e^{-\alpha X_{i-1}} (|\lambda|^2 + (e^{-\tilde{\alpha} X_1} + e^{-\tilde{\alpha} X_2}) |D| ) |d| \int_{-X_{i-1}}^0 e^{-|\nu(\lambda)|y} dy \\
&+ C (|\lambda| + e^{-2 \alpha X_1} + e^{-2 \alpha X_2}) ( e^{(\alpha - |\nu(\lambda)|) X_{i-1}} |\lambda|^2 + e^{-\alpha X_{i-1}} |D|)|d|\int_{-X_{i-1}}^0 e^{-|\nu(\lambda)|y} e^{-\alpha(X_{i-1} + y)} e^{-\alpha(X_{i-1} + y)} dy \\
&+ C (|\lambda| + e^{-2 \alpha X_1} + e^{-2 \alpha X_2}) e^{-\alpha X_{i-1}} |\lambda|^2 |d| \int_{-X_{i-1}}^0 e^{-|\nu(\lambda)|y} e^{-\alpha(X_{i-1} + y)} e^{\nu(\lambda)y} dy
\end{align*}

These three integral terms are almost identical to those in Lemma \ref{noncenterW}. The only differences are there is a factor of $|\lambda| + e^{-2 \alpha X_1} + e^{-2 \alpha X_2}$ out front, and the $e^{\alpha y}$ in the integrals is replaced with $e^{-|\nu(\lambda)|y}$. \\

For the first integral, we have

\begin{align*}
\int_{-X_{i-1}}^0 e^{-|\nu(\lambda)|y} dy &\leq C e^{-|\nu(\lambda)| X_{i-1}}
\end{align*}

For the third integral, we have

\begin{align*}
\int_{-X_{i-1}}^0 e^{-|\nu(\lambda)|y} e^{-\alpha(X_{i-1} + y)} e^{\nu(\lambda)y} dy
&\leq \int_{-X_{i-1}}^0 e^{-\alpha X_{i-1}} e^{-(\alpha + 2|\nu(\lambda)|) y} dy \\
&\leq \int_{-X_{i-1}}^0 e^{-\alpha X_{i-1}} e^{-(\alpha + (\alpha - \tilde{\alpha})) y} dy \\
&= e^{(\alpha - \tilde{\alpha}) X_{i-1}} \int_{-X_{i-1}}^0 e^{-(\alpha + (\alpha - \tilde{\alpha})) X_{i-1}} e^{-(\alpha + (\alpha - \tilde{\alpha})) y} dy \\
&= e^{(\alpha - \tilde{\alpha}) X_{i-1}} \int_{-X_{i-1}}^0 e^{-(2 \alpha - \tilde{\alpha})( X_{i-1} + y)} dy \\
&\leq C e^{(\alpha - \tilde{\alpha}) X_{i-1}}
\end{align*}

For the second integral, we have

\begin{align*}
\int_{-X_{i-1}}^0 e^{-|\nu(\lambda)|y} e^{-\alpha(X_{i-1} + y)} e^{-\alpha(X_{i-1} + y)} dy &\leq \int_{-X_{i-1}}^0 e^{|\nu(\lambda)|X_{i-1}} e^{-2\alpha(X_{i-1} + y)}  dy \\
&\leq C e^{-|\nu(\lambda)|X_{i-1}}
\end{align*}

Putting this all together, we have

\begin{align*}
&\left| \langle \Psi(0), \int_{-X_{i-1}}^0
e^{\nu(\lambda)y} v_-(0; \lambda) \langle G_i^-(y)W_i^-(y), w_-(y; \lambda) \rangle dy \rangle \right| \\
&\leq C (|\lambda| + e^{-2 \alpha X_1} + e^{-2 \alpha X_2}) \Big( e^{-\alpha X_{i-1}} (|\lambda|^2 + (e^{-\tilde{\alpha} X_1} + e^{-\tilde{\alpha} X_2} ) |D|)e^{-|\nu(\lambda)|X_{i-1}} \\
&+ (e^{(\alpha - |\nu(\lambda)|) X_{i-1}} |\lambda|^2 + e^{-\alpha X_{i-1}} |D|)e^{-|\nu(\lambda)|X_{i-1}} \\
&+ e^{-\alpha X_{i-1}} e^{-|\nu(\lambda)|X_{i-1}} |\lambda|^2 \Big) |d| \\
&\leq C \Big( e^{-\tilde{\alpha} X_{i-1}} |\lambda|^3 + (e^{-\tilde{\alpha} X_1} + e^{-\tilde{\alpha} X_2} )|\lambda| |D| \Big) |d| \\
&\leq C |\lambda| (e^{-\tilde{\alpha} X_1} + e^{-\tilde{\alpha} X_2}) (|\lambda|^2 + |D| ) |d|
\end{align*}

The other integral has the same bound.

\end{proof}
\end{lemma}

% lemma : center not involving H

Finally, we look at the ``center'' integral term not involving $W$.

\begin{lemma}

\begin{align*}
\left| \int_{-X_{i-1}}^0
e^{\nu(\lambda)y} \langle \Psi(0), v_-(0; \beta_i^-, \lambda) \rangle \langle \lambda^2 d_i \tilde{H}(y), w_-(y; \beta_i^-, \lambda) \rangle dy \right| &\leq C (|\lambda| + e^{-2 \alpha X_1} + e^{-2 \alpha X_2} ) |\lambda|^2 |d|\\
\left| \int_{X_i}^0
e^{\nu(\lambda)y} \langle \Psi(0), v_+(0; \beta_i^+, \lambda) \rangle \langle \lambda^2 d_i \tilde{H}(y), w_+(y; \beta_i^+,\lambda) \rangle dy \right| &\leq C (|\lambda| + e^{-2 \alpha X_1} + e^{-2 \alpha X_2} ) |\lambda|^2  |d| \\
\end{align*}

\begin{proof}

For the ``minus'' integral, we have

\begin{align*}
\langle \Psi(0) &, \int_{-X_{i-1}}^0
e^{\nu(\lambda)y} v_-(0; \beta_i^-, \lambda) \langle \lambda^2 d_i \tilde{H}(y), w_-(y; \beta_i^-, \lambda) \rangle dy \rangle \\
&= \int_{-X_{i-1}}^0
e^{\nu(\lambda)y} \langle \Psi(0), v_-(0; \beta_i^-, \lambda) \rangle \langle \lambda^2 d_i \tilde{H}(y), w_-(y; \beta_i^-, \lambda) \rangle dy 
\end{align*}

From Lemma \ref{jumpc}, $\Psi(0), v_-(0; \lambda) \rangle| = \mathcal{O}(|\lambda| + e^{-2 \alpha X_1} + e^{-2 \alpha X_2} )$. Thus we have

\begin{align*}
&\left| \int_{-X_{i-1}}^0
e^{\nu(\lambda)y} \langle \Psi(0), v_-(0; \lambda) \rangle \langle \lambda^2 d_i \tilde{H}(y), w_-(y; \lambda) \rangle dy \right| \\
&\leq C (|\lambda| + e^{-2 \alpha X_1} + e^{-2 \alpha X_2} ) |\lambda|^2 |d| 
\int_{-X_{i-1}}^0 e^{\nu(\lambda)y} |\tilde{H}(y) dy|
\end{align*}

Since we know that $\tilde{H}(y)$ decays exponentially with rate $\alpha$, this becomes

\begin{align*}
&\left| \int_{-X_{i-1}}^0
e^{\nu(\lambda)y} \langle \Psi(0), v_-(0; \beta_i^-, \lambda) \rangle \langle \lambda^2 d_i \tilde{H}(y), w_-(y; \beta_i^-, \lambda) \rangle dy \right| \\
&\leq C (|\lambda| + e^{-2 \alpha X_1} + e^{-2 \alpha X_2} ) |\lambda|^2  |d| 
\int_{-X_{i-1}}^0 e^{(\alpha - |\nu(\lambda)|)y} dy \\
&\leq C (|\lambda| + e^{-2 \alpha X_1} + e^{-2 \alpha X_2} ) |\lambda|^2  |d|
\end{align*}

The ``plus'' integral is similar.

\end{proof} 
\end{lemma}

We are now ready to put all of this together.

% lemma : jump expression

\begin{lemma}\label{jump}

We have the following expression for the jumps 

\[
\xi_i = \langle \Psi(0), W_i^+(0) - W_i^-(0) \rangle
\]
at $x = 0$.

\begin{equation}\label{jumpexp}
\xi_i = \langle \Psi(X_i), P^u_0 D_i d \rangle + \langle \Psi(-X_{i-1}), P^u_0 D_{i-1} d \rangle - d_i \lambda^2 \int_{-\infty}^\infty \langle \Psi(y), H(y) \rangle dy + R(\lambda)_i(d)
\end{equation}

where the remainder term $R(\lambda)_i(d)$ has bound

\begin{align*}
R(\lambda)(d)_i &\leq C \Big( ( e^{-\alpha X_1} + e^{-\alpha X_2} + p_5(\lambda) + |\lambda| )(|\lambda|^2 + (e^{-\tilde{\alpha}X_1} + e^{-\tilde{\alpha}X_2}) |D|\Big) |d|
\end{align*}

\begin{proof}
Using the fixed point equations at $x = 0$, we evaluate $\langle \Psi(0), W_i^+(0) - W_i^-(0) \rangle$. The first two terms on the RHS of \eqref{jumpexp} come from Lemma \ref{jumpa}. The higher order Melnikov integral term comes from Lemma \ref{noncenterH}. For the remainder term, we use all the jump estimation lemmas above to get the estimate

\begin{align*}
|R&(\lambda)(d)_i| \leq C \Big( (e^{-\alpha X_1} + e^{-\alpha X_2})
( |\lambda| + e^{-\tilde{\alpha} X_1} + e^{-\tilde{\alpha} X_2}) |D| ) |d| \\
&+ p_5(\lambda) ( (e^{-\tilde{\alpha} X_1} + e^{-\tilde{\alpha} X_2}) |D| + |\lambda|^2 )|d| \\
&+ (|\lambda| + e^{-2 \alpha X_1} + e^{-2 \alpha X_2})( (e^{-\tilde{\alpha} X_1} + e^{-\tilde{\alpha} X_2}) |D| + |\lambda|^2 )|d| \\
&+ ( (e^{-\alpha X_1} + e^{-\alpha X_2}) |\lambda|^2 + (e^{-(\alpha + \tilde{\alpha}) X_1} + e^{-(\alpha + \tilde{\alpha}) X_2}) |D|) |d| \\
&+ ( |\lambda| + e^{-\alpha X_1} + e^{-\alpha X_2}) |\lambda|^2 |d| \\
&+ |\lambda| ( e^{-\tilde{\alpha} X_1} + e^{-\tilde{\alpha} X_2})(|\lambda|^2 + |D|) |d| \\
&+ (|\lambda| + e^{-2 \alpha X_1} + e^{-2 \alpha X_2} ) |\lambda|^2  |d|
\end{align*}

Simplifying and dropping higher order terms, this becomes

\begin{align*}
R(\lambda)(d)_i &\leq C \Big( ( e^{-\alpha X_1} + e^{-\alpha X_2} + p_5(\lambda) + |\lambda|  )(|\lambda|^2 + (e^{-\tilde{\alpha}X_1} + e^{-\tilde{\alpha}X_2}) |D|\Big) |d|
\end{align*}

\end{proof}
\end{lemma}

We are now ready to substitute in all the things to get something we can solve for the eigenvalues $\lambda$.

\begin{theorem}

The eigenvalue problem has a solution for nonzero eigenvalue $\lambda$ if and only if

\begin{equation}\label{matrixdet}
E(\lambda) = \det S(\lambda) = \det(A - \lambda^2 MI + R(\lambda) ) = 0
\end{equation}

where $A$ is the matrix defined by

\[
A = 
\begin{pmatrix}
-a_1 + \tilde{a}_2 & a_1 - \tilde{a}_2 \\
-\tilde{a}_1 + a_2 & \tilde{a}_1 - a_2 
\end{pmatrix}
\]

The entries of the matrix $A$ are given by
\begin{align*}
a_i &= \langle \Psi (X_i), Q'(-X_i) \rangle \\
\tilde{a}_i &= \langle \Psi(-X_i), Q'(X_i) \rangle
\end{align*}

and the remainder $R(\lambda)$ has bound

\begin{align*}
|R(\lambda)(d)| &\leq C \Big( e^{-(\alpha + \tilde{\alpha})X_1}( e^{-\alpha X_1} + |\lambda|  )  
+ (e^{-\alpha X_1} + |\lambda| ) |\lambda|^2 \Big) |d|
\end{align*}

\begin{proof}

First, we plug in $D_i$ and $D_{i-1}$ into \eqref{jumpexp} from the previous lemma. 
Recall that we have the following expression for $D_i$ from Hypothesis \ref{problembounds}.

\[
D_i = ( Q'(X_i) + Q'(-X_i))(d_{i+1} - d_i ) + \mathcal{O} \left( e^{-\alpha X_i} \left( |\lambda| +  e^{-\alpha X_i}  \right) |d| \right)
\]

Substituting this into $\langle \Psi(X_i), P^u_0 D_i d \rangle$ and using the fact that $\Psi(\pm X_i)$ is order $e^{-\alpha X_i}$, we have

\begin{align*}
\langle \Psi(X_i), P^u_0 D_i d \rangle &= \langle \Psi(X_i), (Q'(X_i) + Q'(-X_i)(d_{i+1} - d_i ) \rangle + \mathcal{O} \left( e^{-2 \alpha X_i} \left( |\lambda| +  e^{-\alpha X_i}  \right) |d| \right)
\end{align*}

Now we assume what was done in (3.36) in San98 applies here, i.e. that the ``matching'' terms $\langle \Psi(X_i), Q'(X_i) \rangle$ and $\langle \Psi(-X_i), Q'(-X_i) \rangle$ are higher order (say $\mathcal{O}(e^{-3 \alpha X_i}$) and can be tossed into the remainder. Thus we are only left with the ``nonmatching'' terms $\langle \Psi(X_i), Q'(-X_i) \rangle$ and similar. WE DO NEED TO VERIFY THIS. Then we get

\begin{align*}
\langle \Psi(X_i), P^u_0 D_i d \rangle &= \langle \Psi(X_i), Q'(-X_i) \rangle (d_{i+1} - d_i ) + \mathcal{O} \left( e^{-2 \alpha X_i} \left( |\lambda| +  e^{-\alpha X_i}  \right) |d| \right)
\end{align*}

Similarly, we have

\begin{align*}
\langle \Psi(-X_{i-1}), P^u_0 D_{i-1} d \rangle &= \langle \Psi(-X_{i-1}), Q'(X_{i-1}) \rangle (d_i - d_{i-1} ) + \mathcal{O} \left( e^{-2 \alpha X_{i-1}} \left( |\lambda| +  e^{-\alpha X_{i-1}}  \right) |d| \right)
\end{align*}

Thus the jumps at $x = 0$ are given by

\begin{align*}
\xi_i &= \langle \Psi(0), W_i^+(0) - W_i^-(0) \rangle \\
&= \langle \Psi(X_i), Q'(-X_i) \rangle (d_{i+1} - d_i ) + \langle \Psi(-X_{i-1}), Q'(X_{i-1}) \rangle (d_i - d_{i-1} ) - \lambda^2 d_i M + R(\lambda)(d)_i
\end{align*}

where $M$ is the higher order Melnikov integral

\[
M = \int_{-\infty}^\infty \langle \Psi(y), H(y) \rangle dy
\]

The bound for the remainder term $R(\lambda)(d)_i$ in this expression is given by the bound for the remainder term from Lemma \ref{jump} together with the additional remainder terms from the substitution of $D_i$.

\begin{align*}
R(\lambda)(d)_i &\leq C \Big( ( e^{-\alpha X_1} + e^{-\alpha X_2} + p_5(\lambda) + |\lambda|  )(|\lambda|^2 + (e^{-\tilde{\alpha}X_1} + e^{-\tilde{\alpha}X_2}) |D|) \\
&+ e^{-2 \alpha X_1}(|\lambda| + e^{-\alpha X_1}) 
+ e^{-2 \alpha X_2}(|\lambda| + e^{-\alpha X_2}) \Big) |d|
\end{align*}

For the the final remainder bound, we substitute the following estimates from prior lemmas and hypothesis.

\begin{align*}
|D| &\leq C (e^{-\alpha X_1} + e^{-\alpha X_2}) |d| \\
|p_5(\lambda)| &\leq C( e^{-2 \alpha X_1} + e^{-2 \alpha X_2} + |\lambda|)
\end{align*}

Thus we have

\begin{align*}
R(\lambda)(d)_i &\leq C \Big( ( e^{-\alpha X_1} + e^{-\alpha X_2} + |\lambda|  )(|\lambda|^2 + (e^{-\tilde{\alpha}X_1} + e^{-\tilde{\alpha}X_2}) (e^{-\alpha X_1} + e^{-\alpha X_2})) \\
&+ e^{-2 \alpha X_1}(|\lambda| + e^{-\alpha X_1}) 
+ e^{-2 \alpha X_2}(|\lambda| + e^{-\alpha X_2}) \Big) |d|
\end{align*}

Simplifying this and dropping higher order terms, the bound becomes

\begin{align*}
|R(\lambda)(d)_i| &\leq C \Big( (e^{-\alpha X_1} + e^{-\alpha X_2} + |\lambda| )(e^{-(\alpha + \tilde{\alpha})X_1} + e^{-(\alpha + \tilde{\alpha})X_2} 
+ |\lambda|^2 \Big) |d|
\end{align*}

Since we are taking $\lambda = \mathcal{O}(e^{-\alpha X_1} + e^{-\alpha X_2})$, this remainder bound is sufficient!\\

Taking $i = 1, 2$, the jump expressions become

\begin{align*}
\xi_1 &= \langle \Psi(X_1), Q'(-X_1) \rangle (d_2 - d_1 ) + \langle \Psi(-X_2), Q'(X_2) \rangle (d_1 - d_2 ) + \lambda^2 d_1 M + R(\lambda)(d)_1 \\
\xi_2 &= \langle \Psi(X_2), Q'(-X_2) \rangle (d_1 - d_2 ) + \langle \Psi(-X_1), Q'(X_1) \rangle (d_2 - d_1 ) + \lambda^2 d_2 M + R(\lambda)(d)_2
\end{align*}

Next, we define

\begin{align*}
a_i &= \langle \Psi (X_i), Q'(-X_i) \rangle \\
\tilde{a}_i &= \langle \Psi(-X_i), Q'(X_i) \rangle
\end{align*}

Finally, we write our jump equations in matrix form.

\[
E(\lambda) = \det S(\lambda) = \det(A - \lambda^2 MI + R(\lambda) ) = 0
\]

where $A$ is the matrix defined by

\[
A = 
\begin{pmatrix}
-a_1 + \tilde{a}_2 & a_1 - \tilde{a}_2 \\
-\tilde{a}_1 + a_2 & \tilde{a}_1 - a_2 
\end{pmatrix}
\]

Thus we have a solution for nonzero eigenvalue $\lambda$ if and only if $E(\lambda) = 0$.

\end{proof}
\end{theorem}

In our specific case, we can actually compute leading order expressions for the eigenvalues

\begin{corollary}
To leading order, the nonzero eigenvalues are given by

\begin{equation}
\lambda = \pm \sqrt{-2a/M}
\end{equation}

where $M$ is the higher order Melnikov integral and 

\begin{equation}
a = \langle \Psi (X_1), Q'(-X_1) \rangle 
+ \langle \Psi (X_2), Q'(-X_2) \rangle 
\end{equation}

\begin{proof}

As in the exponentially weighted case, we can use the (known) evenness and oddness of the functions involved to get

\[
\tilde{a}_i = \langle \Psi(-X_i), Q'(X_i) \rangle = -\langle \Psi(X_i), Q'(-X_i) \rangle = -a_i
\]

Thus the matrix $A$ becomes

\[
A = 
\begin{pmatrix}
-a_1 - a_2 & a_1 + a_2 \\
a_1 + a_2 & -a_1 - a_2 
\end{pmatrix}
\]

Let $a = a_1 + a_2$ and redefine $A$ by

\[
A = 
\begin{pmatrix}
-1 & 1 \\
1 & -1
\end{pmatrix}
\]

then we have the matrix equation

\[
E(\lambda) = \det S(\lambda) = \det(a A - \lambda^2 MI + R(\lambda) ) = 0
\]

Thus, to leading order we are computing the determinant of $a A + \lambda^2 MI$, i.e. 

\[
a A - \lambda^2 MI = 
\begin{pmatrix}
-a - \lambda^2 M & a \\
a & -a - \lambda^2 M
\end{pmatrix}
\]

The characteristic polynomial for this is

\begin{align*}
0 &= (-a - \lambda^2 M)^2 - a^2 \\
&= a^2 + 2 a \lambda^2 M + \lambda^4 M^2 - a^2 \\
&= 2 a \lambda^2 M + \lambda^4 M^2 \\
&= \lambda^2 M (2a + M \lambda^2 )
\end{align*}

This has two roots at 0 (which are expected) as well as two roots at $\pm \sqrt{-2a/M}$. These will be purely imaginary or real depending on the sign of $a$.

\end{proof}
\end{corollary}

\end{document}