% \documentclass{book}

\documentclass[12pt]{article}
\usepackage[pdfborder={0 0 0.5 [3 2]}]{hyperref}%
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}%
\usepackage[shortalphabetic]{amsrefs}%
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{amssymb}                
\usepackage{amsmath}                
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage[table,xcdraw]{xcolor}
\usepackage{tikz}
\usepackage{float}
\usepackage{booktabs}
\usepackage{svg}
\usepackage{mathtools}
\usepackage{cool}
\usepackage{url}
\usepackage{graphicx,epsfig}
\usepackage{makecell}
\usepackage{array}

\def\noi{\noindent}
\def\T{{\mathbb T}}
\def\R{{\mathbb R}}
\def\N{{\mathbb N}}
\def\C{{\mathbb C}}
\def\Z{{\mathbb Z}}
\def\P{{\mathbb P}}
\def\E{{\mathbb E}}
\def\Q{\mathbb{Q}}
\def\ind{{\mathbb I}}

\graphicspath{ {images17/} }

\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}

\begin{document}

\section*{24 August 2017}

\subsection*{Eigenfunction symmetry}

In \texttt{KdV11} we looked at even/odd symmetry of eigenfunctions for the case where we have a purely imaginary eigenvalue. Here we look at the general case. The linearized 5th order problem about the stationary solution $u^*(x)$ is $Lv = \partial_x Hv = \lambda v$, where $L = \partial_x( \partial_x^4 - \partial_x^2 + c - 2u^*)$. We want to consider the eigenvalue problem $Lv(x) = \lambda v(x)$. \\

Suppose we have an eigenfunction $v(x)$ of $L$ with corresponding eigenvalue $\lambda$. The following have already been shown.
\begin{enumerate}
	\item $\bar{v}(x)$ is an eigenfunction of $L$ corresponding to eigenvalue $\bar{\lambda}$:
	\item Assuming that the stationary solution $u^*(x)$ is an even function (which it is for our case), $v(-x)$ is an eigenfunction of $L$ corresponding to eigenvalue $-\lambda$.
\end{enumerate}
Write the eigenfunction as $v(x) = u(x) + i w(x)$, where $u(x)$ and $w(x)$ are both real. Also write our eigenvalue as $\lambda = \alpha + i \beta$. If we substitute these into the eigenvalue problem and equate real/imaginary parts, we get
\begin{align*}
Lu &= \alpha u - \beta w \\
Lw &= \alpha w + \beta u
\end{align*}
We considered before the special case where $\alpha = 0$. The real and imaginary parts of $v$ are related. One relationship is given by
\[
w = -\frac{1}{\beta}(Lu - \alpha u)
\]

\subsection*{Integrated Eigenfunction Construction (revised yet again)}
For the 5th order KdV equation (written in traveling frame), assume for a specific value of $c$ (which we know is greater than 1/4) we have constructed a 2-pulse $q_2(x)$. The 1-pulse is given by $q(x)$. Then we can find a real number $L$ so that we can write $q_2(x)$ piecewise as:

\begin{equation}\label{q2piecewise}
\begin{cases}
q(x) + r_1^-(x) & \text{on } (-\infty, 0] \\
q(x) + r_1^+(x) & \text{on } [0, L] \\
q(x) + r_2^-(x) & \text{on } [-L, 0] \\
q(x) + r_2^+(x) & \text{on } [0, \infty) \\ 
\end{cases}
\end{equation}

where the two pieces are spliced together one after the other so that $\pm L$ corresponds to 0 and the resulting function (and all derivatives) are continuous at the splice point. Note that this a special case of Theorem 1 in Sandstede (1998), since we are not varying a parameter to break a homoclinic orbit, but rather are perturbing that homoclinic orbit directly. Thus the two pieces $q^-$ and $q^+$ from Sandstede (1998) are both the homoclinic orbit $q(x)$. Although we are not breaking the orbit, we still need to perform joints at 0.
\\

We have the matching conditions

\begin{align*}
q(0) + r_1^-(0) &= q(0) + r_1^+(0) \\
q(L) + r_1^+(L) &= q(-L) + r_2^-(-L) \\
q(0) + r_2^-(0) &= q(0) + r_2^+(0)
\end{align*}

Since $q(x)$ is even, $q(L) = q(-L)$, and this simplifes to

\begin{align}\label{q2match}
r_1^-(0) &= r_1^+(0) \\
r_1^+(L) &= r_2^-(-L) \\
r_2^-(0) &= r_2^+(0)
\end{align}


Consider the eigenvalue problem as before. Linearizing about the solution $q_2(x)$ we get the eigenvalue problem $\partial_x H v = \lambda v$, where 
\begin{equation}\label{hamiltonian}
H = \partial_x^4 - \partial_x^2 + c - 2 q_2(x)
\end{equation}

Now integrate both sides of the eigenvalue problem from $a$ to $x$. This is the integrated eigenvalue problem. The value of $a$ will be determined later. We will assume that any solution $v$ is localized, i.e. we will seek solutions $v(x)$ such that the function and all its derivatives decay to 0 exponentially as $x \rightarrow \pm \infty$. There's probably a nice function space with this property, but we won't worry about that for now. Thus the integrated eigenvalue problem becomes

\begin{equation}\label{inteigproblem}
(Hv)(x) - (Hv)(a) = \lambda \int_{a}^x v(y) dy
\end{equation}

Asssuming that $v(x)$ has the decay properties we mentioned above, if we take $a = -\infty$ and let $x \rightarrow \infty$ (using the DCT on the integral on the RHS, since $v$ is integrable), we get

\[
\lambda \int_{-\infty}^\infty v(y) dy = 0
\]
So the eigenfunction has mean 0 if the eigenvalue is nonzero. As per our discussion in the periodic case, I don't think this gets us anything new.\\

We want to write this as a first-order system, but before we do that, we will split up the eigenfunction $v(x)$ into pieces similar to what we did with $q_2(x)$. The idea here is that we can choose a different lower limit of integration for each piece and want to do this in a way that minimizes the notational mess. We will use the same piecewise domain as we did with $q_2(x)$. The derivative of the integrals will be correct on each piece, and if we have the appropriate matching condition, we will be all set. \\

We split up $v(x)$ as follows:

\begin{equation}\label{splitv}
\begin{cases}
v_1^-(x) & \text{on } (-\infty, 0]  \\
v_1^+(x) & \text{on } [0, L]  \\
v_2^-(x) & \text{on } [-L, 0] \\
v_2^+(x) & \text{on } [0, \infty) 
\end{cases}
\end{equation}

where we have similar matching conditions to before:

\begin{align}\label{match}
v_1^-(0) &= v_1^+(0) \\
v_1^+(L) &= v_2^-(-L) \\
v_2^-(0) &= v_2^+(0)
\end{align}

Now we write the integrated eigenvalue problem for each piece. Since we seek solutions which decay at $\pm \infty$, we will use those as the endpoints $a$ for two of the pieces, so $(Hv)(a) = 0$ in those cases. For the other two, we will integrate from $\pm L$. (We could also try integrating from 0, but this will lead to less cancellation.)

\begin{equation}
\begin{cases}
(Hv_1^-)(x) = \lambda \int_{-\infty}^x v_1^-(y) dy & x \in (-\infty, L] \\
(Hv_1^+)(x) = (Hv_1^+)(L) + \lambda \int_{L}^x v_1^+(y) dy & x \in [0, L] \\
(Hv_2^-)(x) = (Hv_2^-)(-L) + \lambda \int_{-L}^x v_2^-(y) dy & x \in [-L, 0] \\ 
(Hv_2^+)(x) = \lambda \int_{\infty}^x v_2^+(y) dy & x \in [-L, \infty) \\ 
\end{cases}
\end{equation}

We want to write these piecewise equations as a first-order system. The terms $(Hv_1^+)(L)$ and $(Hv_2^-)(-L)$ are annoying boundary terms, but there is nothing we can do about them for now, so we will work them into our system as best we can.\\

The idea is to get these four equations to ``look the same''. With that in mind, we define the integration endpoints by

\[
(a_1^-, a_1^+, a_2^-, a_2^+) = (-\infty, L, -L, \infty)
\]

We then can write the four equations in the form

\begin{equation}
(Hv_i^\pm)(x) = (Hv_i^\pm)(a_i^\pm) + \lambda \int_{a_i^\pm}^x v_i^{\pm} dy
\end{equation}

We would like to write this as a first-order system. Before we do that, it is helpful to expand $H$.

\begin{align*}
\partial_x^4 v_i^\pm(x) - \partial_x^2 v_i^\pm(x) + (c - 2 q_2(x))v_i^\pm(x) &= \partial_x^4 v_i^\pm(a_i^\pm) - \partial_x^2 v_i^\pm(a_i^\pm) + (c - 2 q_2(a_i^\pm))v_i^\pm(a_i^\pm) + \lambda \int_{a_i^\pm}^x v_i^{\pm} dy
\end{align*}

We can move everything not involving 4th derivatives to the RHS.


\begin{align*}
\partial_x^4 v_i^\pm(x) - \partial_x^4 v_i^\pm(a_i^\pm) &= [\partial_x^2 v_i^\pm(x) - \partial_x^2 v_i^\pm(a_i^\pm)] + [(2 q_2(x) - c )v_i^\pm(x) - (2 q_2(a_i^\pm) - c)v_i^\pm(a_i^\pm)] + \lambda \int_{a_i^\pm}^x v_i^{\pm} dy
\end{align*}

\begin{align*}
\begin{pmatrix}v_i^\pm(x)\\(v_i^\pm)_x(x)\\(v_i^\pm)_{xx}(x)\\(v_i^\pm)_{xxx}(x)\end{pmatrix}_x &- \begin{pmatrix}(v_i^\pm)_x(a_i^\pm)\\(v_i^\pm)_{xx}(a_i^\pm)\\(v_i^\pm)_{xxx}(a_i^\pm)\\(v_i^\pm)_{xxxx}(a_i^\pm)\end{pmatrix}
= \begin{pmatrix}0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\ 2q_2(x) - c & 0 & 1 & 0\end{pmatrix}
\begin{pmatrix}v_i^\pm(x) \\(v_i)_x(x) \\(v_i^\pm)_{xx}(x) \\(v_i^\pm)_{xxx}(x) \end{pmatrix}\\
&-\begin{pmatrix}0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\ 2q_2(a_i^\pm) - c & 0 & 1 & 0\end{pmatrix}
\begin{pmatrix}v_i^\pm(a_i^\pm) \\(v_i)_x(a_i^\pm) \\(v_i^\pm)_{xx}(a_i^\pm) \\(v_i^\pm)_{xxx}(a_i^\pm)\end{pmatrix} + \lambda 
\begin{pmatrix}0\\0\\0\\ \int_{a_i^\pm}^x v_i^\pm(y) dy\end{pmatrix}
\end{align*}

In matrix form, this becomes
\[
(V_i^\pm)'(x) - (V_i^\pm)'(a_i^\pm) = A(q_2(x)) V_i^\pm(x) - A(q_2(a_i^\pm)) V_i^\pm(a_i^\pm) + \lambda (K_i^\pm B V_i^\pm)(x)
\]

where $V_i^\pm = (v_i^\pm, (v_i^\pm)_x, (v_i^\pm)_{xx}, (v_i^\pm)_{xxx})^T$ and $A(q_2)$ is the first matrix on the RHS above.

$B$ is the matrix
\[
\begin{pmatrix}0 & 0 & 0 & 0 \\0 & 0 & 0 & 0 \\0 & 0 & 0 & 0 \\1 & 0 & 0 & 0 \end{pmatrix}
\]

which moves the first component to fourth and gets rid of everything else. In particular, $B V_i^\pm = (0, 0,0, v_i^\pm)^T$, which is the context in which we will use it.\\

The $K_i^\pm$ are integration operators, which are given by:

\begin{align*}
(K_i^\pm U)(x) = \int_{a_i^\pm}^x U(y) dy
\end{align*}
where the integration on $U$ is carried out component-wise (doesn't really matter since $B$ makes all but one component 0).\\

The matching conditions are:

\begin{align*}
V_i^-(0) &= V_i^+(0) \\
V_1^+(L) &= V_2^-(-L)
\end{align*}

which ensure that matching occurs up to the third derivative.\\

Now we recall that the derivative of the double pulse $Q_2'(x)$ is an eigenfunction with eigenvalue 0, so for small $\lambda$, our eigenfunction should be a small perturbation of this. As in (3.5) in Sandstede (1998), we write

\begin{align}
V_i^\pm(x) &= d_i Q_2'(x) + W_i^\pm(x) = d_i (Q'(x) + (R_i^\pm)'(x)) + W_i^\pm(x) && d_i \in \C
\end{align}

Plug this into the integrated eigenvalue problem to get equations for the $W_i$. We use the fact that $Q_2'$ solves the original eigenvalue problem with $\lambda = 0$. We have removed the dependence on $x$ for convenience

\begin{align*}
[d_i Q_2'(x) + W_i^\pm(x)'] &- [d_i Q_2'(a_i^\pm) + W_i^\pm(a_i^\pm)'] = A(q_2(x)) (d_i Q_2'(x) + W_i^\pm(x)) \\
&- A(q_2(a_i^\pm)) (d_i Q_2'(a_i^\pm) + W_i^\pm(a_i^\pm)) + \lambda [K_i^\pm B (d_i Q_2' + W_i^\pm)](x)\\
\end{align*}

First, use the fact that $Q_2'$ solves the original eigenvalue problem with $\lambda = 0$ to get

\begin{align*}
W_i^\pm(x)' &- W_i^\pm(a_i^\pm)' = A(q(x) + r_i^\pm(x)) W_i^\pm(x)) - A(q(a_i^\pm) + r_i^\pm(a_i^\pm)) W_i^\pm(a_i^\pm)) \\
&+ \lambda [K_i^\pm B (d_i Q_2' + W_i^\pm)](x) \\
\end{align*}

This is annoying enough that we will deal with each term in turn. First, let
\[
G_i^\pm(x) = A(q(x) + r_i^\pm(x)) - A(q(x))
\]
Then we can write

\begin{align*}
W_i^\pm(x)' &- W_i^\pm(a_i^\pm)' = A(q(x)) W_i^\pm(x) + G_i^\pm(x) W_i^\pm(x)
 - [A(q(a_i^\pm)) W_i^\pm(a_i^\pm)) + G_i^\pm(a_i^\pm) W_i^\pm(a_i^\pm)] \\
&+ \lambda [K_i^\pm B (d_i Q_2' + W_i^\pm)](x) \\
\end{align*}

Next we can integrate the derivative $Q_2'$. By the fundamental theorem of calculus, we have

\begin{align*}
(K_i^\pm B Q_2')(x) &= B( Q_2(x) - Q_2(a_i^\pm) )
\end{align*}

Substituting, this in, we get

\begin{align*}
W_i^\pm(x)' &- W_i^\pm(a_i^\pm)' = A(q(x)) W_i^\pm(x) + G_i^\pm(x) W_i^\pm(x)
 - [A(q(a_i^\pm)) W_i^\pm(a_i^\pm) + G_i^\pm(a_i^\pm) W_i^\pm(a_i^\pm)] \\
&+ \lambda d_i B[ Q_2(x) - Q_2(a_i^\pm) ] + \lambda K_i^\pm B W_i^\pm(x)
\end{align*}

We have matching conditions
\begin{align*}
W_i^-(0) &= W_i^+(0) \\
W_1^+(L) - W_2^-(-L) &= D_1 d
\end{align*}

where

\begin{align*}
D_1 d &= d_2 Q_2'(-L) - d_1 Q_2'(L)\\
&= d_2 [ Q'(-L) + R_2'(-L)] - d_1 [ Q'(L) + R_1'(L) ]
\end{align*}

So we have our system

\begin{align*}
W_i^\pm(x)' &- W_i^\pm(a_i^\pm)' = [A(q(x)) W_i^\pm(x) + G_i^\pm(x) W_i^\pm(x)]
 - [A(q(a_i^\pm)) W_i^\pm(a_i^\pm) + G_i^\pm(a_i^\pm) W_i^\pm(a_i^\pm)] \\
&+ \lambda d_i B[ Q_2(x) - Q_2(a_i^\pm) ] + \lambda K_i^\pm B W_i^\pm(x) \\
W_i^-(0) &= W_i^+(0) \\
W_1^+(L) - W_2^-(-L) &= D_1 d \\
W_i^\pm(x) &\in \C \psi(0) \oplus Y^+ \oplus Y^- \\
W_i^+(0) - W_i^-(0) &\in \C \psi(0) 
\end{align*}

where
\begin{align*}
G_i^\pm(x) &= A(q(x) + r_i^\pm(x)) - A(q(x)) \\
D_1 d &= d_2 [ Q'(-L) + R_2'(-L)] - d_1 [ Q'(L) + R_1'(L) ]
\end{align*}

The only term in $W_i^\pm(a_i^\pm)'$ which really matters is the fourth component (the 4th derivative). All the other components cancel something on the RHS. I'm not sure if this the right idea or not. We could move this term to the RHS and call it a constant $C_i^\pm$, but I see no way to get rid of it if we are setting up the problem this way.

\subsection*{Alternate Setups}


I can think of at least two alternate setups. (Spoilers: they are actually the same.) One of these may be more useful that what we have above.

\subsubsection*{Use the other limit for integral of center pieces}

Here, all we do is change the lower limit of integration to

\[
(a_1^-, a_1^+, a_2^-, a_2^+) = (-\infty, 0, 0, \infty)
\]

The system looks exactly like we have above, but we can use the matching condition to deal with the boundary terms remaining on the LHS. As an example, we write out the first two pieces explicitly. For the first piece, the boundary terms disappear since all functions in the function space in question decay at $\pm \infty$

\begin{align*}
W_1^-(x)' &= A(q(x)) W_1^-(x) + G_1^-(x) W_1^-(x) + \lambda d_1 B Q_2(x) + \lambda (K_1^- B W_1^-)(x) \\
W_1^+(x)' &= W_1^+(0)' + [A(q(x)) W_1^+(x) + G_1^+(x) W_1^+(x)]
 - [A(q(0)) W_1^+(0) + G_1^+(0) W_1^+(0)] \\
&+ \lambda d_1 B[ Q_2(x) - Q_2(0) ] + \lambda (K_1^+ B W_1^+)(x)
\end{align*}

Let's write $W_1^+(0)'$ as a constant $C_1^+$. From the matching condition, we know that $W_1^-(0) = W_1^+(0)$. The same should hold for their derivatives (or at least we want it to!), so our half-system becomes:

\begin{align*}
W_1^-(x)' &= A(q(x)) W_1^-(x) + G_1^-(x) W_1^-(x) + \lambda d_1 B Q_2(x) + \lambda (K_1^- B W_1^-)(x) \\
W_1^+(x)' &= C_1^+ + [A(q(x)) W_1^+(x) + G_1^+(x) W_1^+(x)] - [A(q(0)) W_1^+(0) + G_1^+(0) W_1^+(0)] \\
&+ \lambda d_1 B[ Q_2(x) - Q_2(0) ] + \lambda (K_1^+ B W_1^+)(x) \\
C_1^+ &= W_1^-(0)'
\end{align*}

If we want, we can actualy make the substitution and see what happens. Using the matching conditions, this is what we get.

\begin{align*}
W_1^+(x)' &= A(q(0)) W_1^-(0) + G_1^-(0) W_1^-(0) + \lambda d_1 B Q_2(0) + \lambda (K_1^- B W_1^-)(0) \\
&+ [A(q(x)) W_1^+(x) + G_1^+(x) W_1^+(x)] - [A(q(0)) W_1^+(0) + G_1^+(0) W_1^+(0)] \\
&+ \lambda d_1 B[ Q_2(x) - Q_2(0) ] + \lambda (K_1^+ B W_1^+)(x) \\
&= [A(q(0)) W_1^+(0) + G_1^+(0) W_1^+(0)] + \lambda d_1 B Q_2(0) + \lambda (K_1^- B W_1^-)(0) \\
&+ [A(q(x)) W_1^+(x) + G_1^+(x) W_1^+(x)] - [A(q(0)) W_1^+(0) + G_1^+(0) W_1^+(0)] \\
&+ \lambda d_1 B[ Q_2(x) - Q_2(0) ] + \lambda (K_1^+ B W_1^+)(x) \\ 
\end{align*}

We get lots of cancellation, so the the end, our system becomes
\begin{align*}
W_1^-(x)' &= A(q(x)) W_1^-(x) + G_1^-(x) W_1^-(x) + \lambda d_1 B Q_2(x) + \lambda (K_1^- B W_1^-)(x) \\
W_1^+(x)' &= A(q(x)) W_1^+(x) + G_1^+(x) W_1^+(x) + \lambda d_1 B Q_2(x) + \lambda (K_1^+ B W_1^+)(x) + \lambda (K_1^- B W_1^-)(0)  
\end{align*}

Using symmetry (for the other end), we can complete the system as follows.

\begin{equation}
W_i^\pm(x)' = A(q(x)) W_i^\pm(x) + G_i^\pm(x) W_i^\pm(x) + \lambda d_1 B Q_2(x) + \lambda (K_i^\pm B )W_i^\pm(x) + \lambda C_i^\pm
\end{equation}

where 
\begin{align*}
C_1^- &= 0 \\
C_1^+ &= (K_1^- B W_1^-)(0) \\
C_2^- &= (K_1^- B W_2^+)(0) \\
C_2^+ &= 0
\end{align*}

So if we make the substitution, we have an extra integral term in two of the equations, but the extra term is fixed. It depends on integration of a different piece, but it does not depend on $x$. Whether this is easy to bound/deal with remains to be seen.

\subsubsection*{Always integrate from the ends}

Here we integrate the two left pieces from $-\infty$ and the two right pices from $\infty$. The main drawback is that now we have the center two pieces contain an integral from the outer two pieces. The main advantage is that the boundary term dies. Tradeoffs. I won't go through all the gory detail again, but the system should look like this:

\begin{equation}
\begin{cases}
(Hv_1^-)(x) = \lambda \int_{-\infty}^x v_1^-(y) dy & x \in (-\infty, L] \\
(Hv_1^+)(x) = \lambda \int_{-\infty}^0 v_1^-(y) dy + \lambda \int_{0}^x v_1^+(y) dy & x \in [0, L] \\
(Hv_2^-)(x) = \lambda \int_{\infty}^0 v_2^+(y) + \lambda \int_{0}^x v_2^-(y) dy & x \in [-L, 0] \\ 
(Hv_2^+)(x) = \lambda \int_{\infty}^x v_2^+(y) dy & x \in [-L, \infty) \\ 
\end{cases}
\end{equation}

The lower limits of integration here are 

\[
(a_1^-, a_1^+, a_2^-, a_2^+) = (-\infty, 0, 0, \infty)
\]

Using the same matrix notation as above, this becomes:

\[
(V_i^\pm)'(x) = A(q_2(x)) V_i^\pm(x) + \lambda (K_i^\pm B V_i^\pm)(x) + C_i^\pm
\]

where $C_1^- = C_2^+ = 0$, $C_1^+ = (K_1^- B V_1^-)(0)$ and $C_2^- = (K_2^+ B V_2^+)(0)$.\\ 

We can then write $V_i^\pm$ as a perturbation of $Q_2$ as above. If we do this, we get the exact same thing as in the case above. So essentially the only choice we have is what limits to use for the middle two pieces. 

\subsection*{Special Cases}

I HAVE NOT CHANGED ANYTHING PAST HERE \\

Here we look at some special cases of this. Recall that the parameter $L$ determines which double pulse we have constructed. Roughly, $L$ is an integer multiple (to leading order) of a phase parameter. Every other multiple is an unstable double pulse, where the interaction eigenvalues are real and negatives of each other. The remaining multiples we hypothesize are stable, where the interaction eigenvalues are pure imaginary.\\

\subsection*{Unstable Double Pulses}
For values of $L$ leading to unstable double pulses, we know that $\lambda$ is real and so the eigenfunctions must be real as well. Thus the above system should have a solution, with the only modification that all functions are real-values, and that:
\begin{align*}
W_i^+(0) - W_i^-(0) &\in \R \psi(0) \\
d_i &\in \R
\end{align*}

\subsection*{Potentially Stable Double Pulses}
For values of $L$ leading to potentially stable double pulses, we hypothesize that the $\lambda$ is pure imaginary, i.e. $\lambda = \beta i$, where $\beta > 0$. We showed in \texttt{KdV11} that in this case, we can always ``rotate'' our eigenfunction by multiplying by a unit complex number so that the real part is even and the imaginary part is odd. (That was for actual eigenfunctions, not for integrated eigenfunctions, but since we want these to be the same, that is ok.)\\

Write our eigenfunction $v(x)$ as $v = u + i \tilde{u}$. Then, with $L = \partial_x H$ as usual, we showed in \texttt{KdV11} that the eigenvalue problem can be written as

\[ \begin{cases}
Lu = -\beta \tilde{u} \\
L\tilde{u} = \beta u
\end{cases}
\]
Also, for actual eigenfunctions, we showed that the imaginary and real part are related by
\[
\tilde{u} = -\frac{1}{\beta}Lu
\]
Thus if we can find, say, the real part, we are done. To set this up, we look at our piecewise construction of the eigenfunction $V$

\begin{align}
V_i^\pm(x) &= d_i Q_2'(x) + W_i^\pm(x) = d_i (Q'(x) + (R_i^\pm)'(x)) + W_i^\pm(x) && d_i \in \C
\end{align}

Since $Q_2'(x)$ is a real function but $V(x)$ is complex, we will split up into real and imaginary parts as follows. We will write $d_i$ as $d_i + i \tilde{d}_i$ and $W_i^\pm$ as $W_i^\pm + i \tilde{W}_i^\pm$. The notation is not ideal, but we are running out of letters, so the convention here is that the tilde represents the imaginary part. Taking $\lambda = i \beta$ above and making the real/imaginary split, we have

\begin{align*}
(W_i^\pm + i \tilde{W}_i^\pm)'(x) &= A(Q) (W_i^\pm + i \tilde{W}_i^\pm)(x) + G_i^\pm (W_i^\pm + i \tilde{W}_i^\pm)(x) \\
 &\:\:\:\:+ i \beta (d_i + i \tilde{d}_i) B[ Q_2(x) - Q_2(a_i^\pm) ] + i \beta K_i^\pm B (W_i^\pm + i \tilde{W}_i^\pm)(x) + F_i^\pm B (W_i^\pm + i \tilde{W}_i^\pm) \\
 &= A(Q) (W_i^\pm + i \tilde{W}_i^\pm)(x) + G_i^\pm (W_i^\pm + i \tilde{W}_i^\pm)(x) \\
 &\:\:\:\:+ \beta (-\tilde{d}_i + i d_i) B[ Q_2(x) - Q_2(a_i^\pm) ] + \beta K_i^\pm B ( -\tilde{W}_i^\pm + i W_i^\pm)(x) + F_i^\pm B (W_i^\pm + i \tilde{W}_i^\pm)
\end{align*}

Equating real and imaginary parts, this becomes

\begin{align*}
(W_i^\pm)'(x) &= A(Q) W_i^\pm(x) + G_i^\pm W_i^\pm(x) -\beta \tilde{d}_i B[ Q_2(x) - Q_2(a_i^\pm) ] - \beta K_i^\pm B \tilde{W}_i^\pm(x) + F_i^\pm B W_i^\pm \\
(\tilde{W}_i^\pm)'(x) &= A(Q) \tilde{W}_i^\pm(x) + G_i^\pm \tilde{W}_i^\pm(x) + \beta d_i B[ Q_2(x) - Q_2(a_i^\pm) ] + \beta K_i^\pm B W_i^\pm(x) + F_i^\pm B \tilde{W}_i^\pm \\
\end{align*}

The nice thing about looking for a pure imaginary eigenvalue is that we know that if we can find a solution, we can find one where the real part is even and the imaginary part is odd. Thus we only have to solve for the first two pieces $(-\infty, 0]$ and $[0, L]$ with the appropriate BCs at $x = L$. Note that the BCs are on $V$ not on $W$.\\

For the first two pieces, the real part of the eigenfunction is given by $d_1 Q_2' + W_1^\pm$. We want this to have even BCs at $x = L$, so this means that the first and third derivatives at $x = L$ must be 0. The double pulse $q_2$ is even, so its derivative $q_2'$ is odd, so we don't get any extra help there. The BCs we want are

\begin{align*}
B_e [d_1 Q_2'(L) + W_1^+(L)] &= 0  && B_e = \textrm{Diag}(0, 1, 0, 1)
\end{align*}

For the the first two pieces, the imaginary part of the eigenfunction is given by $\tilde{d}_1 Q_2' + \tilde{W}_1^\pm$. We want this to have odd BCs at $x = L$, so this means that the function and secondd derivatives at $x = L$ must be 0. Since the derivative $q_2'$ is already odd, we can ignore that, so the BCs we want are

\begin{align*}
B_o \tilde{W}_1^+(L) &= 0  && B_o = \textrm{Diag}(1, 0, 1, 0)
\end{align*}

Putting all of this together, we have for the pure imaginary eigenvalue case the following system:

\begin{align*}
(W_1^\pm)'(x) &= A(Q) W_1^\pm(x) + G_i^\pm W_1^\pm(x) -\beta \tilde{d}_1 B[ Q_2(x) - Q_2(a_1^\pm) ] - \beta K_1^\pm B \tilde{W}_1^\pm(x) + F_1^\pm B W_1^\pm \\
(\tilde{W}_1^\pm)'(x) &= A(Q) \tilde{W}_1^\pm(x) + G_i^\pm \tilde{W}_1^\pm(x) + \beta d_1 B[ Q_2(x) - Q_2(a_1^\pm) ] + \beta K_1^\pm B W_1^\pm(x) + F_1^\pm B \tilde{W}_1^\pm \\
\end{align*}

with boundary and matching conditions

\begin{align*}
B_e [d_1 Q_2'(L) + W_1^+(L)] &= 0  && B_e = \textrm{Diag}(0, 1, 0, 1) \\
B_o \tilde{W}_1^+(L) &= 0  && B_o = \textrm{Diag}(1, 0, 1, 0) \\
W_1^-(0) &= W_1^+(0) \\
\tilde{W}_1^-(0) &= \tilde{W}_1^+(0) \\
W_i^\pm(0), \tilde{W}_i^\pm(0) &\in \R \psi(0) \oplus Y^+ \oplus Y^- \\
W_i^+(0) - W_i^-(0) &\in \R \psi(0)  \\
\tilde{W}_i^+(0) - \tilde{W}_i^-(0) &\in \R \psi(0)
\end{align*}

where
\begin{align*}
G_i^\pm(x) &= A(Q(x) + R_i^\pm(x)) - A(Q(x)) \\
\end{align*}

We could put all of this into an 8-dimensional system if we want, but not sure how useful that would be.


\subsection*{Estimates}
Ignore this for now, was based on the previous version. These should still hold, but we probably need some other estimates. \\

In order to keep going, we will need some estimates like in Lemma 3.1 in Sandstede (1998)

\begin{lemma}We have the estimates
\begin{align*}
|G_i(x)| &\leq C|R_i(x)| \leq C \sup_{|x| \geq L} |Q(x)| \\
| B Q_2(x) - B Q(x) | & \leq C |R_i(x)| \leq C \sup_{|x| \geq L} |Q(x)| \\
D_1 d &= (Q'(L) + Q'(-L))(d_2 - d_1) +\mathcal{O}\left( e^{-\alpha L} |d| \sup_{|x| \geq L} |Q(x)| \right)
\end{align*}
where $\alpha > 0$ is defined as on pages 432 and 434 of Sandstede (1998).
\begin{proof}
The first estimate is the same as in Sandstede (1998) with $Q$ replacing $Q^+$ and $Q^-$, and follows from the smoothness of $A$ together with (2.6)(i) in Sandstede (1998). The second estimate follows from (2.6)(i) in Sandstede (1998) and the expansion of $Q^2$ as $Q + R_i$. The third estimate is as in Lemma 3.1 of Sandstede (1998).
\end{proof}
\end{lemma}

\subsection*{Reduction}
This corresponds to section 3.2 in Sandstede (1998). What we will do first is consider (3.9)(i) in the paper, except we replace the $B_i^\pm$ with an integration operator.\\

Consider the ODE system:

\[
W_i^\pm(x)' = A(q(x)) W_i^\pm(x) + G_i^\pm(x) W_i^\pm(x) + \lambda d_i B Q_2(x) + \lambda (K_i^\pm B )W_i^\pm(x)
\]

where we have the bounds
\begin{align*}
|G_i^\pm(x)| \leq \delta \\
|Q_2(x) - Q(x) | \leq \delta \\
|\lambda| \leq \delta \\
\end{align*}

The matrix $B$ is the same one as above. We do everything that was done in Lemma 3.2. In order to keep going, we will need to work in exponentially weighted spaces, since we have an extra integral involved. Given an exponential weight $\eta > 0$, we define the following exponentially weighted spaces.

\begin{align*}
C^0_\eta[-a, 0] &= \{ f \in C^0[-a, 0] : \sup_{x \in [-a, 0]} |e^{-\eta x} f(x) | < \infty \} && a \geq 0 \\
C^0_\eta[0, a] &= \{ f \in C^0[0, a] : \sup_{x \in [0, a]} |e^{\eta x} f(x) | < \infty \} && a \geq 0 
\end{align*}

Since we want our functions in this space to decay exponentially at $\pm \infty$, we use the negative weight for $x \leq 0$ and the positive weight for $x \geq 0$. The corresponding norms are 

\begin{align*}
|| f ||_\eta &= \sup_{x \in [-a, 0]} |e^{-\eta x} f(x) | && f \in C^0_\eta[-a, 0] \\
|| f ||_\eta &= \sup_{x \in [0, a]} |e^{\eta x} f(x) | && f \in C^0_\eta[0, a] \\
\end{align*}

Now, following (3.14) in Sandstede (1998) we write our ODE system as the fixed point equation

\begin{align*}
W_i^- &= \Phi^s_-(x, -X_{i-1})a^-_{i-1} + \Phi^u_-(x, 0)b_i^- \\
&+ \int_0^x \Phi^u_-(x, y)[G_i^-(y) W_i^-(y) + \lambda (K_i^- B W_i^-)(y) + \lambda d_i B Q_2(y) ] dy \\
&+ \int_{-X_{i-1}}^x \Phi^s_-(x, y)[G_i^-(y) W_i^-(y) + \lambda (K_i^-B W_i^-)(y) + \lambda d_i B Q_2(y) ] dy \\
W_i^+(x) &= \Phi^u_+(x, X_i)a^+_{i} + \Phi^s_+(x, 0)b_i^+ \\
&+ \int_0^x \Phi^s_+(x, y)[G_i^+(y) W_i^+(y) + \lambda (K_i^+ B W_i^+)(y) + \lambda d_i B Q_2(y) ] dy \\
&+ \int_{X_{i}}^x \Phi^u_+(x, y)[G_i^+(y) W_i^+(y) + \lambda (K_i^+ B W_i^+)(y) + \lambda d_i B Q_2(y) ] dy
\end{align*}

The idea is that we integrate L to R along stable projection and from R to L along unstable projection. The $X_i^\pm$ are the appropriate interval endpoints, as we have above.\\

Now we will look for an analog of Lemma 3.3, except for a map between exponentially weighted spaces. \\

Define a linear operator $L_1(\lambda)$ (piecewise from our exponentially weighted space to itself) by

\begin{align*}
(L_1(\lambda)W_i^-)(x) = \int_0^x &\Phi^u_-(x, y)[G_i^-(y) W_i^-(y) + \lambda (K_i^- B W_i^-)(y) ] dy \\
&+ \int_{-X_{i-1}}^x \Phi^s_-(x, y)[G_i^-(y) W_i^-(y) + \lambda (K_i^-B W_i^-)(y) ] dy \\
(L_1(\lambda)W_i^+)(x) = \int_0^x &\Phi^s_+(x, y)[G_i^+(y) W_i^+(y) + \lambda (K_i^+ B W_i^+)(y)] dy \\
&+ \int_{X_{i}}^x \Phi^u_+(x, y)[G_i^+(y) W_i^+(y) + \lambda (K_i^+ B W_i^+)(y) ] dy
\end{align*}

We will now show this operator is bounded in the weighted supremum norm. We do this on each piece. The key is that the bounds do not depend on the $X_i^\pm$. First, we do the negative piece. Note that when taking the absolute value we have to integrate in the positive direction.

\begin{align*}
|e^{-\eta x} & (L_1(\lambda)W_i^-)(x) | \leq \delta e^{-\eta x} \int_x^0 |\Phi^u_-(x, y)|[|W_i^-(y)| + |(K_i^- B W_i^-)(y)| ] dy \\
&+ \delta e^{-\eta x} \int_{-X_{i-1}}^x |\Phi^u_-(x, y)|[|W_i^-(y)| + |(K_i^- B W_i^-)(y)| ] dy \\
&\leq \delta \left( \int_x^0 e^{\alpha^u (x-y)}e^{-\eta(x-y)}|e^{-\eta y} W_i^-(y)| dy 
+ \int_x^0 e^{\alpha^u (x-y)}e^{-\eta(x-y)}|e^{-\eta y} (K_i^- B W_i^-)(y)| dy  \right) \\
&+ \delta \left( \int_{-X_{i-1}}^x e^{-\alpha^s (x-y)}e^{-\eta(x-y)}|e^{-\eta y} W_i^-(y)| dy 
+ \int_{-X_{i-1}}^x e^{-\alpha^s (x-y)}e^{-\eta(x-y)}|e^{-\eta y} (K_i^- B W_i^-)(y)| dy  \right) \\ 
\end{align*}

The two terms not involving the integration operator are similar to those in Sandstede (1998), so let's take care of those first.

\begin{align*}
\int_x^0 e^{\alpha^u (x-y)}e^{-\eta(x-y)}|e^{-\eta y} W_i^-(y)| dy &= \int_x^0 e^{(\alpha^u - \eta) (x-y)}|e^{-\eta y} W_i^-(y)| dy \\
&\leq ||W_i^-||_\eta \frac{1 - e^{(\alpha^u - \eta)x}}{\alpha^u - \eta}
\end{align*}
We need the RHS of this to be positive, which is true when $\eta < \alpha^u$. Provided this is the case, we have the bound
\begin{align*}
\int_x^0 e^{\alpha^u (x-y)}e^{-\eta(x-y)}|e^{-\eta y} W_i^-(y)| dy &\leq ||W_i^-||_\eta \frac{1}{\alpha^u - \eta}
\end{align*}

For the other one:

\begin{align*}
\int_{-X_{i-1}}^x e^{-\alpha^s (x-y)}e^{-\eta(x-y)}|e^{-\eta y} W_i^-(y)| dy &= \int_{-X_{i-1}}^x e^{(-\alpha^s - \eta) (x-y)}|e^{-\eta y} W_i^-(y)| dy \\
&\leq ||W_i^-||_\eta \int_{-\infty}^x e^{(-\alpha^s - \eta) (x-y)} dy = ||W_i^-||_\eta \frac{1}{\alpha^s + \eta}
\end{align*}
So this has a nice bound independent of the $X_i$. The RHS is always positive, so this doesn't give us any extra conditions on $\eta$. \\

So far, all of this would have worked without the exponential weight. Where we need it is in the terms involving the integration operator. Let's do those terms now. Recall that because of the operator $B$ we are only integrating the scalar function $w_i^\pm$

\begin{align*}
\int_x^0 e^{\alpha^u (x-y)}e^{-\eta(x-y)}|e^{-\eta y} (K_i^- B W_i^-)(y)| dy &\leq \int_x^0 e^{(\alpha^u - \eta)(x-y)}e^{-\eta y} \int_{a_i^-}^y |w_i^-(u)| du dy \\
&= \int_x^0 e^{(\alpha^u - \eta)(x-y)}e^{-\eta y} \int_{a_i^-}^y e^{\eta u} |e^{-\eta u} w_i^-(u)| du dy \\
&\leq ||W_i^-||_\eta \int_x^0 e^{(\alpha^u - \eta)(x-y)}e^{-\eta y} \int_{-\infty}^y e^{\eta u} du dy \\
&= \frac{||W_i^-||_\eta}{\eta} \int_x^0 e^{(\alpha^u - \eta)(x-y)}e^{-\eta y} e^{\eta y} dy \\
&= \frac{||W_i^-||_\eta}{\eta} \frac{1 - e^{(\alpha^u - \eta)x}}{\alpha^u - \eta} 
\end{align*}

We need the RHS of this to be positive, which is true when $\eta < \alpha^u$. We had that condition from before, so that is good. Note that we changed the lower limit of the inner integral from $a_i^-$ to $-\infty$. This assumes that the integration opetator $K_i^-$ integrates from L to R. This is not the case in our alternate version above (where $K_2^-$ integrates from 0 to $-L$). We can handle that case as well; we just get a more restrictive condition on our exponential weight. In fact, what we have below handles any lower limit of integration $a_i^-$ within our interval $[-X_{i-1}, 0]$.

\begin{align*}
\int_x^0 e^{\alpha^u (x-y)}e^{-\eta(x-y)}|e^{-\eta y} (K_i^- B W_i^-)(y)| dy &\leq \int_x^0 e^{(\alpha^u - \eta)(x-y)}e^{-\eta y} \int_{a_i^-}^y e^{\eta u} |e^{-\eta u} w_i^-(u)| du dy \\
&\leq ||W_i^-||_\eta \int_x^0 e^{(\alpha^u - \eta)(x-y)}e^{-\eta y} \int_{-\infty}^0 e^{\eta u} du dy \\
&= \frac{||W_i^-||_\eta}{\eta} \int_x^0 e^{(\alpha^u - \eta)(x-y)}e^{-\eta y} dy \\
&= \frac{||W_i^-||_\eta}{\eta} \int_x^0 e^{(\alpha^u - \eta)(x-y)}e^{-\eta y} dy \\
&\leq \frac{||W_i^-||_\eta}{\eta} \int_x^0 e^{(\alpha^u - \eta)(x-y)}e^{-\eta x}e^{-\eta y} dy \\
&=\frac{||W_i^-||_\eta}{\eta} \int_x^0 e^{(\alpha^u - \eta)(x-y)}e^{-\eta(x+y)} dy \\
&=\frac{||W_i^-||_\eta}{\eta} \int_x^0 e^{(\alpha^u - 2 \eta)(x-y)} dy \\
&= \frac{||W_i^-||_\eta}{\eta} \frac{1 - e^{(\alpha^u - 2 \eta)x}}{\alpha^u - 2 \eta} 
\end{align*}

Where in the 5th line we multipled by $e^{-\eta x} \geq 1$ since $x \leq 0$. This gives us the condition$\eta < \alpha^u/2$. If we require this, we can handle the above case as well.\\

Here is the other integral term. First we do the case where the integral operator $K_i^-$ integrates from L to R.

\begin{align*}
\int_{-X_{i-1}}^x e^{-\alpha^s (x-y)}e^{-\eta(x-y)}|e^{-\eta y} (K_i^- B W_i^-)(y)| dy &\leq \int_{-X_{i-1}}^x e^{(-\alpha^s - \eta)(x-y)}e^{-\eta y} \int_{a_i^-}^y |w_i^-(u)| du dy \\
&= \int_{-X_{i-1}}^x e^{(-\alpha^s - \eta)(x-y)}e^{-\eta y} \int_{a_i^-}^y e^{\eta u} |e^{-\eta u} w_i^-(u)| du dy \\
&\leq ||W_i^-||_\eta \int_{-\infty}^x e^{(-\alpha^s - \eta)(x-y)}e^{-\eta y} \int_{-\infty}^y e^{\eta u} du dy \\
&= \frac{||W_i^-||_\eta}{\eta} \int_{-\infty}^x e^{(-\alpha^s - \eta)(x-y)}e^{-\eta y} e^{\eta y} dy \\
&= \frac{||W_i^-||_\eta}{\eta} \frac{1}{\alpha^s + \eta}
\end{align*}

This also has a nice bound independent of the $X_i$. The RHS is always positive, so this doesn't give us any extra conditions on $\eta$.\\

We can do the same thing we did above to handle the case where we integrate in the other direction of from another starting point.

\begin{align*}
\int_{-X_{i-1}}^x e^{-\alpha^s (x-y)}e^{-\eta(x-y)}|e^{-\eta y} (K_i^- B W_i^-)(y)| dy &\leq \int_{-X_{i-1}}^x e^{(-\alpha^s - \eta)(x-y)}e^{-\eta y} \int_{a_i^-}^y |w_i^-(u)| du dy \\
&= \int_{-X_{i-1}}^x e^{(-\alpha^s - \eta)(x-y)}e^{-\eta y} \int_{a_i^-}^y e^{\eta u} |e^{-\eta u} w_i^-(u)| du dy \\
&\leq ||W_i^-||_\eta \int_{-\infty}^x e^{(-\alpha^s - \eta)(x-y)}e^{-\eta y} \int_{-\infty}^0 e^{\eta u} du dy \\
&\leq \frac{||W_i^-||_\eta}{\eta} \int_{-\infty}^x e^{(-\alpha^s - \eta)(x-y)}e^{-\eta(x-y)} dy \\
&\leq \frac{||W_i^-||_\eta}{\eta} \int_{-\infty}^x e^{(-\alpha^s - 2\eta)(x-y)} dy \\
&= \frac{||W_i^-||_\eta}{\eta} \frac{1}{\alpha^s + 2\eta}
\end{align*}

This RHS is also always positive, so we are still good.\\

So far we have the condition $\eta < \alpha^u$ (or $\eta < \alpha^u / 2$. We expect to also have the condition $\eta < \alpha^s$ (or $\eta < \alpha^s/2$). We should get that from using the ``+'' equations. We will show that now. Recall here that since $x \geq 0$, we have to multiply by $e^{\eta x}$ to get the weighted norm. We also must integrate from L to R when taking the absolute value.\\

Here we do the ``+'' equations. This is similar to the ``-'' equations.

\begin{align*}
|e^{\eta x} & (L_1(\lambda)W)_i^+)(x) | \leq \delta e^{\eta x} \int_0^x |\Phi^s_+(x, y)|[|W_i^+(y)| + |(K_i^+ B W_i^+)(y)| ] dy \\
&+ \delta e^{\eta x} \int_{X_i}^x |\Phi^u_+(x, y)|[|W_i^+(y)| + |(K_i^+ B W_i^+)(y)| ] dy \\
&\leq \delta \left( \int_0^x e^{-\alpha^s (x-y)}e^{\eta(x-y)}|e^{\eta y} W_i^+(y)| dy 
+ \int_0^x e^{-\alpha^s (x-y)}e^{\eta(x-y)}|e^{\eta y} (K_i^+ B W_i^+)(y)| dy  \right) \\
&+ \delta \left( \int_x^{X_i} e^{\alpha^u (x-y)}e^{\eta(x-y)}|e^{\eta y} W_i^+(y)| dy 
+ \int_x^{X_i} e^{\alpha^u (x-y)}e^{\eta(x-y)}|e^{\eta y} (K_i^+ B W_i^+)(y)| dy  \right) \\ 
\end{align*}

We do the same thing we did above.

\begin{align*}
\int_0^x e^{-\alpha^s (x-y)}e^{\eta(x-y)}|e^{\eta y} W_i^+(y)| dy &= \int_0^x e^{(-\alpha^s + \eta) (x-y)}|e^{\eta y} W_i^-(y)| dy \\
&\leq ||W_i^+||_\eta \frac{1 - e^{-(\alpha^s - \eta)x} }{\alpha^s - \eta}
\end{align*}

We need the RHS of this to be positive, which is true when $\eta < \alpha^s$. (This is the condition we were expecting). In that case, we have the bound

\begin{align*}
\int_0^x e^{-\alpha^s (x-y)}e^{\eta(x-y)}|e^{\eta y} W_i^+(y)| dy &\leq ||W_i^+||_\eta \frac{1}{\alpha^s - \eta}
\end{align*}

For the other one:

\begin{align*}
\int_x^{X_i} e^{\alpha^u (x-y)}e^{\eta(x-y)}|e^{\eta y} W_i^+(y)| dy &= \int_x^{X_i} e^{(\alpha^u + \eta) (x-y)}|e^{\eta y} W_i^+(y)| dy \\
&\leq ||W_i^+||_\eta \int_x^{\infty} e^{(\alpha^u + \eta) (x-y)} dy = ||W_i^+||_\eta \frac{1}{\alpha^u + \eta}
\end{align*}
Again, this has nice bound independent of the $X_i$. The RHS is always positive, so this doesn't give us any extra conditions on $\eta$. \\

Now we do the integral terms. First we do this assuming that our integration operators $K_i^+$ integrate from R to L. Again, when we take the absolute value, we need to change our limits so we are integrating from L to R.

\begin{align*}
\int_0^x e^{-\alpha^s (x-y)}e^{\eta(x-y)}|e^{\eta y} (K_i^+ B W_i^+)(y)| dy &\leq \int_0^x e^{(-\alpha^s + \eta)(x-y)}e^{\eta y} \int_y^{a_i^+} |w_i^+(u)| du dy \\
&= \int_0^x e^{(-\alpha^s + \eta)(x-y)}e^{\eta y} \int_y^{a_i^+} e^{-\eta u} |e^{\eta u} w_i^+(u)| du dy \\
&\leq ||W_i^+||_\eta \int_0^x e^{(-\alpha^s + \eta)(x-y)}e^{\eta y} \int_y^\infty e^{-\eta u} du dy \\
&= \frac{||W_i^+||_\eta}{\eta} \int_0^x e^{(-\alpha^s + \eta)(x-y)}e^{\eta y} e^{-\eta y} dy \\
&= \frac{||W_i^+||_\eta}{\eta} \frac{1 - e^{-(\alpha^s - \eta)x}}{\alpha^s - \eta} 
\end{align*}

We need the RHS of this to be positive, which is true when $\eta < \alpha^s$. We had that condition from before, so that is good. As above, we can do the case where the integration operator integrates from L to R. In that case, we have the following.

\begin{align*}
\int_0^x e^{-\alpha^s (x-y)}e^{\eta(x-y)}|e^{\eta y} (K_i^+ B W_i^+)(y)| dy &\leq \int_0^x e^{(-\alpha^s + \eta)(x-y)}e^{\eta y} \int_y^{a_i^+} |w_i^+(u)| du dy \\
&= \int_0^x e^{(-\alpha^s + \eta)(x-y)}e^{\eta y} \int_y^{a_i^+} e^{-\eta u} |e^{\eta u} w_i^+(u)| du dy \\
&\leq ||W_i^+||_\eta \int_0^x e^{(-\alpha^s + \eta)(x-y)}e^{\eta y} \int_0^\infty e^{-\eta u} du dy \\
&= \frac{||W_i^+||_\eta}{\eta} \int_0^x e^{(-\alpha^s + \eta)(x-y)}e^{\eta y} dy \\
&= \frac{||W_i^+||_\eta}{\eta} \frac{1 - e^{-(\alpha^s - \eta)x}}{\alpha^s - \eta} 
\end{align*}

\begin{align*}
\int_{-X_{i-1}}^x e^{-\alpha^s (x-y)}e^{-\eta(x-y)}|e^{-\eta y} (K_i^- B W_i^-)(y)| dy &\leq \int_{-X_{i-1}}^x e^{(-\alpha^s - \eta)(x-y)}e^{-\eta y} \int_{a_i^-}^y |w_i^-(u)| du dy \\
&= \int_{-X_{i-1}}^x e^{(-\alpha^s - \eta)(x-y)}e^{-\eta y} \int_{-\infty}^y e^{\eta u} |e^{-\eta u} w_i^-(u)| du dy \\
&\leq ||W_i^-||_\eta \int_{-\infty}^x e^{(-\alpha^s - \eta)(x-y)}e^{-\eta y} \int_{-\infty}^y e^{\eta u} du dy \\
&= \frac{||W_i^-||_\eta}{\eta} \int_{-\infty}^x e^{(-\alpha^s - \eta)(x-y)}e^{-\eta y} e^{\eta y} dy \\
&= \frac{||W_i^-||_\eta}{\eta} \frac{1}{\alpha^s + \eta}
\end{align*}


\end{document}