\documentclass[12pt]{article}
\usepackage[pdfborder={0 0 0.5 [3 2]}, plainpages=false]{hyperref}%
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}%
\usepackage[shortalphabetic]{amsrefs}%
\usepackage{amsmath}
\usepackage{enumerate}
% \usepackage{enumitem}
\usepackage{amssymb}                
\usepackage{amsmath}                
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage[table,xcdraw]{xcolor}
\usepackage{tikz}
\usepackage{float}
\usepackage{booktabs}
\usepackage{svg}
\usepackage{mathtools}
\usepackage{cool}
\usepackage{url}
\usepackage{graphicx,epsfig}
\usepackage{makecell}
\usepackage{array}

\def\noi{\noindent}
\def\T{{\mathbb T}}
\def\R{{\mathbb R}}
\def\N{{\mathbb N}}
\def\C{{\mathbb C}}
\def\Z{{\mathbb Z}}
\def\P{{\mathbb P}}
\def\E{{\mathbb E}}
\def\Q{\mathbb{Q}}
\def\ind{{\mathbb I}}

\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\ran}{range}

\setlength\parindent{0pt}

\graphicspath{ {kdv5/} }

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{remark}{Remark}

\newtheorem{notation}{Notation}

\begin{document}

\section{5th order KdV Equation}

\subsection{Background}\label{sec:background}

The fifth-order Korteweg-de Vries equation (KdV5) is a nonlinear, dispersive partial differential equation which is used to model physical phenomena such as capillary-gravity water waves and plasma waves. There are many versions of this equation. The one we will study is

\begin{equation}\label{KdV5}
u_t = u_{xxxxx} - u_{xxx} - 2 u u_x
\end{equation}

which can also be written as

\begin{equation}\label{KdV5alt}
u_t = \partial_x(u_{xxxx} - u_{xx} - u^2)
\end{equation}

Since we are interested in traveling wave solutions, we write \eqref{KdV5} in a co-moving frame with speed $c$ by letting $\xi = x - ct$. Making this substitution, and renaming the independent variable back to $x$, we have

\begin{equation}\label{KdV5c}
u_t = u_{xxxxx} - u_{xxx} + c u_x - 2 u u_x
\end{equation}

Equation \eqref{KdV5c} is Hamiltonian, with energy given by 

\begin{equation} \label{energy}
E(u) = -\int_{-\infty}^{\infty} \left( \frac{1}{2}u_{xx}^2 + \frac{1}{2}u_x^2 + \frac{1}{2}cu^2 - \frac{1}{3}u^3 \right) dx
\end{equation}

and \eqref{KdV5c} can be written in standard Hamiltonian form as

\[
u_t = \partial_x E'(u)
\]

where the operator $\partial_x$ is skew-symmetric. It is not hard to show that the energy $E(u)$ is conserved (in time).\\

An equilibrium solution to \eqref{suspc} satisfies the 5th order nonlinear ODE

\begin{equation}\label{eqODE}
u_{xxxxx} - u_{xxx} + c u_x - 2 u u_x = 0
\end{equation}

It is clear that the trivial solution $u = 0$ is a solution to \eqref{eqODE}. A primary pulse solution is a homoclinic orbit which connects the trivial solution $u = 0$ to itself. Since such a solution decays (exponentially) to 0 at both ends, a primary pulse solution must also satisfy the 4th order ODE

\begin{equation}\label{eqODE4}
u_{xxxx} - u_{xx} + c u - u^2 = 0,
\end{equation}

which is obtained from \eqref{eqODE} by integrating once and taking the constant of integration to be 0, since we are assuming the solution decays to 0 at both ends. Equation \eqref{eqODE4} is also Hamiltonian (in $x$), with energy given by

\begin{equation}\label{Hamiltonian}
H(u, u', u'', u''') = u'u''' - \frac{1}{2}(u'^2) - \frac{1}{2}(u'')^2 + \frac{c}{2}u^2 - \frac{1}{3}u^3 
\end{equation}

It is not hard to show that $H$ is conserved (in $x$).\\

The linearization of \eqref{eqODE4} about a given solution $u_*$ of \eqref{eqODE4} is given by the operator

\begin{equation}\label{defA0}
A_0(u^*) = \partial_x^4 - \partial_x^2 + c - 2 u^* 
\end{equation}

For the linearization about the trivial solution $u^* = 0$, the eigenvalues are given by the solutions to the fourth-order polynomial equation $\nu^4 - \nu^2 + c = 0$, which are

\begin{align}\label{specA0}
\nu = \pm \sqrt{ \frac{1 \pm \sqrt{1 - 4c} }{2}}
\end{align}

Since two eigenvalues have positive real part and two have negative real part, the equibrium at 0 is hyperbolic with a two-dimensional stable manifold and a two-dimensional unstable manifold. For $0 < c < 1/4$, all four eigenvalues are real. A bifurcation takes place at $c = 1/4$, and for $c > 1/4$, there is a quartet of eigenvalues of the form $\pm \alpha \pm \beta i$, where $\alpha, \beta > 0$.\\

A primary pulse solution connects the stable and unstable manifolds. Its existence is given in the following theorem. The proof involves the mountain pass technique.

\begin{theorem}[Groves98, Chug07]\label{singleexist}
\[\]
\begin{enumerate}[(i)]
\item For $c>0$, a primary pulse solution $q(x; c)$ exists to \eqref{eqODE4}. This solution is smooth, even, and decays exponentially to 0 as $x \rightarrow \pm \infty$.
\item The linearization $A_0(q(x; c))$ about this primary pulse soution has exactly one negative eigenvalue with an even eigenfunction and a simple kernel with the odd eigenfunction $q'(x; c)$.
\end{enumerate}
\end{theorem}

We are interested in the existence and stability of multi-pulse equilibrium solutions to \eqref{KdV5}. A multi-pulse is a localized, multi-modal solution $q_n(x; c) $to \eqref{eqODE4} which resembles multiple, well-separated copies of the primary pulse $q(x; c)$. The existence of multi-pulse solutions is given in the following theorem, which is adapted from SanStrut

\begin{theorem}\label{multiexist}[SanStrut]
Let $q(x; c)$ be a localized solution to \eqref{eqODE4}, with $c > 1/4$. Recall that the spectrum of $A_0(0)$ is given by $\nu = \pm \alpha \pm \beta i$, where $\alpha, \beta > 0$. Then for any $n \geq 2$ and any sequence of nonnegative integers $k_1, \dots, k_{n-1}$ with at least one of the $k_j \in \{0, 1 \}$, there exists a nonnegative integer $m_0$ and $\delta > 0$ such that
\begin{enumerate}[(i)]
	\item For any integer $m$ with $m \geq m_0$, there exists a unique $n-$modal solution $q_n(x, c)$ to \eqref{eqODE4} which is of the form
	\begin{align}\label{qn}
	q_n(x; c) = \sum_{j = 1}^{n} q^j(x; c) + r(x; c),
	\end{align}
	where each $q^j(x; c)$ is a translate of the primary pulse $q(x; c)$. The distance between the peaks of $q^j$ and $q^{j+1}$ is $2 X_j$, where
	\begin{equation*}
	X_j \approx \frac{\pi}{\beta}(2 m + k_j) + \tilde{X},
	\end{equation*}
	and $\tilde{X}$ is a constant. The remainder term $r(x; c)$ satisfies
	\begin{equation}\label{rbound}
	||r|| \leq C e^{-\alpha X_m},
	\end{equation}
	where $X_m = \min\{X_1, \dots, X_{n-1}\}$.

	\item The linear operator $A_0(q_n(x; c))$ on $L^2(\R)$ has precisely $n$ real eigenvalues $\nu_j$ with $|\nu_j| < \delta$, where $\nu_n = 0$ is a simple eigenvalue, and for $j = 1, \dots, n-1$,
	\begin{align*}
	\nu_j < 0 \text{ if } k_j \text{ is odd} \\
	\nu_j > 0 \text{ if } k_j \text{ is even} 
	\end{align*}

\end{enumerate}

\begin{proof}
By Theorem \ref{singleexist}, $q(x; c)$ is a transversely-constructed, localized solution to \eqref{eqODE4} with simple kernel $q'(x; c)$. Since $c > 1/4$, the eigenvalues of $A_0(0)$ are of the form $\pm \alpha \pm \beta i$, with $\alpha, \beta > 0$. Furthermore, equation \eqref{eqODE4} is Hamiltonian with energy $H$ given by \eqref{Hamiltonian}. Thus the hypotheses of Theorem 3.6 in \cite{Sandstede1997} are satisfied. Since the Melnikov integral $M = \int_{-\infty}^\infty |q_x|^2 dx$ is positive, (i) and (ii) follow from Theorem 3.6 in \cite{Sandstede1997}, except for the bound on $r(x; c)$ in (i), which follows from \cite{Sanstede1993} and \cite{Sandstede1998}. The eigenvalues $\nu_j$ are real since $A_0(q_n)$ is self-adjoint.
\end{proof}
\end{theorem}

To determine linear PDE stability of the multi-pulse solutions constructed in Theorem \ref{multiexist}, we look at the linearization of the PDE \eqref{KdV5c} about $q_n(x; c)$. Substituting the usual lineariazation ansatz $u(x, t) = e^{\lambda t} v(x)$ into \eqref{KdV5c} and simplifying, we obtain the PDE eigenvalue problem $L(q_n(x)) v = \lambda v$, where

\begin{equation}\label{PDEeig}
L(q_n(x)) = \partial_x A_0(q_n(x)) = \partial_x^5 - \partial_x^5 + c \partial_x - 2 q_n \partial_x - 2 \partial_x q_n  
\end{equation}

For $c > 0$, by the Weyl essential spectrum theorem \cite[Theorem 2.2.6]{Kapitula2013}, the essential spectrum of \eqref{PDEeig} is independent of $q_n$ and consists of the entire imaginarly axis. Linear PDE stability thus depends on the point spectrum. \\

Unfortunately, the linearization of \eqref{KdV5c} about the trivial solution $u = 0$ is not hyperbolic since $L(0)$ has an eigenvalue of 0. Thus we cannot directly apply the results of \cite{Sandstede1998}. To get around this problem, we will use an exponentially weighted space.

\subsection{Exponentially Weighted Spaces}\label{sec:expwt}

We will pose our eigenvalue problem in the exponentially weighted space $H_\eta^k(\R)$, where $\eta$ is the exponential weight, and the norm of the space is given by

\[
||u||_{H_\eta^k(\R)} = ||e^{\eta x}u||_{H^k}
\]

In the exponentially weighted space, the operator $L(q_n(x))$ becomes the operator $L_\eta(q_n(x))$, which is given by

\begin{equation}\label{Leta}
L_\eta = e^{\eta x} L e^{-\eta x} = e^{\eta x} \partial_x H e^{-\eta x}
\end{equation}

We can write the eigenvalue problem as a first-order system. For the unweighted problem, we do this in the usual way. Letting $V = (v, v', v'', v''', v'''')$, the unweighted eigenvalue problem becomes

\[
V' = A(q_n(x))V + \lambda B V
\]

where

\begin{align*}
B = \begin{pmatrix}0 & 0 & 0 & 0 & 0 \\0 & 0 & 0 & 0 & 0 \\0  & 0 & 0 & 0 & 0 \\0 & 0 & 0 & 0 & 0 \\1 & 0 & 0 & 0 & 0 \end{pmatrix} && 
A(q_n(x)) = \begin{pmatrix}0 & 1 & 0 & 0 & 0 \\0 & 0 & 1 & 0 & 0 \\0 & 0 & 0 & 1 & 0 \\0 & 0 & 0 & 0 & 1 \\
2q_n'(x) & 2q_n(x) - c & 0 & 1 & 0 \end{pmatrix} 
\end{align*}

Note that $L_\eta u = \lambda u$ if and only if $L v = \lambda v$, where $v = e^{-\eta x} u$. Motivated by this, let $V = e^{-\eta x} U$. Substituting this into the unweighted problem, we get

\begin{align*}
e^{-\eta x} U' - \eta e^{-\eta x}U &= A(q_n(x))e^{-\eta x}U + \lambda B e^{-\eta x}U \\
U' &= (A(q_n(x)) + \eta I) U + \lambda B U
\end{align*}

Renaming $U$ back to $V$, the weighted eigenvalue problem is

\begin{equation}\label{weightedeig}
V' = (A(q_n(x)) + \eta I)V + \lambda B V
\end{equation}

We see that each row of $V$ is $(\partial_x - \eta)$ operated on the previous row, which is consistent with how we have defined our exponentially weighted space. We also note that the weight $\eta$ shifts the (spatial) eigenvalues by $\eta$ (in the direction given by the sign of $\eta$). 

Before we go too much further, we need to choose $\eta$. For the linearization about the zero-solution, recall that the nonzero eigenvalues of $A(0)$ are given by

\[
\nu = \pm \sqrt{ \frac{1 \pm \sqrt{1 - 4c} }{2}}
\]

Two have positive real part and two have negative real part, and these are negatives of each other. Let $\alpha > 0$ be the smallest real part of the unstable eigenvalues, so $-\alpha$ is the largest real part of the stable eigenvalues. Since the primary pulse (and all its derivatives) decays asymptotally exponentially with rate equal to or faster than $\alpha$ (i.e. $|e^{-\alpha |x|} q(x)|$ is bounded), we need to choose $\eta$ with either $0 < \eta < \alpha$ or $-\alpha < \eta < 0$ so that $q(x)$ (and its derivatives) still decays in the exponentially weighted space. Since it does not matter which one we choose, we will take $0 < \eta < \alpha$. Thus the eigenvalues of $A(0) + \eta I$ lie outside the open interval $(-\alpha + \eta, \eta)$.\\

Recall that in the unweighted space, $L(q_n)$ has a kernel with algebraic multiplicy 2 and geometric multiplicity 1. The eigenfunction and generalized eigenfunction are given by

\begin{align*}
L(q_n) q_n' &= 0 \\
L(q_n) (-\partial_c q_n) &= q_n'
\end{align*}

In the first-order system, these become

\begin{align*}
(Q_n')' &= A(q_n) Q_n' \\
(-\partial_c Q_n)' &= A(q_n) (-\partial_c Q_n) + B Q_n'
\end{align*}

In the weighted space, these become

\begin{align*}
(e^{\eta x} Q_n')' &= (A(q_n(x)) + \eta I) e^{\eta x}  Q_n' \\
(-e^{\eta x} \partial_c Q_n)' &= (A(q_n(x)) + \eta I) (-e^{\eta x} \partial_c Q_n) + B e^{\eta x} Q_n'
\end{align*}

Finally, consider the variational equation and adjoint variational equation

\begin{align}
V' &= (A(q(x)) + \eta I)V \label{vareq} \\
W' &= -(A(q(x)) + \eta I)^* W \label{adjvareq}
\end{align}
  
Let $\Phi(x, y; \eta)$ be the evolution operator for the variational equation. 

Then we decompose $\Phi(x, y; \eta)$ in an exponential dichotomy according to the following lemma, which is the same as Lemma 3.2 in \cite{Sandstede1998}.

% lemma : exponential dichotomy 

\begin{lemma}\label{dichotomy}
The evolution $\Phi(x, y; \eta)$ can be decomposed in exponential dichotomies on $\R^+$ and $R^-$ as

\begin{align*}
\Phi(x, y; \eta) &= \Phi^s_+(x, y; \eta) + \Phi^u_+(x, y; \eta) && x, y \geq 0 \\
\Phi(x, y; \eta) &= \Phi^s_-(x, y; \eta) + \Phi^u_-(x, y; \eta) && x, y \leq 0 
\end{align*}

for which we have estimates

\begin{align*}
|\Phi^s_+(y,x; \eta)| &\leq Ce^{-\alpha^s(y-x)} && 0 \leq x \leq y \\
|\Phi^u_+(y,x; \eta)| &\leq Ce^{\alpha^u(y-x)}  && 0 \leq y \leq x \\
|\Phi^s_-(y,x; \eta)| &\leq Ce^{-\alpha^s(y-x)} && x \leq y \leq 0 \\
|\Phi^u_-(y,x; \eta)| &\leq Ce^{\alpha^u(y-x)}  && y \leq x \leq 0 \\
\end{align*}

The projection operators at $x$ are given by

\begin{align*}
P^s_\pm(x; \eta) &= \Phi^s_\pm(x,x)\\
P^u_\pm(x; \eta) &= \Phi^u_\pm(x,x)\\
\end{align*}

for which we have estimates

\begin{align*}
|P^s_+(x; \eta) - P_0^s(\eta)| \leq Ce^{-\alpha^s x} && x \geq 0 \\
|P^u_+(x; \eta) - P_0^u(\eta)| \leq Ce^{-\alpha^s x} && x \geq 0 \\
|P^s_-(x; \eta) - P_0^s(\eta)| \leq Ce^{\alpha^u x} && x \leq 0 \\
|P^u_-(x; \eta) - P_0^u(\eta)| \leq Ce^{\alpha^u x} && x \leq 0 \\
\end{align*}

where $P_0^s(\eta)$ and $P_0^u(\eta)$ are the eigenprojections for the equilibrium at 0 in the weighted space. 

\end{lemma}

We make the following hypothesis.

\begin{hypothesis}
There exists a unique bounded solution $V_\eta(x)$ to the weighted variational equation \eqref{vareq}.
\end{hypothesis}

This unique bounded solution must be given by $V_\eta(x) = e^{\eta x} Q'(x)$, since the unweighted  variational equation has solution $Q'(x)$, given by

\begin{equation}
Q'(x) = (q'(x), q''(x), q'''(x), q''''(x), q'''''(x))^T.
\end{equation}

It follows that the weighted adjoint variational equation \eqref{adjvareq} has a unique bounded solution $\Psi_\eta(x)$. Since we have a solution $\Psi(x)$ to the unweighted adjoint variational equation given by

\begin{equation}\label{Psi}
\Psi(x) = \begin{pmatrix}
q''''(x) - q''(x) + (-2q(x) + c)q(x)\\
-q'''(x) + q'(x) \\
q''(x) - q(x) \\
-q'(x) \\
q(x)
\end{pmatrix}
\end{equation}

the unique bounded solution to \eqref{adjvareq} must be given by $\Psi_\eta(x) = e^{-\eta x}\Psi(x)$.

\subsection{Piecewise Ansatz}

Let the multi-pulse $q_n(x; c)$ be constructed as in Theorem \ref{multiexist} with peak distances $2 X_1, \dots, 2 X_{n-1}$. Let $X_0 = X_n = \infty$. Following \cite{Sandstede1998}, we will construct our ansatz piecewise on the $2n$ intervals

\[
(-X_0, 0], [0, X_1], [-X_1, 0], \dots, [0, X_{n-1}], [-X_{n-1}, 0], [0, X_n) 
\]

On these intervals, we can then write the multi-pulse $q_n$ piecewise as

\[
q_n(x) = q(x) + r_i^\pm(x)
\]

where for the remainder term, we have the uniform bound from Theorem \ref{multiexist}

\begin{equation}\label{ripmbound}
|r_i^\pm(x)| = e^{-\alpha X_m} 
\end{equation}

which holds for all derivatives of $r_i^\pm(x)$. Returning to the first-order system, we make the piecewise ansatz 

\[
V_i^\pm = d_i (e^{\eta x} Q' + \lambda e^{\eta x} \partial_c Q) + W_i^\pm
\]

where the $d_i$ are constants which we will solve for. Plugging this into the eigenvalue problem and simplifying using the relations above, $(W_i^\pm, d_i)$ solves

\begin{align*}
(W_i^\pm)' &= A(q_n(x); \eta) W_i^\pm + \lambda B W_i^\pm + \lambda^2 e^{\eta x} \partial_c Q_n \\
W^\pm(x) &\in \C \psi(0) \oplus Y^+ \oplus Y^- \\
W^+(0) - W^-(0) &\in \C \psi(0) \\
W_i^+(X_i) - W_{i+1}^-(-X_i) &= D_i(\eta) d
\end{align*}

where

\begin{equation}\label{Aeta}
A(u(x); \eta) = A(u(x)) + \eta I
\end{equation}

and

\begin{equation}\label{Did}
D_i(\eta) d = d_{i+1} e^{-\eta X_i}[ Q_n'(-X_i) + \lambda \partial_c Q_n(-X_i)] 
- d_i e^{\eta X_i}[ Q_n'(X_i) + \lambda \partial_c Q_n(X_i)] 
\end{equation}

Finally, let 

\begin{align*}
G_i^\pm(x; \eta) &= A(q_n(x); \eta) - A(q(x); \eta) \\
\tilde{H}_i^\pm(x) &= e^{\eta x} \partial_c Q_n(x) \\
H(x) &= e^{\eta x} \partial_c Q(x)
\end{align*}

Then the system we will investigate is

\begin{align*}
(W_i^\pm)' &= A(q_n(x); \eta) W_i^\pm + \lambda B W_i^\pm + \lambda^2 \tilde{H} \\
W^\pm(x) &\in \C \psi(0) \oplus Y^+ \oplus Y^- \\
W^+(0) - W^-(0) &\in \C \psi(0) \\
W_i^+(X_i) - W_{i+1}^-(-X_i) &= D_i(\eta) d
\end{align*}

In the next lemma, we show some estimates similar to that in Lemma 3.1 in Sandstede (1998).

% estimates lemma

\begin{lemma}\label{estimates}

We have the estimates

\begin{align*}
|G_i^\pm(x; \eta)| &\leq C|R_i^\pm(x)| \leq C e^{-\alpha X_m} \\
| \tilde{H}_i^\pm(x)  - H(x) | & \leq C | e^{\eta x} R_i^\pm(x)| \leq C e^{-(\alpha - \eta) X_m}  \\
D_i d &= ( Q'(X_i) + Q'(-X_i) )(d_2 e^{-\eta X_i} - d_1 e^{\eta X_i}) + \mathcal{O} \left( e^{-(\alpha - \eta) X_m}\left( |\lambda| +  e^{-\alpha X_m} \right) |d| \right) \\
\end{align*}
where $X_m = \min\{X_1, \dots, X_n\}$.

\begin{proof}
Since 

\begin{align*}
G_i^\pm(x; \eta) = A(q_n(x); \eta) - A(q(x); \eta) &= (A(q_n(x)) + \eta I) - (A(q(x)) + \eta I) \\
&= A(q_n(x)) - A(q(x)) \\
&= A(r_i^\pm(x))
\end{align*}

$G_i^\pm(x) = \mathcal{O}(r_i^\pm(x)|)$, for which we have the bound \eqref{ripmbound}. The estimate (ii),

\[
\tilde{H}_i^\pm(x)  - H(x) = e^{\eta x} \partial_c R_i^\pm(x),
\]
and the bound follows \eqref{ripmbound}. For estimate (iii), we first write $D_i d$ as

\begin{align*}
D_i d &= d_{i+1} e^{-\eta X_i}[ Q_n'(-X_i) + \lambda \partial_c Q_n(-X_i)] 
- d_i e^{\eta X_i}[ Q_n'(X_i) + \lambda \partial_c Q_n(X_i)]  \\
&= d_{i+1} e^{-\eta X_i} [ Q'(-X_i) + \lambda \partial_c Q(-X_i) + (R_{i+1}^-)'(-X_i) + \lambda  \partial_c R_{i+1}^-(-X_i)] \\
&- d_i e^{\eta X_i} [ Q'(X_i) + \lambda \partial_c Q(X_i) + (R_i^+)'(X_i) + \lambda \partial_c R_i^+(X_i)] 
\end{align*}

Adapting Lemma 2.6 in \cite{Sandstede1998}, we have

\begin{align*}
|R_{i+1}^-(-X_i) - Q(X_i)| \leq C e^{-2 \alpha X_m} \\
|R_{i}^+(X_i) - Q(-X_i)| \leq C e^{-2 \alpha X_m}
\end{align*}

which hold as well for derivatives with respect to $x$ and $c$. Substituting these into the expression for $D_i d$

\begin{align*}
D_i d &= d_{i+1} e^{-\eta X_i} \left[ Q'(X_i) + Q'(-X_i) + \lambda( \partial_c Q(X_i) + \partial_c Q_c(-X_i)) + \mathcal{O} \left( e^{-2 \alpha X_m} \right) \right] \\
&- d_i e^{\eta X_i} \left[ Q'(X_i) + Q'(-X_i) + \lambda( \partial_c Q(X_i) + \partial_c Q_c(-X_i)) + \mathcal{O} \left( e^{-2 \alpha X_m} \right) \right]
\end{align*}

Finally, since $\partial_c Q(\pm X_i)$ is of order $e^{-\alpha X_i}$, this becomes

\begin{align*}
D_i d &= ( Q'(X_i) + Q'(-X_i) )(d_{i+1} e^{-\eta X_i} - d_i e^{\eta X_i})
+ \mathcal{O} \left( e^{-(\alpha - \eta) X_m}\left( |\lambda| +  e^{-\alpha X_m} \right) |d| \right)
\end{align*}

which is estimate (iii).

\end{proof}
\end{lemma}

We are now in position to use Lin's method, following \cite{Sandstede1998}. The only differences between what we have and the situation in \cite{Sandstede1998} is that the term $\lambda \tilde{H}$ is replaced by $\lambda^2 \tilde{H}$, and the estimate for $D_i d$ takes a slightly different form. Thus we can adapt Lemma 3.6 in \cite{Sandstede1998}. Recalling that $e^{-\eta x}\Psi(x)$ is the unique bounded solution to the adjoint variational equation, we can use Lin's method to obtain a unique, piecewise smooth solution $(W_i^\pm, d_i)$ with $n$ jumps which are given by

\begin{align*}
\xi_i &= \langle \Psi_\eta(X_i), P_0^u D_i d \rangle
+ \langle \Psi_\eta(-X_{i-1}), P_0^u D_{i-1} d \rangle
- \lambda^2 d_i \int_{-\infty}^\infty \langle \Psi_\eta(y), H(y) \rangle dy
+ (R(\lambda)d)_i
\end{align*}

The remainder term has bound

\begin{align*}
|(R&(\lambda)d)_i| \leq C \left( (e^{-\alpha X_m} + |G| + |\lambda|)^2 |D| + (e^{-\alpha X_m} + |G| + ||\tilde{H}_i - H|| + |\lambda| )|\lambda^2| \right)|d|
\end{align*}

\subsection{The Substitution}

Finally, we substitute our expressions for $D_i$, $\Psi_\eta$, $\tilde{H}$, and $H$ into the jump equations. The Melnikov integral term is given by

\begin{align*}
M &= \int_{-\infty}^{\infty} \langle \Psi_\eta(y), H(y) \rangle dy \\
&= \int_{-\infty}^{\infty} \langle e^{-\eta y} \Psi(y), e^{-\eta y} B \partial_c q(y) \rangle dy \\
&= \int_{-\infty}^{\infty} e^{-\eta y} q(y), e^{-\eta y} \partial_c q(y) dy \\
&= \int_{-\infty}^{\infty} q(y) \partial_c q(y) dy \\
&= \int_{-\infty}^{\infty} \partial_c \left( \frac{1}{2} q(y)^2 \right) dy
\end{align*}

For the terms involving $d_i$, we use the estimate from Lemma \ref{estimates}.

\begin{align*}
\langle \Psi_\eta(X_i), &P_0^u D_i d \rangle
= \langle e^{-\eta X_i} \Psi(X_i), P_0^u( Q'(X_i) + Q'(-X_i) )(d_{i+1} e^{-\eta X_i} - d_i e^{\eta X_i}) + \mathcal{O} \left( e^{-(\alpha - \eta) X_m}\left( |\lambda| +  e^{-\alpha X_m} \right) |d| \right) \rangle \\
&= \langle \Psi(X_i), Q'(-X_i) \rangle ( e^{-2 \eta X_i}d_{i+1} - d_i ) + \mathcal{O} \left( e^{-(\alpha - \eta) X_m}\left( |\lambda| +  e^{-\alpha X_m} \right) |d| \right)
\end{align*}

where we used the fact that $\Psi(-X_i) \perp Q'(-X_i)$. The other term is similar.

\begin{align*}
\langle \Psi_\eta(-X_{i-1}), P_0^s D_{i-1} d \rangle 
&= \langle \Psi(-X_{i-1}), Q'(X_{i-1}) \rangle (d_i - e^{2 \eta X_{i-1}} d_{i-1} ) + \mathcal{O} \left( e^{-(\alpha - \eta) X_m}\left( |\lambda| +  e^{-\alpha X_m} \right) |d| \right)
\end{align*}

For the remainder bound, using Lemma \ref{estimates} and the fact that $|D| = \mathcal{O}(e^{-(\alpha - \eta)X_m}|d|)$, we have

\begin{align*}
|(R&(\lambda)d)_i| \leq C 
\left( (e^{-\alpha X_m} + |\lambda|)^2 e^{-(\alpha - \eta)X_m}  
+ (e^{-\alpha X_m} + |\lambda| )|\lambda^2| \right)|d|
\end{align*}

Finally, using the expressions for $Q'(x)$ and $\Psi(x)$ and the fact that $q(x)$ is an even function, we have

\[
\langle \Psi(-X), Q'(X) \rangle = -\langle \Psi(X), Q'(-X) \rangle
\]

Using this relation with the jump expression and combining the various remainder terms into a single remainder term $\tilde{R}(\lambda)_i$ gives us our final jump expressions

\begin{align*}
\xi_i & = \langle \Psi(X_i), Q'(-X_i) \rangle ( e^{-2 \eta X_i}d_{i+1} - d_i ) 
- \langle \Psi(X_{i-1}), Q'(-X_{i-1}) \rangle (d_i - e^{2 \eta X_{i-1}} d_{i-1} ) - \lambda^2 d_i M  + (\tilde{R}(\lambda)d)_i \\
|(\tilde{R}(\lambda)d)_i| &\leq C 
\left( (e^{-\alpha X_m} + |\lambda|)^2 e^{-(\alpha - \eta)X_m}  
+ (e^{-\alpha X_m} + |\lambda| )|\lambda^2| \right)|d|
\end{align*}

Let 

\[
a_i = \langle \Psi(X_i), Q'(-X_i) \rangle
\]

Then we can write the jump equations as

\[
\xi_i = a_i ( e^{-2 \eta X_i}d_{i+1} - d_i ) 
- a_{i-1} (d_i - e^{2 \eta X_{i-1}} d_{i-1} ) - \lambda^2 d_i M  + (\tilde{R}(\lambda)d)_i \\
\]

We have a smooth solution to the eigenvalue problem when all $n$ jumps are 0. We can write the $n$ jump conditions in matrix form as

\[
S_\eta(\lambda)d = (A_\eta - \lambda^2 M I + \tilde{R}(\lambda))d = 0
\]

where $A_\eta$ is the tri-diagonal matrix

\[
A_\eta = \begin{pmatrix}
-a_1 & e^{-2 \eta X_1} a_1 \\
e^{2 \eta X_1} a_1 & -a_2 - a_1 & e^{-2 \eta X_2} a_2 \\
& e^{2 \eta X_2} a_2 & -a_3 - a_2 & e^{-2 \eta X_3} a_3 \\
\vdots & & & \vdots \\
& & & e^{2 \eta X_{n-1}} a_{n-1} & -a_{n-1} 
\end{pmatrix}
\]

Thus we have a nontrivial solution if and only if $\det S_\eta(\lambda) = 0$.\\

However, we are not finished, since we would like (and we expect) the jump conditions to be independent of the choice of weight $\eta$. We prove that this is the case in the next lemma.

% Lemma : jump conditions

\begin{lemma}
We have a nontrivial solution if and only if $\det S(\lambda) = 0$, where

\[
S(\lambda) = A - \lambda^2 M I + R(\lambda)
\]

the matrix $A$ is given by

\begin{equation}\label{defA}
A = \begin{pmatrix}
-a_1 & a_1 \\
a_1 & -a_2 - a_1 & a_2 \\
& a_2 & -a_3 - a_2 & a_3 \\
\vdots & & & \vdots \\
& & & a_{n-1} & -a_{n-1} 
\end{pmatrix}
\end{equation}

and we have bound

\begin{equation}
||R(\lambda)|| \leq C 
\left( (e^{-\alpha X_m} + |\lambda|)^2 e^{-(\alpha - \eta)X_m}  
+ (e^{-\alpha X_m} + |\lambda| )|\lambda^2| \right)
\end{equation}

\begin{proof}
There is probably a much more elegant way to show this, but this one should work. For any nonsingular matrix $T$, $\det S_\eta(\lambda) = \det T^{-1} S_\eta(\lambda) T$. All we need to do is choose the appropriate matrix $T$. Let $T = U_1 U_2 \dots U_{n-1}$, where the $U_i$ are the elementary matrices

\begin{align*}
U_1 &= \text{diag}(1, e^{2 \eta X_1}, 1, \dots, 1) \\
U_2 &= \text{diag}(1, 1, e^{2 \eta (X_1 + X_2)}, \dots, 1) \\
&\vdots \\
U_{n-1} &= \text{diag}(1, 1, \dots, e^{2 \eta (X_1 + X_2 + \dots + X_{n-1})}) \\
\end{align*}

First, we look at $T^-1 A_\eta T = U_{n-1}^{-1} \dots U_2^{-1} U_1^{-1} A_\eta U_1 U_2 \dots U_{n-1}$. We perform the multiplication from inside to outside. For the innermost multiplication,

\begin{align*}
U_1^{-1} A_\eta U_1 &= \begin{pmatrix}
-a_1 & e^{2 \eta X_1} e^{-2 \eta X_1} a_1 \\
e^{-2 \eta X_1} e^{2 \eta X_1} a_1 & e^{2 \eta X_1} e^{-2 \eta X_1}(-a_2 - a_1) & e^{-2 \eta X_1}e^{-2 \eta X_2} a_2 \\
& e^{2 \eta X_1} e^{2 \eta X_2} a_2 & -a_3 - a_2 & e^{-2 \eta X_3} a_3 \\
\vdots & & & \vdots \\
& & & e^{2 \eta X_{n-1}} a_{n-1} & -a_{n-1} 
\end{pmatrix}\\
&= \begin{pmatrix}
-a_1 & a_1 \\
a_1 & -a_2 - a_1 & e^{-2 \eta (X_1+X_2)} a_2 \\
& e^{2 \eta (X_1+X_2)} a_2 & -a_3 - a_2 & e^{-2 \eta X_3} a_3 \\
\vdots & & & \vdots \\
& & & e^{2 \eta X_{n-1}} a_{n-1} & -a_{n-1} 
\end{pmatrix}
\end{align*}

Repeat this $n-2$ more times to get $T^{-1} A_\eta T = A$. Since $M I$ is diagonal, we can see that $T^{-1} MI T = MI$. Finally, for the remainder term, let $R(\lambda) = T^{-1} \tilde{R}(\lambda)T$. Then we have 

\begin{align*}
||T^{-1} \tilde{R}(\lambda)T|| \leq ||T^{-1} ||\:|| \tilde{R}(\lambda)||\:||T|| = || \tilde{R}(\lambda)||
\end{align*}

Thus $R(\lambda)$ has the same bound as $\tilde{R}(\lambda)$. Combining all of these gives us the result.

\end{proof}
\end{lemma}


\bibliography{suspension.bib}


\end{document}