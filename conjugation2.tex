\documentclass[12pt]{article}
\usepackage[pdfborder={0 0 0.5 [3 2]}]{hyperref}%
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}%
\usepackage[shortalphabetic]{amsrefs}%
\usepackage{amsmath}
\usepackage{enumerate}
% \usepackage{enumitem}
\usepackage{amssymb}                
\usepackage{amsmath}                
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage[table,xcdraw]{xcolor}
\usepackage{tikz}
\usepackage{float}
\usepackage{booktabs}
\usepackage{svg}
\usepackage{mathtools}
\usepackage{cool}
\usepackage{url}
\usepackage{graphicx,epsfig}
\usepackage{makecell}
\usepackage{array}

\def\noi{\noindent}
\def\T{{\mathbb T}}
\def\R{{\mathbb R}}
\def\N{{\mathbb N}}
\def\C{{\mathbb C}}
\def\Z{{\mathbb Z}}
\def\P{{\mathbb P}}
\def\E{{\mathbb E}}
\def\Q{\mathbb{Q}}
\def\ind{{\mathbb I}}

\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\ran}{ran}

\graphicspath{ {periodic/} }

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{hypothesis}{Hypothesis}

\newtheorem{notation}{Notation}

\begin{document}

\section{Stability of Periodic Multi-Pulse Solutions}

\subsection{Background and Eigenvalue Problem}

In a previous section, we showed that a periodic $n-$pulse solution $q_{np}(x)$ to KdV5 exists, where the distances between the peaks are given by the $n$ lengths $X_0, \dots, X_{n-1}$. We can write $q_{np}(x)$ piecewise as

\[
q_i^\pm(x) = q^\pm(x; \beta_i^\pm) + u_i^\pm(x)
\]

on the $2n$ intervals 

\begin{align*}
\{ [-X_{i-1}, 0], [0, X_i] \} && i = 0, \dots, n-1
\end{align*}

where the subscript $i$ is $\mod n$, since we are in a periodic domain. The distances between consecutive peaks in $q_{np}(x)$ are $2 X_0, \dots, 2 X_{n-1}$.\\

The functions $q^\pm(x; \beta_i^\pm)$ evolve in the $W^s(0)$ and $W^u(0)$, with initial conditions $\beta_i^\pm$. The functions $u_i^\pm(x)$ are small remainder terms. From the existence problem, we have bounds on all of these terms.

\begin{align*}
|q^\pm(x; \beta_i^\pm)| &\leq C |\beta_i^\pm| e^{-\alpha |x|} \\
|\beta_i^\pm| &\leq C (e^{-2 \alpha X_i} + e^{-2 \alpha X_{i-1}}) \\
|u_i^-(x)| &\leq C e^{-\alpha X_{i-1}} e^{-\alpha(X_{i-1} + x) } \\
|u_i^+(x)| &\leq C e^{-\alpha X_i} e^{-\alpha(X_i - x) } 
\end{align*}

Our goal is to determine the linear stability of periodic multi-pulse solutions to KdV5. To do this, we look at the PDE eigenvalue problem resulting from the linearization of KdV5 about an the periodic multi-pulse $q_{np}(x)$.\\

Recall that the linearization of KdV5 about an equilibrium solution $q(x)$ is given by the linear operator $L(q)$.

\begin{equation}
L(q) = \partial_x^5 - \partial_x^3 + c \partial_x
 - 2 q \partial_x - 2 q_x
\end{equation}

We can write this operator as $L = \partial_x H(q)$, where $H(q)$ is the self-adjoint operator

\begin{equation}
H(q) = \partial_x^4 - \partial_x^2 + c - 2 q
\end{equation}

Thus for the adjoint operator $L(q)^*$, we have

\begin{equation}
L(q)^* = -H(q) \partial_x
\end{equation}

It is not hard to show that

\begin{align*}
L(q)q_x &= 0 \\
L(q)(-\partial_c q) &= q_x \\
L(q)^* q &= 0
\end{align*}

In addition, for a finite or periodic domain, the constant functions are in the kernel of $L(q)^*$.

\[
L(q)^* 1 = 0
\]

For the primary pulse solution $q(x)$ to KdV5, we will assume that that $L(q)$ has a one-dimensional kernel which is spanned by $q_x$. If there were another generalized eigenfunction in the kernel of $L(q)$, we would be able to solve the equation $L(q) v = \partial_c q$, which is only possible if $\partial_c q \perp \ker L(q)^*$. In particular, this would require $\partial_c q \perp q$. We will assume this is not the case, i.e. we take the following hypothesis.

\begin{hypothesis}\label{Melnikovnonzero}
The following higher order Melnikov integral is nonzero.
\[
M = \langle q, \partial_c q \rangle_{L^2(\R)} 
= \int_{-\infty}^\infty q(x) \partial_c q(x) dx \neq 0
\]
\end{hypothesis}

Returning to the linearization about the periodic multipulse $q_{np}(x)$, let $V = (v, v_x, v_{xx}, v_{xxx}, v_{xxxx})^T$, and write the eigenvalue problem $L(q_{np}(x))v(x) = \lambda v(x)$ as the first-order system

\begin{align*}
V'(x) = A(q_{np}(x)) V(x) + \lambda B V(x)
\end{align*}

where

\begin{align*}
A(q(x)) &= \begin{pmatrix}0 & 1 & 0 & 0 & 0 \\0 & 0 & 1 & 0 & 0 \\0 & 0 & 0 & 1 & 0 \\0 & 0 & 0 & 0 & 1 \\
2 \partial_x q(x) + \lambda & 2 q(x) - c & 0 & 1 & 0 \end{pmatrix}, &&
B = \begin{pmatrix}0 & 0 & 0 & 0 & 0 \\0 & 0 & 0 & 0 & 0 \\0  & 0 & 0 & 0 & 0 \\0 & 0 & 0 & 0 & 0 \\1 & 0 & 0 & 0 & 0 \end{pmatrix} 
\end{align*}

\subsection{Variational Equation}

Consider the variational and adjoint variational equations for the linearization about the primary pulse solution $q(x)$ to KdV5. These are given by

\begin{align}
V' = A(q(x))V \label{vareq} \\
W' = -A(q(x))^*W \label{adjvareq}
\end{align}

Note that $A(q(x))$ is exponentially asymptotic to the constant-coefficient matrix $A(0)$, which is given by

\begin{align}\label{A0}
A(0) = \begin{pmatrix}0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 & 1 \\
0 & -c & 0 & 1 & 0 
\end{pmatrix}
\end{align}

For $c > 1/4$, $A(0)$ has eigenvalues $\nu = \{ 0, \pm \alpha_0 \pm \beta_0 i\}$, where $\alpha_0, \beta_0 > 0$. The eigenvectors of $A(0)$ and $-A(0)^*$ corresponding to the eigenvalue 0 are $V_0$ and $W_0$ (respectively), which are given by

\begin{align*}
V_0 &= (1/c, 0, 0, 0, 0)^T \\
W_0 &= (c, 0, -1, 0, 1)^T 
\end{align*}

where we have scaled $V_0$ so that $\langle W_0, V_0 \rangle = 1$.\\

Since $A(0)$ is not hyperbolic, we cannot directly apply the results of San98. Thus the equilibrium at 0 has 2-dimensional stable/unstable manifolds, and a 1-dimensional center manifold. Let $W^{s/u/c}(0)$ be these manifolds.\\

Since $L(q)q_x = 0$ and $L(q)(-\partial_c q) q_x$, we have the expressions

\begin{align*}
(Q')' &= A(q) Q' \\
(\partial_c Q)' &= A(q) (\partial_c Q) + B Q'
\end{align*}

For the adjoint variational problem, we have an exponentially decaying solution $\Psi(x)$, which is given by

\begin{equation}\label{Psi}
\Psi(x) = \begin{pmatrix}
q^{(4)}(x) - q''(x) + (-2q(x) + c)q(x)\\
-q^{(3)}(x) + q'(x) \\
q''(x) - q(x) \\
-q'(x) \\
q(x)
\end{pmatrix}
\end{equation}

For a bounded or periodic domain, we also have a solution $\Psi^c(x)$ to the adjoint variational problem which is bounded but does not decay exponentially.

\begin{align*}
\Psi^c(x) = (c - 2 q(x), 0, -1, 0, 1)^T
\end{align*}

In fact, $\Psi^c(x) \rightarrow W_0$ as $x \rightarrow \pm \infty$.

We will now decompose the tangent space at $Q(0)$. First, we make the following non-degeneracy assumption.

\begin{hypothesis}\label{nondegen}
\[
T_{Q(0)} W^u(0) \cap T_{Q(0)} W_s(0) = \R Q'(0)
\]
\end{hypothesis}

where $Q(x)$ is the single pulse solution on $\R$, written as a vector-valued function in $R^5$. Next, we define $Y^-$ and $Y^+$ to be the remaining dimensions of the tangent space of the unstable/stable manifolds.

\begin{align*}
T_{Q(0)} W^u(0) &= \R Q'(0) \oplus Y^- \\
T_{Q(0)} W^s(0) &= \R Q'(0) \oplus Y^+
\end{align*}

So far $\text{dim }\R Q'(0) \oplus Y^- \oplus Y^+ = 3$. To fill out the remaining 2 dimensions, we look at solutions to the adjoint variational equation.\\

First, we summarize some useful facts about the variational equation in the following lemma. We define the inner product on $\C^n$ by $\langle x, y \rangle = \sum_i x_i \bar{y_i}$, i.e. the complex conjugation is on the second component.

% lemma : facts about our eigenvalue problem

\begin{lemma}\label{eigadjoint}
Consider the linear ODE $V' = A(x)V$ and the corresponding adjoint problem $W' = -A(x)^* W$, where $A$ is an $n \times n$ matrix depending on $x$. Then the following are true.
\begin{enumerate}[(i)]
\item $\dfrac{d}{dx}\langle V(x), W(x) \rangle = 0$, thus the inner product is constant in $x$.
\item If $\Phi(y, x)$ is the evolution operator for $V' = A(x)V$, then $\Phi(x, y)^*$ is the evolution operator for the adjoint problem $W' = -W(x)^* W$.
\end{enumerate}
\begin{proof}
For (i), take the derivative of the inner product and use the expressions for $V'$ and $W'$. For (ii), take the derivative of the expression $\Phi(y, x)\Phi(x, y) = I$.
\end{proof}
\end{lemma}

Since $\langle \Psi(x), Q'(x) \rangle$ is constant in $x$ and $Q'(x) \rightarrow 0$ as $x \rightarrow \infty$, by the continuity of the inner product, we must have $\langle \Psi(0), Q'(0) \rangle = 0$. Similarly, if taking a solution $V(x)$ to the variational equation \eqref{vareq} with initial condition in $Y^+$ or $Y^-$, we can show that $\Psi(0) \perp Y^+$ and $\Psi(0) \perp Y^-$. The same holds for $\Psi^c(0)$. Thus we have shown that

\[
\Psi(0), \Psi^c(0) \perp \R Q'(0) \oplus Y^- \oplus Y^-
\]

Let 

\begin{equation}
S = \text{span }\{ \Psi(0), \Psi^c(0) \}
\end{equation}

Since $\Psi(0), \Psi^c(0)$ are linearly independent (but not orthogonal), $S$ has dimension 2. Since $S \perp \R Q'(0) \oplus Y^- \oplus Y^-$, we can write the tangent space at $Q(0)$ as the direct sum

\begin{equation}
\R^5 = \R Q'(0) \oplus Y^- \oplus Y^+ \oplus S
\end{equation}

\subsubsection{Piecewise Formulation}

To exploit these relations, we take the following piecewise ansatz for the eigenfunction $V(x)$

\begin{equation}
V_i^\pm(x) = d_i (Q_{np}'(x) + \lambda (Q_{np})_c(x)) + W_i^\pm 
\end{equation}

where the $V_i^-$ equation is defined on $[-X_{i-1}, 0]$, the $V_i^+$ equation is defined on $[0, X_i]$, and the $d_i \in \C$ are arbitrary constants. Substituting this into the eigenvalue problem and simplifying, we obtain the system for $W_i^\pm$

\begin{align*}
&(W_i^\pm)' = A( q_i^\pm(x) ) W_i^\pm + \lambda B W_i^\pm + \lambda^2 d_i \tilde{H}_i^\pm \\
&W_i^-(0) = W_i^+(0) \\
&W_i^\pm(0) \in S \oplus Y^+ \oplus Y^- \\
&W_i^+(X_i) - W_{i+1}^-(-X_i) = D_i d
\end{align*}

where

\begin{align*}
D_i d &= d_{i+1}[Q_{i+1}'(-X_i) + \lambda \partial_c Q_{i+1}(-X_i)]
- d_i [ Q_i'(X_i) + \lambda \partial_c Q_i(-X_i) ] \\
\tilde{H}_i^\pm &= -B \partial_c Q_i^\pm
\end{align*}

The conditions at $x = \pm X_i$ and $x = 0$ are the requisite matching conditions that guarantee continuity of the eigenfunction $V(x)$.\\

For the final form of the eigenvalue problem, we will combine the matrices $A( q_i^\pm(x) )$ and $\lambda B$ to obtain the piecewise eigenvalue problem

\begin{align*}
&(W_i^\pm)' = A_i^\pm(x; \lambda) W_i^\pm + \lambda^2 d_i \tilde{H}_i^\pm \\
&W_i^-(0) = W_i^+(0) \\
&W_i^\pm(0) \in S \oplus Y^+ \oplus Y^- \\
&W_i^+(X_i) - W_{i+1}^-(-X_i) = D_i d
\end{align*}

where

\begin{align*}
A_i^\pm(x; \lambda) &= A( q_i^\pm(x) ) + \lambda B 
\end{align*}

The system we will investigate is 

\begin{align*}
&(W_i^\pm)' = A_i^\pm(x; \lambda) W_i^\pm + \lambda^2 d_i \tilde{H}_i^\pm \\
&W_i^\pm(0) \in S \oplus Y^+ \oplus Y^- \\
&W_i^+(0) - W_i^-(0) \in S \\
&W_i^+(X_i) - W_{i+1}^-(-X_i) = D_i d
\end{align*}

A solution to this system solves the eigenvalue problem if and only if the $n$ jumps at $x = 0$, which can only be in the subspace $S$, are 0. Since $S$ is spanned by $\Psi(0)$ and $\Psi^c(0)$, this is true if and only if 

\begin{align*}
\langle \Psi(0), W_i^+(0) - W_i^-(0) \rangle &= 0 \\
\langle \Psi^c(0), W_i^+(0) - W_i^-(0) \rangle &= 0
\end{align*}

From the existence problem and from San98 we have the following estimates.

\begin{align*}
|H(x)|, |\tilde{H}_i^\pm(x)| &\leq C e^{-\alpha |x|} \\
|\Delta H_i^\pm| &= |\tilde{H}_i^\pm - H| \leq C(e^{-\alpha X_i} + e^{-\alpha X_{i-1}} ) \\
|\Delta H_i^-(x)| &\leq C e^{-\alpha X_{i-1}} e^{-\alpha(X_{i-1} + x) } \\
|\Delta H_i^+(x)| &\leq C e^{-\alpha X_i} e^{-\alpha(X_i - x) } \\
D_i d &= ( Q'(X_i) + Q'(-X_i))(d_{i+1} - d_i ) + \mathcal{O} \left( e^{-\alpha X_i} \left( |\lambda| +  e^{-\alpha X_i}  \right) |d| \right) \\
\end{align*}

\subsection{Conjugation}

To simplify the system, we would like to apply a change of coordinates so that the linear operator $A_i^\pm(x; \lambda)$ is transformed into a constant coefficient matrix. To do that, we will use the Conjugation Lemma, which follows from the Gap Lemma. Both are stated below.\\

First, we state and prove the Gap Lemma, which is modified from Zum2018. 

\begin{lemma}[Gap Lemma]\label{gaplemma}
Let $W \in \C^N$, and consider the family of ODEs on $\R$

\begin{equation}\label{LambdaEVP}
W(x)' = A(x; \Lambda) W
\end{equation}

where $\Lambda \in \Omega$ is a parameter vector and $\Omega$ is a Banach space. Assume that

\begin{enumerate}
	\item The map $\Lambda \mapsto A(\cdot; \Lambda)$ is analytic in $\Lambda$.
	\item $A(x; \Lambda) \rightarrow A_\pm(\lambda)$ (independent of $\Lambda$) as $x \rightarrow \pm \infty$, and for $|\Lambda| < \delta$ we have the uniform exponential decay estimates 
	\begin{align}
	\left| \frac{\partial^k}{\partial x^k} A(x; \Lambda) - A_\pm(\Lambda) \right| 
	&\leq C e^{-\theta |x|} && 0 \leq k \leq K
	\end{align}
	where $\alpha > 0$, $C > 0$, and $K$ is a nonnegative integer.
\end{enumerate}

Suppose $V^-(\Lambda)$ is an eigenvector of $A_-(\Lambda)$ with corresponding eigenvalue $\mu(\Lambda)$, both analytic in $\Lambda$. Then there exists a solution of \ref{LambdaEVP} of the form 

\begin{equation}
W(x; \Lambda) = V(x; \Lambda) e^{\mu(\Lambda)x}
\end{equation}

where $V$ is $C^1$ in $x$ and analytic in $\Lambda$ for $|\Lambda| < \delta$, and for any fixed $\tilde{\theta} < \theta$

\begin{align}
V(x; \Lambda) = V^-(\Lambda) + \mathcal{O}(e^{-\tilde{\theta}|x|}|V^-(\Lambda)|) && x < 0
\end{align}

\begin{proof}
This is almost identical to Zum2018. The only difference here is that the parameter vector $\Lambda$ is in a general Banach space instead of a subset of $C^p$.\\

Let $W(x; \Lambda) = V(x; \Lambda) e^{\mu(\Lambda) x}$. Substituting this into \eqref{LambdaEVP} and simplifying, we obtain the equivalent ODE

\begin{equation}\label{VEVP}
V(x; \Lambda)' = (A_- - \mu(\Lambda)I)V(x; \Lambda) + \Theta(x; \Lambda) V(x; \Lambda)
\end{equation}

where $\Theta(x; \Lambda) = (A(x; \Lambda) - A_-(\Lambda)) = \mathcal{O}(e^{-\theta|x|})$. Choose any $\tilde{\theta} < \theta_1 < \theta$ such that the real part of the spectrum of $A_-$ lies either to the left or to the right of the vertical line $\text{Re}(\nu) = \text{Re}(\mu(\Lambda) + \theta_1$ in the complex plane. We should be able to make sure this is case for all $|\Lambda| < \delta$ since all the eigenvalues of $A_(\Lambda)$ are analytic in $\Lambda$.\\

Then for $|\Lambda| < \delta$, we can define the spectral projections $P(\Lambda)$ and $Q(\Lambda)$, where $P(\Lambda)$ projects onto the direct sum of all eigenspaces of $A_-(\Lambda)$ corresponding to eigenvalues $\nu$ with $\text{Re}(\nu) < \text{Re}(\mu(\Lambda) + \theta_1$, and $Q(\Lambda)$ projects onto the direct sum of all eigenspaces of $A_-(\Lambda)$ corresponding to eigenvalues $\nu$ with $\text{Re}(\nu) > \text{Re}(\mu(\Lambda) + \theta_1$. $P(\Lambda)$ and $Q(\Lambda)$ are analytic in $\Lambda$ for $|\Lambda| < \delta$, and from our definition of $\theta_1$ we have the estimates

\begin{align*}
\left|e^{(A_-(\Lambda) - \mu(\Lambda)I)x}P \right| &\leq C e^{\theta_1 x} && x \geq 0 \\
\left|e^{(A_-(\Lambda) - \mu(\Lambda)I)x}Q \right| &\leq C e^{\theta_1 x} && x \leq 0
\end{align*}

Note that $P(\Lambda) + Q(\Lambda) = I$. Define the map $T$ on $L^\infty(-\infty, -M]$ by

\begin{align*}
TV(x; \Lambda) &= V^-(\Lambda) 
+ \int_{-\infty}^x e^{(A_-(\Lambda) - \mu(\Lambda)I)(x-y)}P\Theta(y; \Lambda) V(y; \Lambda) dy \\
&- \int_x^{-M} e^{(A_-(\Lambda) - \mu(\Lambda)I)(x-y)}Q\Theta(y; \Lambda) V(y; \Lambda) dy
\end{align*}

Taking the absolute value of both sides, for $x \leq 0$

\begin{align*}
|TV(x; \Lambda)| &\leq |V^-(\Lambda)| + C ||V(x; \Lambda)||_{L^\infty(-\infty, -M]}
\left( \int_{-\infty}^x e^{\theta_1 (x - y)} e^{\theta y} dy + \int_x^{-M} e^{\theta_1 (x - y)} e^{\theta y} dy \right) \\
&\leq |V^-(\Lambda)| + C ||V(x; \Lambda)||_{L^\infty(-\infty, -M]} e^{\theta_1 x} \int_{-\infty}^M e^{(\theta - \theta_1) y} dy \\
&= \leq |V^-(\Lambda)| + C ||V(x; \Lambda)||_{L^\infty(-\infty, -M]} e^{\theta_1 x} \frac{e^{-(\theta - \theta_1)M}}{\theta - \theta_1}\\
&\leq |V^-(\Lambda)| + C ||V(x; \Lambda)||_{L^\infty(-\infty, -M]} e^{\theta_1 x} e^{-(\theta - \theta_1)M} \\
&\leq |V^-(\Lambda)| + C ||V(x; \Lambda)||_{L^\infty(-\infty, -M]} e^{-(\theta - \theta_1)M} \\
& < \infty
\end{align*}

Since the RHS is independent of $x$, we have $T: L^\infty(-\infty, -M] \rightarrow L^\infty(-\infty, -M]$. Next, we look at

\begin{align*}
|TV_1(x; \Lambda) - TV_2(x; \Lambda)| &\leq C ||V_1(x; \Lambda) - V_2(x; \Lambda)||_{L^\infty(-\infty, -M]} e^{\theta_1 x} \frac{e^{-(\theta - \theta_1)M}}{\theta - \theta_1}\\
\end{align*}

Since $e^{-(\theta - \theta_1)M} \rightarrow 0$ as $m \rightarrow \infty$, for sufficiently large $M$ we have 

\begin{align*}
|TV_1(x; \Lambda) - TV_2(x; \Lambda)|_{L^\infty(-\infty, -M]} &\leq \frac{1}{2} ||V_1(x; \Lambda) - V_2(x; \Lambda)||_{L^\infty(-\infty, -M]} 
\end{align*}

Thus the map $T$ is a contraction. Since $L^\infty(-\infty, -M]$ is a Banach space, by the Banach fixed point theorem, the map $T$ has a unique fixed point $V = TV$, i.e. we have a function $V \in L^\infty(-\infty, -M]$ such that 

\begin{align*}
V(x; \lambda) &= V^-(\Lambda) 
+ \int_{-\infty}^x e^{(A_-(\Lambda) - \mu(\Lambda)I)x}P\Theta(y; \Lambda) V(y; \Lambda) dy 
- \int_x^{-M} e^{(A_-(\Lambda) - \mu(\Lambda)I)x}Q\Theta(y; \Lambda) V(y; \Lambda) dy
\end{align*}

Differentiating this with respect to $x$, we obtain

\begin{align*}
V'(x; \Lambda) &= P\Theta(x; \Lambda) V(x; \Lambda) +
(A_-(\Lambda) - \mu(\Lambda)I) \int_{-\infty}^x e^{(A_-(\Lambda) - \mu(\Lambda)I)(x-y)}P\Theta(y; \Lambda) V(y; \Lambda) dy \\
&-(-Q\Theta(x; \Lambda) V(x; \Lambda))
-(A_-(\Lambda) - \mu(\Lambda)I) \int_x^{-M} e^{(A_-(\Lambda) - \mu(\Lambda)I)(x-y)}Q\Theta(y; \Lambda) V(y; \Lambda) dy \\
&= P\Theta(x; \Lambda) V(x; \Lambda) + Q\Theta(y; \Lambda) V(x; \Lambda) + (A_-(\Lambda) - \mu(\Lambda)I)(T V(x; \lambda) - V^-(\Lambda) ) \\
&= (P + Q)\Theta(x; \Lambda) V(x; \Lambda) + (A_-(\Lambda) - \mu(\Lambda)I)(V(x; \lambda) - V^-(\Lambda) ) \\
&= (A_-(\Lambda) - \mu(\Lambda)I)V(x; \lambda) + \Theta(x; \Lambda) V(x; \Lambda) - (A_-(\Lambda) - \mu(\Lambda)I)V^-(\Lambda) \\
&= (A_-(\Lambda) - \mu(\Lambda)I)V(x; \lambda) + \Theta(x; \Lambda) V(x; \Lambda)
\end{align*}

where we used the fact that $TV = V$ and $(A_-(\Lambda) - \mu(\Lambda)I)V^-(\Lambda) = 0$. Thus $V(x; \Lambda$ solves \eqref{VEVP}. Since $TV = V$, we let $V_1 = V$ and $V_2 = 0$ in the above to get the estimate

\begin{align*}
|V(x; \Lambda) - V^-(\Lambda)| &= |T(V(x; \Lambda)) - T(0)| \\
&\leq C ||V(x; \Lambda) - 0||_{L^\infty(-\infty, -M]} e^{\theta_1 x} \\
\end{align*}

Similarly, for sufficiently large $M$, we have

\begin{align*}
|V(x; \Lambda)| - |V^-(\Lambda)| &\leq | |V(x; \Lambda)| - |V^-(\Lambda)| | \\
&\leq |V(x; \Lambda) - V^-(\Lambda)| \\
&= |T(V(x; \Lambda)) - T(0)| \\
&\leq \frac{1}{2} ||V(x; \Lambda)||_{L^\infty(-\infty, -M]}
\end{align*}

Thus

\begin{align*}
||V(x; \Lambda)||_{L^\infty(-\infty, -M]} \leq 2 |V^-(\Lambda)|
\end{align*}

Combining these, we have

\begin{align*}
|V(x; \Lambda) - V^-(\Lambda)| &\leq C e^{\tilde{\theta} x}|V^-(\Lambda)| \\
\end{align*}

from which we get

\begin{align*}
|V(x; \Lambda) = V^-(\Lambda) + \mathcal{O}( e^{\tilde{\theta} x}|V^-(\Lambda)| )\\
\end{align*}

At the moment, $V(x; \Lambda)$ is only defined for $x < -M$. We extend $V(x; \Lambda)$ to all of $R^-$ using the evolution operator for the system.

\end{proof}
\end{lemma}

As a corollary to this, we state and prove the Conjugation Lemma, which allows us to make a smooth change of coordinates to convert the linear ODE $Z'(x) = A^\pm(x) Z(x)$ into a constant coefficient system.

\begin{lemma}[Conjugation Lemma]
Let $W \in \C^N$, and consider the family of ODEs on $\R$

\begin{equation}\label{EVPconj}
W(x)' = A(x; \Lambda) W(x) + F(x) 
\end{equation}

where $\Lambda \in \Omega$ is a parameter vector and $\Omega$ is a Banach space. Take the same assumptions as in the Gap Lemma, i.e. 

\begin{enumerate}
	\item The map $\Lambda \mapsto A(\cdot; \Lambda)$ is analytic in $\Lambda$.
	\item $A(x; \Lambda) \rightarrow A_\pm(\lambda)$ (independent of $\Lambda$) as $x \rightarrow \pm \infty$, and for $|\Lambda| < \delta$ we have the uniform exponential decay estimates 
	\begin{align}
	\left| \frac{\partial^k}{\partial x^k} A(x; \Lambda) - A_\pm(\Lambda) \right| 
	&\leq C e^{-\theta |x|} && 0 \leq k \leq K
	\end{align}
	where $\alpha > 0$, $C > 0$, and $K$ is a nonnegative integer.
\end{enumerate}

Then in a neighborhood of any $\Lambda_0 \in \Omega$ there exist invertible linear transformations

\begin{align*}
P_+(x, \Lambda) &= I + \Theta_+(x, \Lambda) \\
P_-(x, \Lambda) &= I + \Theta_-(x, \Lambda) 
\end{align*}

defined on $\R^+$ and $\R^-$, respectively, such that

\begin{enumerate}[(i)]
\item The change of coordinates $W = P_\pm Z$ reduces \eqref{EVPconj} to the equations on $\R^\pm$

\begin{align}\label{conjZ}
Z'(x) = A^\pm(\Lambda) Z(x) + P_\pm(x, \Lambda)^{-1} F(x)
\end{align}

\item For any fixed $0 < \tilde{\theta} < \theta$, $0 \leq k \leq K+1$, and $j \geq 0$ we have the decay rates
\begin{align*}
\left| \partial_\Lambda^j \partial_x^k \Theta_\pm \right| \leq C(j, k)e^{-\tilde{\theta}|x|}
\end{align*}
\end{enumerate}
\begin{proof}
We prove this for the case where $B(x) = 0$ and $F(x) = 0$. The form of the conjugated system easily follow for general $F$.\\

We will do the case on $\R^-$. The other case is similar. 
Let $W = P_-(x, \Lambda) Z$, where we will figure out what $P_-(x, \Lambda)$ is later. Suppose that \eqref{conjZ} holds, and substitute these into \eqref{EVPconj}.

\begin{align*}
[P_-(x, \Lambda) Z(x)]' &= A(x; \Lambda)(P_-(x, \Lambda) Z(x)) \\
P_-'(x, \Lambda) Z(x) + P_-(x, \Lambda) Z'(x)
&= A(x; \Lambda)P_-(x, \Lambda) Z(x) \\
P_-'(x, \Lambda) Z(x) + P_-(x, \Lambda) A_- Z(x)
&= A(x; \Lambda)P_-(x, \Lambda) Z(x)
\end{align*}

Rearranging this, we obtain

\begin{equation}
P_-'(x, \Lambda) Z(x)
= [A(x; \Lambda)P_-(x, \Lambda) - P_-(x, \Lambda) A_-]Z(x)
\end{equation}

Suppose now that
\[
P_-'(x, \Lambda) = A(x; \Lambda)P_-(x, \Lambda) - P_-(x, \Lambda) A_-
\]

Then, upon making the substitution $W = P_-(x, \Lambda) Z$, \eqref{EVPconj} reduces to

\begin{align*}
[P_-(x, \Lambda) Z(x)]' &= A(x; \Lambda)(P_-(x, \Lambda) Z(x)) \\
P_-'(x, \Lambda) Z(x) + P_-(x, \Lambda) Z'(x)
&= A(x; \Lambda)P_-(x, \Lambda) Z(x) \\
(A(x; \Lambda)P_-(x, \Lambda) - P_-(x, \Lambda) A_-)Z(x) + P_-(x, \Lambda) Z'(x)
&= A(x; \Lambda)P_-(x, \Lambda) Z(x) \\
A(x; \Lambda)P_-(x, \Lambda)Z(x) - P_-(x, \Lambda) A_- Z(x) + P_-(x, \Lambda) Z'(x)
&= A(x; \Lambda)P_-(x, \Lambda) Z(x) \\
P_-(x, \Lambda) Z'(x) &= P_-(x, \Lambda) A_- Z(x) \\
Z'(x) &= A_- Z(x)
\end{align*}

which is what we want. In the last line, we used the fact that $P_-(x, \Lambda)$ is invertible, so we should make sure that is the case. Thus, we wish to find $P_-(x, \Lambda)$ such that

\[
P_-'(x, \Lambda) = A(x; \Lambda)P_-(x, \Lambda) - P_-(x, \Lambda) A_-
\]

We note that the this equation has the form 

\begin{equation}\label{solvePminus}
P_-'(x, \Lambda) = \mathcal{A}(x; \Lambda) P_-(x, \Lambda)
\end{equation}

where $\mathcal{A}(x; \Lambda)$ is the linear operator

\[
\mathcal{A}(x; \Lambda) P = A(x; \Lambda) P - P A_-
\]

By our assumptions on $A(x; \Lambda)$, $\mathcal{A} \rightarrow \mathcal{A}_-$ as $x \rightarrow -\infty$, where the limiting linear operator $\mathcal{A}_-$ is defined by

\[
\mathcal{A}_- P = A_- P - P A_-
\]

The limiting operator has analytic eigenvalue/eigenvector pair $0, I$ for all $\Lambda$, thus by the Gap Lemma, there exists a solution of \eqref{solvePminus} of the form 

\begin{equation}
P_-(x, \Lambda) = I + \mathcal{O}(e^{-\tilde{\theta}|x|})
\end{equation}

In other words, 

\begin{equation}
P_-(x, \Lambda) = I + \Theta_-(x, \Lambda)
\end{equation}

where 

\begin{equation}\label{Thetabound}
|\Theta_-(x, \Lambda)| \leq C e^{-\tilde{\theta}|x|}
\end{equation}

The $x$-derivative bound follow from the derivative bounds in the Gap Lemma, and the $\Lambda$-derivative bounds follow from standard analytic function theory.\\

Finally, we need to show that $P_-(x, \Lambda)$ is invertible for all $x \in \R^-$. Using \eqref{Thetabound}, we can find $M$ sufficiently large and negative such that for all $x \leq M$,

\[
|\Theta_-(x, \Lambda)| < 1/2
\]

It follows that $P_-(x, \Lambda)$ is invertible for $X \leq M$. To extend invertibility to all $x \in \R^-$, suppose that $P_-(x, \Lambda)^{-1}$ exists for all $x \in R^-$. Then, differentiating $P_-(x, \Lambda)^{-1} P_-(x, \Lambda) = I$ and solving for $[P_-(x, \Lambda)^{-1}]'$ (as in the proof of the inverse function theorem), we have (suppressing the dependence on $\Lambda$ for convenience)

\begin{align*}
(P_-^{-1})'(x) &= -P_-^{-1}(x)P_-'(x)P_-^{-1}(x) \\
&= -P_-^{-1}(x)( A(x)P_-(x) - P_-(x) A_-)P_-^{-1}(x) \\
&= A_- P_-^{-1}(x) - A(x) P_-^{-1}(x)
\end{align*}

We have a solution to this ODE for $x \leq M$, and by variation of constants, this ODE has a unique solution for all $x \in \R^-$. Thus $P_-(x, \Lambda)^{-1}$ is obtained for all $x \in \R^-$ by evolving this ODE forward from an initial condition at some $x \leq M$. In this manner, we have shown that $P_-(x, \Lambda)^{-1}$ exists for all $x \in \R^-$.

\end{proof}
\end{lemma}

We will use the Conjugation Lemma to transform the linear operator $A_i^\pm(x; \lambda) = A( q_i^\pm(x) ) + \lambda B$ into a constant coefficient matrix. For all $\lambda$, $A^\pm(x; \lambda)$ decays exponentially to the constant-coefficient matrix $A(\lambda)$. 

\begin{align}\label{Alambda}
A(\lambda) &= \begin{pmatrix}0 & 1 & 0 & 0 & 0 \\0 & 0 & 1 & 0 & 0 \\0 & 0 & 0 & 1 & 0 \\0 & 0 & 0 & 0 & 1 \\
\lambda & -c & 0 & 1 & 0 \end{pmatrix}
\end{align}

Let $\Lambda = (\lambda, q(x))$ be the parameter vector we will use in the Conjugation Lemma, where $q(x)$ is in the Banach space of continuous functions on $[X_{i-1}, 0]$ or $[0, X_i]$. (This requires a version of the Conjugation Lemma which allows parameters to be in an arbitrary Banach space.) Since 
$A( q(x) ) + \lambda B$ is linear, thus analytic, in $\lambda$ and in $q(x)$, let $P_i^\pm(x; \lambda, q(x)$ be the conjugation operator for $A( q(x) ) + \lambda B$. For convenience, let  

\begin{equation}
P_i^\pm(x; \lambda) = P^\pm(x; \lambda, q_i^\pm(x) )
\end{equation}

and let

\begin{equation}
P^\pm(x; \lambda) = P^\pm(x; \lambda, q(x) )
\end{equation}

Using the Conjugation Lemma, we make the substitution $W_i^\pm = P_i^\pm(x; \lambda) Z_i^\pm$. Then our system becomes

\begin{align*}
&(Z_i^\pm(x))' = A(\lambda) Z_i^\pm(x) + \lambda^2 d_i P_i^\pm(x; \lambda)^{-1} \tilde{H}_i^\pm(x) \\
&P_i^\pm(0; \lambda) Z_i^\pm(0) \in \C \Psi(0) \oplus Y^+ \oplus Y^- \oplus Y^0 \\
&P_i^+(0; \lambda) Z_i^+(0) - P_i^-(0; \lambda) Z_i^-(0) \in S  \\
&P_i^+(X_i; \lambda) Z_i^+(X_i)\ - P_{i+1}^-(-X_i; \lambda) Z_{i+1}^-(-X_i; \lambda) = D_i d
\end{align*}

and the jump conditions become

\begin{align*}
\langle \Psi(0), P_i^+(0; \lambda) Z_i^+(0) - P_i^-(0; \lambda) Z_i^-(0) \rangle &= 0 \\
\langle \Psi^c(0), P_i^+(0; \lambda) Z_i^+(0) - P_i^-(0; \lambda) Z_i^-(0) \rangle &= 0
\end{align*}

\subsection{Evolution}

Since $A(\lambda)$ is constant coefficient, we know exactly how solutions of $V' = A(\lambda)V$ will behave.\\

Since $A(\lambda)$ depends linearly on $\lambda$ and $A(0)$ has a simple eigenvalue at 0, for sufficiently small $\lambda$, $A(\lambda)$ will have a simple, small eigenvalue $\nu(\lambda)$, and $\nu(\lambda) = \mathcal{O}(\lambda)$.\\ 

We define the following.

\begin{enumerate}
	\item Let

	\begin{align*}
	X_m &= \min(X_0, \dots, X_{n-1}) \\
	X_M &= \max(X_0, \dots, X_{n-1}) \\
	\end{align*}

	\item Choose $\rho > 0$ sufficiently small so that $\alpha_0 - 4 \rho > 0$. Let

	\begin{align*}
	\alpha &= \alpha_0 - \rho \\
	\tilde{\alpha} &= \alpha - 3 \rho
	\end{align*}

	\item Choose $\delta$ sufficiently small so that for all $|\lambda| < \delta$
	\begin{enumerate}
		\item For the small eigenvalue of $A(\lambda)$ we have $|\nu(\lambda)| < \rho$
		\item The real part of any other eigenvalue of $A(\lambda)$ lies outside the interval $[-\alpha, \alpha]$.
	\end{enumerate}

	\item Choose $X_m$ sufficiently large so that
	\begin{equation}
	e^{-\tilde{\alpha} X_m} < \delta
	\end{equation}

\end{enumerate}

Let $E^{u/s/c}(\lambda)$ be the stable/unstable/center eigenspaces of $A(\lambda)$, where $E^c$ is the one-dimensional subspace spanned by the eigenvector corresponding to the small eigenvalue $\nu(\lambda)$. This is a ``true'' center subspace only when $\nu(\lambda)$ has no real part, but we will always call it a center subspace for convenience. Let $P^{u/s/c}_0(\lambda)$ be the corresponding eigenprojections for the eigenspaces $E^{u/s/c}(\lambda)$.\\

Let $\Phi(x, y; \lambda) = e^{A(\lambda)(x-y)}$ be the evolution of the constant-coefficient ODE

\[
Z' = A(\lambda) Z
\]

Let $\Phi^{u/s/c}(x, y; \lambda) = \Phi(x, y; \lambda)P^{u/s/c}_0(\lambda)$ be the evolutions on the respective eigenspaces. For these evolutions, we have bounds

\begin{align*}
|\Phi^s(x, y; \lambda)| &\leq C e^{-\alpha(x - y)} \\
|\Phi^u(x, y; \lambda)| &\leq C e^{-\alpha(y - x)} \\
|\Phi^c(x, y; \lambda)| &\leq C e^{\rho|x - y|} 
\end{align*}

Since $E^c(\lambda)$ is one-dimensional, we have a formula for $\Phi^c(x, y; \lambda)$.

\begin{align*}
\Phi^c(x, y; \lambda) v &= e^{\nu(\lambda)(x - y)} v && v \in E^c(\lambda)
\end{align*}

Finally, we will look at the variational and adjoint variational e
equations for the linearization about the primary pulse. Recall that these are given by 

\begin{align*}
V_i' &= A(q(x)) V_i \\
W_i' &= -A(q(x))^* W_i
\end{align*}

Let $\Theta(y, x)$ be the evolution operator for the variational equation. Then $\Theta(x, y)^*$ is the evolution operator for the adjoint variational equation. Then (as defined above) $P^\pm(x)$ conjugate $A(q(x))$. We have the following relationship between $\Theta(y, x)$ and $\Phi(y, x;, 0)$.

\begin{align*}
\Theta(y, x) &= P^+(y) \Phi(y, x; 0) P^+(x)^{-1} && x, y \geq 0 \\
\Theta(y, x) &= P^-(y) \Phi(y, x; 0) P^-(x)^{-1} && x, y \leq 0
\end{align*}

Finally, recalling that

\begin{align*}
P_i^\pm(x; \lambda) = P^\pm(x; \lambda; q_i^\pm(x)) \\
P^\pm(x; \lambda) = P^\pm(x; \lambda; q(x))
\end{align*}

we can expand $P_i^\pm(x; \lambda)$ in a Taylor series about $(0, q(x))$ to get

\begin{align*}
P_i^\pm(x; \lambda) = P^\pm(0) + C_1 |\lambda| 
+ C_2| q_i^\pm(x) - q(x) | + \text{h.o.t.}
\end{align*}

Since 

\begin{align*}
| q_i^\pm(x) - q(x) | &= | q^\pm(x; \beta_i^\pm) + u_i^\pm(x) - q(x) | \\
&\leq |q^\pm(x; \beta_i^\pm) - q(x)| + |u_i^\pm(x)|
\end{align*}

we have at $x = 0$

\begin{equation}\label{PTaylor}
P_i^\pm(0; \lambda) = P^\pm(0) + \mathcal{O}(|\lambda| + e^{-2 \alpha X_m})
\end{equation}

We will only need the Taylor expansion at $x = 0$, since for large $x$, the conjugation operators are approximately equal to the identity.

\subsection{Inversion}

Define the spaces

\begin{align*}
V_a &= \bigoplus_{i=0}^{n-1} E^u(\lambda) \oplus E^s(\lambda) \\
V_b &= \bigoplus_{i=0}^{n-1} E^u(0) \oplus E^s(0) \\
V_c^- &= \bigoplus_{i=0}^{n-1} E^c(\lambda) \\
V_c^+ &= \bigoplus_{i=0}^{n-1} E^c(\lambda) \\
V_c &= V_c^- \oplus V_c^+ \\
V_\lambda &= B_\delta(0) \subset \C
\end{align*}

where the subscripts are all $\mod n$, as in the existence problem. We use the $\lambda-$dependent eigenspaces for $a_i^\pm$ and $c_i^\pm$, since we will be evolving them under the $\lambda-$dependent evolution $\Phi(y, x; \lambda)$.\\

All the product spaces are endowed with the maximum norm, e.g. for $V_c$, $|c| = \max(|c_0^-|, \dots, |c_{n-1}^-|, |c_0^+|, \dots, |c_{n-1}^+|)$. In addition, we take the following convention. If we eliminate either a subscript or a superscript (or both) in the norm, we are taking the maximum over the eliminated thing. For example,

\begin{enumerate}
	\item $|c_i| = \max(|c_i^+|, |c_i^-|)$ 
	\item $|c^+| = \max(|c_0^+|, \dots, |c_{n-1}^+|)$
\end{enumerate}

In order to solve our system, we first look at the ODE

\[
(Z_i^\pm(x))' = A(\lambda) Z_i^\pm(x) + \lambda^2 d_i P_i^\pm(x; \lambda)^{-1} \tilde{H}_i^\pm(x)
\]

The solution this solves the following fixed point equations. For $i = 0, \dots, n-1$, the fixed point equations for $Z_i^\pm(x)$ are

\begin{align*}
Z_i^-(x) &= \Phi^s(x, -X_{i-1}; \lambda) a_{i-1}^- + \Phi^u(x, 0; \lambda) b_i^- + \Phi^c(x, -X_{i-1}; \lambda) c_{i-1}^- \\
&+ \lambda^2 d_i \int_0^x \Phi^u(x, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y)] dy \\
&+ \lambda^2 d_i \int_{-X_{i-1}}^x \Phi^s(x, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy \\
&+ \lambda^2 d_i \int_{-X_{i-1}}^x \Phi^c(x, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy  \\ 
Z_i^+(x) &= \Phi^u(x, X_i; \lambda) a_i^+ + \Phi^s(x, 0; \lambda) b_i^+ + \Phi^c(x, X_i; \lambda) c_i^+ \\
&+ \lambda^2 d_i \int_0^x \Phi^s(x, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
&+ \lambda^2 d_i \int_{X_i}^x \Phi^u(x, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
&+ \lambda^2 d_i \int_{X_i}^x \Phi^c(x, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
\end{align*}

% match at ends

\subsubsection{Matching at ends}

In the first inversion lemma, we solve the matching conditions at $\pm X_i$, $i = 0, \dots, n-1$.

\[
P_i^+(X_i; \lambda) Z_i^+(X_i) - P_{i+1}^-(-X_i; \lambda) Z_{i+1}^-(-X_i) = D_i d
\]

% first inversion lemma : match at \pm X_i

\begin{lemma}\label{inv1}

There exists an operator

\begin{align*}
A_1: V_\lambda \times V_b \times V_c^- \times V_d \rightarrow V_a \times V_c^+\\
\end{align*}

such that $(a, c^+) = A_1(\lambda)(b, c^-,d)$ solves our system. This operator is analytic in $\lambda$ and linear in $(b,vc^-,d)$. Piecewise bounds for $A_1$ are given by

\begin{align}\label{A1bound}
|A_1&(\lambda)_i(b, c^-, d)|
\leq C \Big( e^{-\alpha X_i} (|b_i^+| + |b_{i+1}^-|) + |c_i^-| + e^{-(\alpha - \rho) X_i} |\lambda^2||d| + |D_i||d| \Big)
\end{align} 

In addition, we can write $a_i^\pm$ and $c_i^+$ as 

\begin{align*}
a_i^+ &= P_i^+(X_i; \lambda) P_0^u(\lambda) D_i d + A_2(\lambda)_i^+(b, c^-, d) \\
a_i^- &= -P_i^-(-X_i; \lambda) P_0^s(\lambda) D_i d + A_2(\lambda)_i^-(b, c^-, d) \\
c_i^+ &= c_i^- + P_0^c(\lambda) D_i d + A_2(\lambda)_i^c(b, c^-, d) )
\end{align*}

where $A_2$ is analytic in $\lambda$, linear in $(b, c^-, d)$, and has piecewise bounds

\begin{align*}
|A_2&(\lambda)_i(b, c^-, d)|
\leq C \Big( e^{-\alpha X_i} (|b_i^+| + |b_{i+1}^-| + |c_i^-|) + e^{-(\alpha - \rho) X_i} |\lambda|^2|d| + e^{-\alpha X_i} |D_i||d| \Big)
\end{align*}

Finally, we have the estimate

\begin{equation}\label{P0cDid}
|P_0^c(\lambda) D_i d| \leq C e^{-\alpha X_i}(|\lambda| + e^{-\alpha X_i})|d|
\end{equation}

\begin{proof}

At $\pm X_i$, the fixed point equations become

\begin{align*}
Z_{i+1}^-(-X_i) &= a_i^- + \Phi^u(-X_i, 0; \lambda) b_{i+1}^- + c_i^- 
+ \lambda^2 d_{i+1} \int_0^{-X_i} \Phi^u(-X_i, y; \lambda) P_{i+1}^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy \\
Z_i^+(X_i) &= a_i^+ + \Phi^s(X_i, 0; \lambda) b_i^+ + c_i^+ 
+ \lambda^2 d_i \int_0^{X_i} \Phi^s(X_i, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy
\end{align*}

To obtain these, we used the fact that, for example, $a_i^- \in E^s(\lambda)$ and $\Phi^s(-X_{i-1}, -X_{i-1}; \lambda)$ is the identity on $E^s(\lambda)$. From the Conjugation Lemma, we have the estimate

\begin{equation}\label{conjest}
P_i^\pm(\pm X_i; \lambda) = I + \mathcal{O}(e^{-\alpha X_i})
\end{equation}

which we will use on the $a_i^\pm$ and $c_i^\pm$ terms. Thus we obtain the equation

\begin{align}\label{Dideq1}
D_i d &= a_i^+ - a_i^- + c_i^+ - c_i^- + L_3(\lambda)_i(a, b, c^+, c^-, d)
\end{align}

For a bound on $L_3$, we look at the individual terms. As usual, we will in general only look at one of the two pieces.

\begin{enumerate}

\item For the $a_i^\pm$ and $c_i^\pm$ terms, we have a term of order $\mathcal{O}(e^{-\alpha X_i}(|a_i| + |c_i^+| + |c_i^-|)$, which comes from the estimate \eqref{conjest} for the conjugation operators $P_i^\pm(\pm X_i; \lambda)$.

\item For the terms involving $b$, we have

\[
| P_i^-(-X_i; \lambda) \Phi^u(-X_i, 0; \lambda) b_{i+1}^-| \leq C e^{-\alpha X_i} |b_{i+1}
^-|
\]

\item For the integral terms, we have

\begin{align*}
&\left|
P^+(X_i; \beta_i^+, \lambda) \int_0^{X_i} \Phi^s(X_i, y; \lambda) P^+(X_i; \beta_i^+, \lambda)^{-1} \tilde{H}_i^+(y) dy \right| \\
&\leq C \int_0^{X_i} e^{-\alpha(X_i - y)}e^{-\alpha y} dy \\
&\leq C \int_0^{X_i} e^{-(\alpha - \rho)(X_i - y)}e^{-\alpha y} dy \\
&= C e^{-(\alpha - \rho) X_i} \int_0^{X_i} e^{-\rho y} dy \\ 
&\leq C e^{-(\alpha - \rho) X_i} 
\end{align*}

\end{enumerate}

Putting these all together, we have the following bound for $L_3$.

\begin{equation}\label{L3bound}
|L_3(\lambda)_i(a, b, c^+, c^-, d)| \leq C \Big( e^{-\alpha X_i} ( |a_i| + |b_i^+| + |b_{i+1}^-| + |c_i^+| + |c_i^-|) + e^{-(\alpha - \rho) X_i} |\lambda^2| |d| \Big)
\end{equation}

Since $e^{-\alpha X_m} < \delta$, this becomes

\begin{align*}
|L_3(\lambda)_i(a, b, c^+, c^-, d)| \leq C \delta ( |a_i| + |c_i^+| ) + C\Big( e^{-\alpha X_i} ( |b_i^+| + |b_{i+1}^-| + |c_i^-|) + e^{-(\alpha - \rho) X_i} |\lambda^2| |d| \Big)
\end{align*}

Let 

\[
J_1: \bigoplus_{j=1}^n (E^s(\lambda) \times E^u(
\lambda) \times E^c(\lambda) ) \rightarrow \bigoplus_{j=1}^n \rightarrow \C^{2m+1}
\]

be defined by $(J_1)_i(a_i^+, a_i^-, c_i^+) = (a_i^+ - a_i^-, c_i^+)$. The map $J_i$ is a linear isomorphism since $E^s(\lambda) \oplus E^u(\lambda) \oplus E^c(\lambda) = \C^{2m + 1}$. Consider the map

\[
S_1(a, c^+) = J_1 (a, c^+) + L_3(\lambda)(a, 0, c^+, 0, 0) = J_1( I + J_1^{-1} L_3(\lambda)(a, 0, c^+, 0, 0))
\]

For sufficiently small $\delta$, we will have the operator norm $||J_1^{-1} L_3(\lambda)(a, 0, c^+, 0, 0)|| < 1$, thus the operator $S_1(a, c^+)$ is invertible. We can solve for $(a, c^+)$ to get

\[
(a, c^+) = A_1(\lambda)(b, c^-, d) = S_i^{-1}(-D d + L_3(\lambda)(0, b, 0, c^-, d)
\]

Using the bound on $L_3$ and noting which pieces are involved, $A_1$ will have piecewise bounds

\begin{align*}
|A_1&(\lambda)_i(b, c^-, d)|
\leq C \Big( e^{-\alpha X_i} (|b_i^+| + |b_{i+1}^-|) + |c_i^-| + e^{-(\alpha - \rho) X_i} |\lambda^2||d| + |D_i||d| \Big)
\end{align*} 

Next, we hit \eqref{Dideq1} with the projections $P_0^{s/u/c}(\lambda)$ on the eigenspaces $E^{s/u/c}(\lambda)$ to obtain the expressions

\begin{align*}
a_i^+ &= P_0^u(\lambda) D_i d + A_2(\lambda)_i^+(b, c^-, d) \\
a_i^- &= -P_0^s(\lambda) D_i d + A_2(\lambda)_i^-(b, c^-, d) \\
c_i^+ &= c_i^- + P_0^c(\lambda) D_i d + A_2(\lambda)_i^c(b, c^-, d) )
\end{align*}

The bound on the remainder term $A_2(\lambda)_i(b, c^-, d)$ is found by substituting the bound for $A_1$ into the bound for $L_3$ and simplifying. 

\begin{align*}
|A_2&(\lambda)_i(b, c^-, d)|
\leq C \Big( e^{-\alpha X_i} (|b_i^+| + |b_{i+1}^-| + |c_i^-|) + e^{-(\alpha - \rho) X_i} |\lambda|^2|d| + e^{-\alpha X_i} |D_i||d| \Big)
\end{align*} 

Anticipating what we will need at the end, we will derive slightly different expressions for $a_i^+$ and $a_i^-$. Using the conjugation operator $P_i^+(X_i; \lambda)$, we write $a_i^+$ as

\begin{align*}
a_i^+ = P_i^+(X_i; \lambda)a_i^+ + (I - P_i^+(X_i; \lambda))a_i^+ &= P_0^u(\lambda) D_i d + A_2(\lambda)_i^+(b, c^-, d)
\end{align*}

Rearranging this, we obtain

\begin{align*}
P_i^+(X_i; \lambda) a_i^+ &= P_0^u(\lambda) D_i d + A_2(\lambda)_i^+(b, c^-, d) - (I - P_i^+(X_i; \lambda))a_i^+ \\
&= P_0^u(\lambda) D_i d + A_2(\lambda)_i^+(b, c^-, d) + \mathcal{O}\Big( e^{-\alpha X_i} ( e^{-\alpha X_i} (|b_i^+| + |b_{i+1}^-|) + |c_i^-| + e^{-(\alpha - \rho) X_i} |\lambda^2||d| + |D_i||d| )\Big)
\end{align*}

where we used the bound $A_1$ and the estimate \eqref{conjest}. The last term on the RHS is the same (or higher) order as $A_2$, so we incorporate that into the bound on $A_2(\lambda)_i^+(b, c^-, d)$ to get

\begin{align*}
P_i^+(X_i; \lambda)a_i^+ &= P_0^u(\lambda) D_i d + A_2(\lambda)_i^+(b, c^-, d)
\end{align*}

Finally, we operate on both sides on the left by $P_i^+(X_i; \lambda)^{-1}$ to solve for $a_i^+$. Since $P_i^+(X_i; \lambda)^{-1}$ a bounded operator, we will also incorporate this into $A_2(\lambda)_i^+(b, c^-, d)$. In doing so, the bound will be unchanged. Thus we have

\begin{align*}
a_i^+ &= P^+(X_i; \beta_i^+, \lambda) P_0^u(\lambda) D_i d + A_2(\lambda)_i^+(b, c^-, d)
\end{align*}

We do the same thing for $a_i^-$, which gives us

\begin{align*}
a_i^- &= -P_i^-(-X_i; \lambda) P_0^s(\lambda) D_i d + A_2(\lambda)_i^-(b, c^-, d)
\end{align*}

Finally, we would like to obtain an estimate for $P_0^c(\lambda) D_i d$. Recall that

\[
D_i d = ( Q'(X_i) + Q'(-X_i))(d_{i+1} - d_i ) + \mathcal{O} \left( e^{-\alpha X_i} \left( |\lambda| +  e^{-\alpha X_i}  \right) |d| \right) 
\]

Looking at the lower order terms,

\begin{align*}
P_0^c(\lambda)&( Q'(X_i) + Q'(-X_i)) 
= P_0^c(0)( Q'(X_i) + Q'(-X_i)) + \mathcal{O}(|\lambda|e^{-\alpha X_i}) \\
&= \mathcal{O}(e^{-\alpha X_i}(|\lambda| + e^{-\alpha X_i}))
\end{align*}

Thus we have

\[
|P_0^c(\lambda) D_i d| \leq C e^{-\alpha X_i}(|\lambda| + e^{-\alpha X_i})|d|
\]

\end{proof}
\end{lemma}

\subsubsection{Conditions at 0}

In the second inversion lemma, we solve the conditions at $x = 0$

\begin{align*}
P_i^\pm(0; \lambda) Z_i^\pm(0) &\in \C \Psi(0) \oplus Y^0 \oplus Y^+ \oplus Y^- \\
P_i^+(0; \lambda) Z_i^+(0) - P_i^-(0; \lambda) Z_i^-(0) &\in \C \Psi(0) \oplus Y^0
\end{align*}

Recall that we have

\[
\C^m = \C Q'(0) \oplus Y^+ \oplus Y^- \oplus S
\]

This condition is equivalent to the three projections

\begin{align*}
P(\C Q'(0) ) P_i^-(0; \lambda) Z_i^-(0) &= 0 \\
P(\C Q'(0) ) P_i^+(0; \lambda) Z_i^+(0) &= 0 \\
P(Y_i^+ \oplus Y_i^-) ( P_i^+(0; \lambda) Z_i^+(0) - P_i^-(0; \lambda) Z_i^-(0) ) &= 0
\end{align*}

where the kernel of each projection is the remaining spaces in the direct sum. We don't need to include $\C Q'(0)$ in the third equation since we eliminated any component in it in the first two equations.

% second inversion lemma

\begin{lemma}\label{inv2}
There exist operators

\begin{align*}
B_1: &V_\lambda \times V_c^- \times V_d \rightarrow V_b \\
A_3: &V_\lambda \times V_c^- \times V_d \rightarrow V_a 
\end{align*}

such that $( (a, c^+) , b ) = ( A_3(\lambda)(c^-,d), B_1(\lambda)(c^-, d) )$ solves our system. These operators are analytic in $\lambda$ and linear in $(c^-,d)$. Piecewise bounds for $B_1$ and $A_3$ are given by

\begin{align}
|B_1&(\lambda)_i(\tilde{c}, d)| \leq C\Big( 
(|\lambda| + e^{-\tilde{\alpha}X_m})( |\tilde{c}_{i-1}^+| + |\tilde{c}_i^-|) + ( e^{-\tilde{\alpha}X_m} |D| + e^{-\tilde{\alpha}X_m}|\lambda| + |\lambda|^2)|d| \Big) \\
|A_3&(\lambda)_i(c^-, d)|
\leq C \Big(  
e^{-\alpha X_m} (|\lambda| + e^{-\tilde{\alpha}X_m})(|\tilde{c}_{i-1}^+| + |\tilde{c}_{i+1}^-|) +|c_i^-| + e^{-(\alpha - \rho) X_i} |\lambda^2||d| + |D_i||d| \Big)
\end{align} 

where

\begin{equation}\label{tildec}
\tilde{c}_i^\pm = e^{\pm \nu(\lambda) X_i} c_i^-
\end{equation}

In addition, we can write

\begin{align*}
a_i^+ &= P_i^+(X_i; \lambda) P_0^u(\lambda) D_i d + A_4(\lambda)_i^+(b, c^-, d) \\
a_i^- &= -P_i^-(-X_i; \lambda) P_0^s(\lambda) D_i d + A_4(\lambda)_i^-(b, c^-, d) \\
c_i^+ &= c_i^- + P_0^c(\lambda) D_i d + A_4(\lambda)_i^c(b, c^-, d) )
\end{align*}

where $A_4(\lambda)(c^-, d)$ is analytic in $\lambda$, linear in $(c^-, d)$, and has piecewise bounds

\begin{align}
|A_4&(\lambda)_i(c^-, d)|
\leq C \Big( 
e^{-\alpha X_i} (|\lambda| + e^{-\tilde{\alpha}X_m})(|\tilde{c}_{i-1}^+| + |\tilde{c}_{i+1}^-|) + e^{-(\alpha - \rho) } |c_i^-| + e^{-\tilde{\alpha} X_m} |\lambda|^2|d| + e^{-\alpha X_m}|D||d| \Big)
\end{align}

Finally, we have the following expression for $e^{-\nu(\lambda)X_i} c_i^+$.

\begin{align}\label{tildecminus}
e^{-\nu(\lambda)X_i} &c_i^+ = e^{-\nu(\lambda)X_i} c_i^- 
+ \mathcal{O}\Big( e^{-\tilde{\alpha}X_m} (|\lambda| + e^{-\tilde{\alpha}X_m})( |\tilde{c}_{i-1}^+| + |\tilde{c}_{i+1}^-|) 
+ e^{-(\alpha - 2 \rho)X_i}|c_i^-| +  e^{-\tilde{\alpha}X_m}(|\lambda| + |D_i|)|d| \Big)
\end{align}

\begin{proof}

Recall that at $Q(0)$, the tangent spaces to the stable and unstable manifold are given by

\begin{align*}
T_{Q(0)} W^u(0) &= \R Q'(0) \oplus Y^- \\
T_{Q(0)} W^s(0) &= \R Q'(0) \oplus Y^+
\end{align*}

Thus we have

\begin{align*}
P^-(0)^{-1} Q'(0) &= V^- \in E^u(0) \\
P^+(0)^{-1} Q'(0) &= V^+ \in E^s(0)
\end{align*}

Let

\begin{align*}
E^u(0) &= \C V^- \oplus E^- \\
E^s(0) &= \C V^+ \oplus E^+ \\
\end{align*}

Then we have

\begin{align*}
P^-(0)^{-1} Y^- = E^- \\
P^+(0)^{-1} Y^+ = E^+ \\
\end{align*}

We decompose $b_i^\pm$ uniquely as $b_i^\pm = x_i^\pm + y_i^\pm$, where $x_i^\pm \in \C V^\pm$ and $y_i^\pm \in E^\pm$.\\

At $x = 0$, the fixed point equations become

\begin{align*}
Z_i^-(0) &= \Phi^s(0, -X_{i-1}; \lambda) a_{i-1}^- + \Phi^u(0, 0; \lambda) b_i^- + \Phi^c(0, -X_{i-1}; \lambda) c_{i-1}^- \\
&+ \lambda^2 d_i \int_{-X_{i-1}}^0 \Phi^s(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy \\
&+ \lambda^2 d_i \int_{-X_{i-1}}^0 \Phi^c(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy  \\ 
Z_i^+(0) &= \Phi^u(0, X_i; \lambda) a_i^+ + \Phi^s(0, 0; \lambda) b_i^+ + \Phi^c(0, X_i; \lambda) c_i^+ \\
&+ \lambda^2 d_i \int_{X_i}^0 \Phi^u(0, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
&+ \lambda^2 d_i \int_{X_i}^0 \Phi^c(0, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
\end{align*}

Noting that $\Phi^u(0, 0; \lambda) = P_0^u(\lambda)$, doing a little manipulation on the $b_i$ terms, and using the known form of the evolution $\Phi^c$ on $E^c(\lambda)$, this becomes

\begin{align*}
Z_i^-(0) &= \Phi^s(0, -X_{i-1}; \lambda) a_{i-1}^- + x_i^- + y_i^- + (P_0^u(\lambda) - P_0^u(0))b_i^- + e^{\nu(\lambda) X_{i-1}} c_{i-1}^- \\
&+ \lambda^2 d_i \int_{-X_{i-1}}^0 \Phi^s(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy \\
&+ \lambda^2 d_i \int_{-X_{i-1}}^0 \Phi^c(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy  \\ 
Z_i^+(0) &= \Phi^u(0, X_i; \lambda) a_i^+ + x_i^+ + y_i^+ + (P_0^s(\lambda) - P_0^s(0)) b_i^+ + e^{-\nu(\lambda)X_i} c_i^+ \\
&+ \lambda^2 d_i \int_{X_i}^0 \Phi^u(0, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
&+ \lambda^2 d_i \int_{X_i}^0 \Phi^c(0, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
\end{align*}

Since $c_i^\pm$ are in the eigenspaces $E^c(\lambda)$, we do some further manipulation to separate out a component in $E^c(0)$.

\begin{align*}
Z_i^-(0) &= \Phi^s(0, -X_{i-1}; \lambda) a_{i-1}^- + x_i^- + y_i^- + (P_0^u(\lambda) - P_0^u(0))b_i^- \\
&+ P_0^c(0) e^{\nu(\lambda) X_{i-1}} c_{i-1}^- + (P_0^c(\lambda) - P_0^c(0)) e^{\nu(\lambda) X_{i-1}} c_{i-1}^- \\
&+ \lambda^2 d_i \int_{-X_{i-1}}^0 \Phi^s(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy \\
&+ \lambda^2 d_i \int_{-X_{i-1}}^0 \Phi^c(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy  \\ 
Z_i^+(0) &= \Phi^u(0, X_i; \lambda) a_i^+ + x_i^+ + y_i^+ + (P_0^s(\lambda) - P_0^s(0)) b_i^+ \\
&+ P_0^c(0) e^{-\nu(\lambda)X_i} c_i^+ + (P_0^c(\lambda) - P_0^c(0)) e^{-\nu(\lambda)X_i} \\
&+ \lambda^2 d_i \int_{X_i}^0 \Phi^u(0, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
&+ \lambda^2 d_i \int_{X_i}^0 \Phi^c(0, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
\end{align*}

Finally, we operate on these by $P_i^\pm(0; \lambda)$. For the $c_i^-$ and $b$ terms, we write these as

\[
P_i^\pm(0; \lambda) = P^\pm(0) + (P_i^\pm(0; \lambda) - P^\pm(0))
\]

We finally wind up with the equations

\begin{align*}
P_i^-(0; \lambda) Z_i^-(0) &= P^-(0)( x_i^- + y_i^- + P_0^c(0) e^{\nu(\lambda) X_{i-1}} c_{i-1}^- ) \\
&+ P_i^-(0; \lambda) \Phi^s(0, -X_{i-1}; \lambda) a_{i-1}^- + (P_i^-(0; \lambda) - P^-(0))b_i^- + P_i^-(0; \lambda)(P_0^u(\lambda) - P_0^u(0))b_i^- \\
&+ (P_i^-(0; \lambda) - P^-(0)) P_0^c(0) e^{\nu(\lambda) X_{i-1}} c_{i-1}^- + P_i^-(0; \lambda) (P_0^c(\lambda) - P_0^c(0)) e^{\nu(\lambda) X_{i-1}} c_{i-1}^- \\
&+ \lambda^2 d_i P_i^-(0; \lambda) \int_{-X_{i-1}}^0 \Phi^s(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy \\
&+ \lambda^2 d_i P_i^-(0; \lambda) \int_{-X_{i-1}}^0 \Phi^c(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy  \\ 
P_i^+(0; \lambda) Z_i^+(0) &=  P^+(0)( x_i^+ + y_i^+ + P_0^c(0) e^{-\nu(\lambda)X_i} c_i^+ )\\
&+ P_i^+(0; \lambda) \Phi^u(0, X_i; \lambda) a_i^+ + (P_i^+(0; \lambda) - P^+(0)) b_i^+ + P_i^+(0; \lambda) (P_0^s(\lambda) - P_0^s(0)) b_i^+ \\
&+ (P_i^+(0; \lambda) - P^+(0))P_0^c(0) e^{-\nu(\lambda)X_i} c_i^+ + P_i^+(0; \lambda) (P_0^c(\lambda) - P_0^c(0)) e^{-\nu(\lambda)X_i} c_i^+\\
&+ \lambda^2 d_i P_i^+(0; \lambda) \int_{X_i}^0 \Phi^u(0, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
&+ \lambda^2 d_i P_i^+(0; \lambda) \int_{X_i}^0 \Phi^c(0, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
\end{align*}

With this setup, the projections on $Q'(0)$ and $Y^+ \oplus Y^-$ will either eliminate or act as the identity on the terms in the first lines of $P_i^-(0; \lambda) Z_i^-(0)$ and $P_i^+(0; \lambda) Z_i^+(0)$. Thus, applying the appropriate projections, we obtain an expression of the form

\begin{equation}\label{projxy}
\begin{pmatrix}x_i^- \\ x_i^+ \\ 
y_i^+ - y_i^- \end{pmatrix} + L_4(\lambda)_i(b, c^-, d) = 0
\end{equation}

To get a bound on $L_4$, we need to bound the individual terms from the fixed point equations above. Where appropriate, we only look at one of each term, i.e. only look at the ``positive'' piece or the ``negative'' piece. For convenience, we define

\[
\tilde{c}_i^\pm = e^{\pm \nu(\lambda) X_i} c_i^-
\]

\begin{enumerate}

\item For the $a_i$ terms, we substitute $a_i^+ = P_0^u(\lambda) D_i d + A_2(\lambda)_i^+(b, c^-, d)$ and $a_{i-1}^- = -P_0^s(\lambda) D_{i-1} d + A_2(\lambda)_{i-1}^-(b, c^-, d)$ and use the bounds for $A_2$.

\begin{align*}
|P_i^-(0; \lambda) &\Phi^s(0, -X_{i-1}; \lambda) a_{i-1}^-| \\
&\leq C \Big( e^{-2 \alpha X_{i-1}} (|b_{i-1}^+| + |b_i^-| + |c_{i-1}^-|) + e^{-(2 \alpha - \rho) X_{i-1}} |\lambda^2| + e^{-\alpha X_{i-1}}|D_{i-1}|)|d| \Big) \\
|P_i^+(0; \lambda) &\Phi^u(0, X_i; \lambda) a_i^+| \\
&\leq C \Big( e^{-2 \alpha X_i} (|b_i^+| + |b_{i+1}^-| + |c_i^-|) + e^{-(2 \alpha - \rho) X_i} |\lambda|^2|d| + e^{-\alpha X_i} |D_i||d| \Big)
\end{align*}

\item For the $b_i$ terms, we have

\begin{align*}
|(P_i^-(0; \lambda) &- P^-(0))b_i^- + P_i^-(0; \lambda)(P_0^u(\lambda) - P_0^u(0))b_i^-| \\
&\leq C ( e^{-\alpha X_m} + |\lambda|)|b_i^-|
\end{align*}

\item For the $c_i^-$ terms, we have

\begin{align*}
|(P_i^-(0; \lambda) &- P^-(0)) P_0^c(0) e^{\nu(\lambda) X_{i-1}} c_{i-1}^- + P_i^-(0; \lambda) (P_0^c(\lambda) - P_0^c(0)) e^{\nu(\lambda) X_{i-1}} c_{i-1}^- | \\
&\leq C (e^{-\alpha X_m} + |\lambda|)|\tilde{c}_{i-1}^+|)
\end{align*}

\item For the $c_i^+$ terms, we have

\begin{align*}
|(P_i^+(0; \lambda) &- P^+(0))P_0^c(0) e^{-\nu(\lambda)X_i} c_i^+ + P_i^+(0; \lambda) (P_0^c(\lambda) - P_0^c(0)) e^{-\nu(\lambda)X_i} c_i^+| \\
&\leq C (e^{-\alpha X_m} + |\lambda|)|e^{-\nu(\lambda)X_i} c_i^+|
\end{align*}

To put this in terms of $c_i^-$, we use the expression

\[
c_i^+ = c_i^- + P_0^c(\lambda) D_i d + A_2(\lambda)_i^c(b, c^-, d) )
\]

from Lemma \ref{inv1}, together with the bound for $A_2$.

\begin{align*}
e^{-\nu(\lambda)X_i} c_i^+ &= e^{-\nu(\lambda)X_i} c_i^- 
+ e^{-\nu(\lambda)X_i} P_0^c(\lambda) D_i d + e^{-\nu(\lambda)X_i} A_2(\lambda)_i^c(b, d)\\
&= e^{-\nu(\lambda)X_i} c_i^- + \mathcal{O}\Big( e^{-(\alpha - \rho) X_i} ( |\lambda| + e^{-\alpha X_i} ) |d|) + e^{-(\alpha - \rho) X_i} (|b_i^+| + |b_{i+1}^-| + |c_i^-|)\\
&+ e^{-(\alpha - 2 \rho) X_i} |\lambda|^2|d| + e^{-(\alpha - \rho) X_i} |D_i||d| )
\end{align*}

Thus we have the expression for $e^{-\nu(\lambda)X_i} c_i^+$

\begin{align}\label{tildecminus2}
e^{-\nu(\lambda)X_i} c_i^+
&= e^{-\nu(\lambda)X_i} c_i^- + \mathcal{O}\Big( e^{-(\alpha - \rho) X_i} ( |b_i^+| + |b_{i+1}^-| + |c_i^-| + |\lambda||d| + |D_i||d|) \Big)
\end{align}

which gives us the overall estimate

\begin{align*}
&|(P_i^+(0; \lambda) - P^+(0))P_0^c(0) e^{-\nu(\lambda)X_i} c_i^+ + P_i^+(0; \lambda) (P_0^c(\lambda) - P_0^c(0)) e^{-\nu(\lambda)X_i} c_i^+| \\
&\leq C \Big( (e^{-\alpha X_m} + |\lambda|)( e^{-\nu(\lambda)X_i} c_i^- + e^{-(\alpha - \rho) X_i} ( |b_i^+| + |b_{i+1}^-| + |c_i^-| + |\lambda||d| + |D_i||d|) \Big) \\
&\leq C \Big( (e^{-\alpha X_m} + |\lambda|)( |\tilde{c}_i^-| + e^{-(\alpha - \rho) X_i} ( |b_i^+| + |b_{i+1}^-| + |c_i^-| + |\lambda||d| + |D_i||d|) \Big)
\end{align*}

\item The bound on the integral terms is determined by the integrals involving the center subspace, since there is potential growth in that subspace.

\begin{align*}
\left| \lambda^2 d_i P_i^-(0; \lambda) \int_{-X_{i-1}}^0 \Phi^c(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy \right| &\leq C |\lambda|^2 |d| \int_{-X_{i-1}}^0 e^{-\rho y} e^{\alpha y} dy \\
&\leq C |\lambda|^2 |d|
\end{align*}

\end{enumerate}

Putting all these together, we obtain the bound for $L_4(\lambda)_i(b, c, d)$.

\begin{align}\label{L4bound}
L_4(\lambda)_i(b, c, d) &\leq 
C\Big( (|\lambda| + e^{-\tilde{\alpha}X_m})|b| 
+ (|\lambda| + e^{-\tilde{\alpha}X_m}) |\tilde{c}_{i-1}^+| + |\tilde{c}_i^-|) + (|\lambda| + e^{-\tilde{\alpha}X_m})( e^{-\alpha X_{i-1}} |c_{i-1}^-| + e^{-\alpha X_i} |c_i^-| ) \nonumber \\
&+ ( e^{-\tilde{\alpha}X_m} |D| + e^{-\tilde{\alpha}X_m}|\lambda| + |\lambda|^2)|d| \Big) 
\end{align}

Since $|\lambda|, e^{-\alpha X_m} < \delta$, this becomes

\begin{align}\label{L4bound}
L_4(\lambda)_i(b, c, d) &\leq C \delta |b| + 
C\Big( (|\lambda| + e^{-\tilde{\alpha}X_m}) |\tilde{c}_{i-1}^+| + |\tilde{c}_i^-|) + (|\lambda| + e^{-\tilde{\alpha}X_m})( e^{-\alpha X_{i-1}} |c_{i-1}^-| + e^{-\alpha X_i} |c_i^-| ) \nonumber \\
&+ ( e^{-\tilde{\alpha}X_m} |D| + e^{-\tilde{\alpha}X_m}|\lambda| + |\lambda|^2)|d| \Big) 
\end{align}

which uniform in $|b|$. Define the map

\[
J_2: \left( \bigoplus_{j=1}^n \C V^+ \oplus \C V^- \right) \oplus
\left( \bigoplus_{j=1}^n E^+ \oplus E^- \right) 
\rightarrow \bigoplus_{j=1}^n \C V^+ \oplus \C V^- \oplus (E^+ \oplus E^-)
\]

by 

\[
J_2( (x_i^+, x_i^-),(y_i^+, y_i^-))_i = ( x_i^+, x_i^-, y_i^+ - y_i^- )
\]

Since $\C^{2m} = E^s(0) \oplus E^u(0) = \C V^+ \oplus \C V^- \oplus (E^+ \oplus E^-)$, $J_2$ is an isomorphism. Using this as the fact that $b_i = (x_i^- + y_i^-, x_i^+ + y_i^+)$, we can write \eqref{projxy} as

\begin{equation}\label{projxy2}
J_2( (x_i^+, x_i^-),(y_i^+, y_i^-))_i 
+ L_4(\lambda)_i(b_i, 0, 0) + L_4(\lambda)_i(0, c^-, d) = 0
\end{equation}

Consider the map

\begin{align*}
S_2(b)_i &= J_2( (x_i^+, x_i^-),(y_i^+, y_i^-))_i 
+ L_4(\lambda)_i(b_i, 0, 0) 
\end{align*}

Substituting this in \eqref{projxy2}, we have

\begin{align*}
S_2(b) &= -L_4(\lambda)(0, c^-, d)
\end{align*}

For sufficiently small $\delta$, the operator $S_2(b)$ is invertible. Thus we can solve for $b$ to get

\begin{align}
b = B_1(\lambda)(c^-,d) 
= -S_2^{-1} L_4(\lambda)(0, c^-, d)
\end{align}

The bound on $B_1$ is given by the bound on $L_4$, where we note which piece is involved.

\begin{align*}
|B_1(\lambda)_i(\tilde{c}, d)| \leq C\Big( 
(|\lambda| + e^{-\tilde{\alpha}X_m})( |\tilde{c}_{i-1}^+| + |\tilde{c}_i^-|
+ e^{-\alpha X_{i-1}} |c_{i-1}^-| + e^{-\alpha X_i} |c_i^-|) + ( e^{-\tilde{\alpha}X_m} |D| + e^{-\tilde{\alpha}X_m}|\lambda| + |\lambda|^2)|d| \Big)
\end{align*}

Since $e^{-\alpha X_{i-1}} |c_{i-1}^-| \leq e^{-\tilde{\alpha} X_{i-1}} |\tilde{c}_{i-1}^+|$ and $e^{-\alpha X_i} |c_i^-| \leq e^{-\tilde{\alpha} X_i} |\tilde{c}_i^-|$, this simplifies to

\begin{align*}
|B_1(\lambda)_i(\tilde{c}, d)| \leq C\Big( 
(|\lambda| + e^{-\tilde{\alpha}X_m})( |\tilde{c}_{i-1}^+| + |\tilde{c}_i^-|) + ( e^{-\tilde{\alpha}X_m} |D| + e^{-\tilde{\alpha}X_m}|\lambda| + |\lambda|^2)|d| \Big)
\end{align*}

We can plug this into the bound for $A_1$ to get $A_3$ with bound

\begin{align*}
|A_3&(\lambda)_i(c^-, d)|
\leq C \Big(  
e^{-\alpha X_m} (|\lambda| + e^{-\tilde{\alpha}X_m})(|\tilde{c}_{i-1}^+| + |\tilde{c}_{i+1}^-|)  \\
&+|c_i^-| + e^{-(\alpha - \rho) X_i} |\lambda^2||d| + |D_i||d| \Big)
\end{align*} 

We can also plug this into the bound for $A_2$ to get $A_4$ with bound

\begin{align*}
|A_4&(\lambda)_i(c^-, d)|
\leq C \Big( 
e^{-\alpha X_i} (|\lambda| + e^{-\tilde{\alpha}X_m})(|\tilde{c}_{i-1}^+| + |\tilde{c}_{i+1}^-|) + e^{-(\alpha - \rho) } |c_i^-| + e^{-\tilde{\alpha} X_m} |\lambda|^2|d| + e^{-\alpha X_m}|D||d| \Big)
\end{align*} 

Finally, we plug $B_1$ into \eqref{tildecminus2} to obtain an expression for $e^{-\nu(\lambda)X_i} c_i^+$.

\begin{align*}
e^{-\nu(\lambda)X_i} &c_i^+ = e^{-\nu(\lambda)X_i} c_i^- 
+ \mathcal{O}\Big( e^{-(\alpha - \rho)X_i} (|\lambda| + e^{-\tilde{\alpha}X_m})( |\tilde{c}_{i-1}^+| + |\tilde{c}_{i+1}^-|) 
+ e^{-(\alpha - 2 \rho) X_i}|c_i^-| +  e^{-\tilde{\alpha}X_m}(|\lambda| + |D_i|)|d| \Big)
\end{align*}

\end{proof}
\end{lemma}

Up to this point, we have solved uniquely for everything except for the $c_i^-$ and $d$. To do that, we will compute the jump conditions in the direction of $\Psi(0)$ and $\Psi^c(0)$.

\subsection{Jump Conditions}

\subsubsection{Center Adjoint Jump}

First, we compute the jump in the direction of $\Psi^c(0)$. Before we do that, we prove the following lemma regarding inner products with $\Psi^c(0)$ and $\Psi(0)$.

% lemma : inner products with Psi and Psi^c

\begin{lemma}\label{PsiIP}
We have the following expressions involving the inner product with $\Psi^c(0)$.
\begin{enumerate}[(i)]
	\item $\langle \Phi^c(0), P^\pm(0) V \rangle = V$ for all $V \in E^c(0)$.
	\item $\langle \Phi(0), P^\pm(0) V \rangle = 0$ for all $V \in E^c(0)$.
	\item $\langle \Phi^c(0), P^-(0) V \rangle = 0$ and $\langle \Phi(0), P^-(0) V \rangle = 0$ for all $V \in E^u(0)$.
	\item $\langle \Phi^c(0), P^+(0) V \rangle = 0$ and $\langle \Phi(0), P^+(0) V \rangle = 0$ for all $V \in E^s(0)$.
\end{enumerate}
\begin{proof}
For (i), recall that $E^c(0) = \text{span }\{ V_0 \}$, where $V_0$ is the eigenvector of $A(0)$ corresponding to the eigenvalue 0. Furthermore, the constant function $Z(x) = V_0$ solves $Z' = A(0) Z$ with initial condition $V_0$. Let $W^-(x) = P^-(x) Z(x) = P^-(x) V_0$. By the Conjugation Lemma, we can write
\[
W^-(x) = V_0 + \mathcal{O}({e^{-\tilde{\alpha}|x|}})
\]
Since the inner product $\langle \Phi^c(x), W^-(x) \rangle$ is constant in $x$, sending $x \rightarrow -\infty$, we conclude by the continuity of the inner product that
\[
\langle \Phi^c(0), W^-(0) \rangle = \langle W_0, V_0 \rangle = 1 
\]
Thus we conclude that $\langle \Phi^c(0), P^-(0) V \rangle = V$ for all $V \in E^c(0)$. Similarly, the same holds for $\langle \Phi^c(0), P^+(0) V \rangle$.\\

For (ii), we use the same argument as in (i), except we look at the inner product $\langle \Phi(x), W^-(x) \rangle$. Since this is constant in $x$, we send $x \rightarrow \infty$. This time, $W^-(x)$ remains bounded, but $\Phi(x)$ decays to 0, thus by the continuity of the inner product, we conclude that $\langle \Phi(0), W^-(0) \rangle = 0$, from which (ii) follows.\\

For (iii) and (iv), we note that $P^-(0)E^u = \C Q'(0) \oplus Y^-$ and $P^+(0)E^u = \C Q'(0) \oplus Y^+$. The result follows since $\Psi^c(0), \Psi(0) \perp \C Q'(0) \oplus Y^+ \oplus Y^-$.
\end{proof}
\end{lemma}

% jump lemma : center adjoint

\begin{lemma}\label{jumpcenteradj}

The jumps in the direction of $\Psi^c(0)$ are given by

\begin{align}\label{xic}
\xi^c_i = e^{-\nu(\lambda) X_i} c_i^- - e^{\nu(\lambda) X_{i-1}} c_{i-1}^- - \lambda^2 d_i M^c + R^c(\lambda)_i(c^-, d)
\end{align}

where $M^c$ is the center Melnikov integral

\begin{equation}\label{Mc}
M^c =  \int_{-\infty}^\infty \langle \Psi^c(y), H(y) \rangle dy 
\end{equation}

and the remainder term $R^c_i(c^-, d)$ has bound

\begin{align}\label{Rc}
R^c&(c^-, d)_i \leq C \Big(
(|\lambda| + e^{-\alpha X_m})(|\tilde{c}_{i-1}^+| + |\tilde{c}_{i}^-|) + (|\lambda| + e^{-\alpha X_m})( e^{-(\alpha - \rho) X_{i-1}} |\tilde{c}_{i-2}^+| + e^{-(\alpha - \rho) X_i} |\tilde{c}_{i+1}^-|)  \\
&+ (|\lambda| + e^{-\tilde{\alpha} X_m})(|\lambda| + e^{-\alpha X_m})|d|
\Big) \nonumber
\end{align}

The jump conditions can be written as the matrix equation

\begin{equation}\label{matrixjumpc}
(K(\lambda) + C_1 K(\lambda) + K_1(\lambda)) c + D_1 d = 0
\end{equation}

where

\begin{align*}
K(\lambda) =  
\begin{pmatrix}
e^{-\nu(\lambda)X_1} & & & & & -e^{\nu(\lambda)X_0} \\
-e^{\nu(\lambda)X_1} & e^{-\nu(\lambda)X_2} \\
& -e^{\nu(\lambda)X_2} & e^{-\nu(\lambda)X_3} \\
\vdots & & \vdots & &&  \vdots \\
& & & & -e^{\nu(\lambda)X_{n-1}} & e^{-\nu(\lambda)X_0} 
\end{pmatrix}
\end{align*}

and we have uniform bounds

\begin{align*}
C_1 &= \mathcal{O}(e^{-\tilde{\alpha} X_m}(|\lambda| + e^{-\alpha X_m})) \\
D_1 &= \mathcal{O}((|\lambda| + e^{-\tilde{\alpha} X_m})(|\lambda| + e^{-\alpha X_m}))
\end{align*}

$K_1(\lambda)$ is a small perturbation of $K(\lambda)$. The specific forms of $K_1(\lambda)$ and $C_1$ are given in the proof.

\begin{proof}

Recall from the previous section that $P_i^\pm(0; \lambda) Z_i^\pm(0)$ are given by

\begin{align*}
P_i^-(0; \lambda) Z_i^-(0) &= P^-(0)( b_i^- + P_0^c(0) e^{\nu(\lambda) X_{i-1}} c_{i-1}^- ) \\
&+ P_i^-(0; \lambda) \Phi^s(0, -X_{i-1}; \lambda) a_{i-1}^- + (P_i^-(0; \lambda) - P^-(0))b_i^- + P_i^-(0; \lambda)(P_0^u(\lambda) - P_0^u(0))b_i^- \\
&+ (P_i^-(0; \lambda) - P^-(0)) P_0^c(0) e^{\nu(\lambda) X_{i-1}} c_{i-1}^- + P_i^-(0; \lambda) (P_0^c(\lambda) - P_0^c(0)) e^{\nu(\lambda) X_{i-1}} c_{i-1}^- \\
&+ \lambda^2 d_i P_i^-(0; \lambda) \int_{-X_{i-1}}^0 \Phi^s(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy \\
&+ \lambda^2 d_i P_i^-(0; \lambda) \int_{-X_{i-1}}^0 \Phi^c(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy  \\ 
P_i^+(0; \lambda) Z_i^+(0) &=  P^+(0)( b_i^+ + P_0^c(0) e^{-\nu(\lambda)X_i} c_i^+ )\\
&+ P_i^+(0; \lambda) \Phi^u(0, X_i; \lambda) a_i^+ + (P_i^+(0; \lambda) - P^+(0)) b_i^+ + P_i^+(0; \lambda) (P_0^s(\lambda) - P_0^s(0)) b_i^+ \\
&+ (P_i^+(0; \lambda) - P^+(0))P_0^c(0) e^{-\nu(\lambda)X_i} c_i^+ + P_i^+(0; \lambda) (P_0^c(\lambda) - P_0^c(0)) e^{-\nu(\lambda)X_i} c_i^+\\
&+ \lambda^2 d_i P_i^+(0; \lambda) \int_{X_i}^0 \Phi^u(0, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
&+ \lambda^2 d_i P_i^+(0; \lambda) \int_{X_i}^0 \Phi^c(0, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
\end{align*}

We will compute the leading order terms first.

\begin{enumerate}
\item For the leading order terms involving $c$, using Lemma \ref{PsiIP}, we have

\begin{align*}
\langle \Psi(0), &P^-(0) P_0^c(0) e^{\nu(\lambda) X_{i-1}} c_{i-1}^- - P^+(0) P_0^c(0) e^{-\nu(\lambda)X_i} c_i^+) = e^{\nu(\lambda) X_{i-1}} c_{i-1}^- - e^{-\nu(\lambda)X_i} c_i^+ 
\end{align*}

For the higher order terms involving $c$, from the previous section we have
\begin{align*}
|(P_i^-(0; \lambda) &- P^-(0)) P_0^c(0) e^{\nu(\lambda) X_{i-1}} c_{i-1}^- + P_i^-(0; \lambda) (P_0^c(\lambda) - P_0^c(0)) e^{\nu(\lambda) X_{i-1}} c_{i-1}^-| \\
&\leq C (|\lambda| + e^{-\alpha X_m}) |e^{\nu(\lambda) X_{i-1}} c_{i-1}^-|\\
&= C (|\lambda| + e^{-\alpha X_m}) |\tilde{c}_{i-1}^+|
\end{align*}

and

\begin{align*}
|(P_i^+(0; \lambda) &- P^+(0))P_0^c(0) e^{-\nu(\lambda)X_i} c_i^+ + P_i^+(0; \lambda) (P_0^c(\lambda) - P_0^c(0)) e^{-\nu(\lambda)X_i} c_i^+| \\
&\leq C (|\lambda| + e^{-\alpha X_m}) |e^{-\nu(\lambda)X_i} c_i^+| \\
&= C (|\lambda| + e^{-\alpha X_m}) |\tilde{c}_i^-|
\end{align*}

All that remains is to write $e^{-\nu(\lambda)X_i} c_i^+$ in terms of $e^{-\nu(\lambda)X_i} c_i^-$ using Lemma \ref{inv2}. For the lower order term involving $c_i^+$, we have

\begin{align*}
e^{-\nu(\lambda)X_i} c_i^+ &= e^{-\nu(\lambda)X_i} c_i^- 
+ \mathcal{O}\Big( e^{-(\alpha - \rho) X_i} (|\lambda| + e^{-\tilde{\alpha}X_m})( |\tilde{c}_{i-1}^+| + |\tilde{c}_{i+1}^-|) 
+ e^{-(\alpha - 2 \rho) X_i}|c_i^-| +  e^{-\tilde{\alpha}X_m}(|\lambda| + |D_i|)|d| \Big) \\
&= e^{-\nu(\lambda)X_i} c_i^- 
+ \mathcal{O}\Big( e^{-(\alpha - \rho) X_i}  (|\lambda| + e^{-\tilde{\alpha}X_m})( |\tilde{c}_{i-1}^+| + |\tilde{c}_{i+1}^-|) 
+ e^{-\tilde{\alpha}X_i}|\tilde{c}_i^-| + e^{-\tilde{\alpha}X_m}(|\lambda| + |D_i|)|d| \Big)
\end{align*}

For the higher order term involving $c_i^+$, we have

\begin{align*}
|(P_i^+(0; \lambda) &- P^+(0))P_0^c(0) e^{-\nu(\lambda)X_i} c_i^+ + P_i^+(0; \lambda) (P_0^c(\lambda) - P_0^c(0)) e^{-\nu(\lambda)X_i} c_i^+| \\
&\leq C (|\lambda| + e^{-\alpha X_m}) |e^{-\nu(\lambda)X_i} c_i^+| \\
&\leq C (|\lambda| + e^{-\alpha X_m}) \Big( |e^{-\nu(\lambda)X_i} c_i^-| + e^{-(\alpha - \rho) X_i}  (|\lambda| + e^{-\tilde{\alpha}X_m})( |\tilde{c}_{i-1}^+| + |\tilde{c}_{i+1}^-|) \\
&+ e^{-(\alpha - 2 \rho) X_i}|c_i^-| +  e^{-\tilde{\alpha}X_m}(|\lambda| + |D_i|)|d| \Big) \\
&\leq C (|\lambda| + e^{-\alpha X_m}) \Big( |\tilde{c}_i^-| + e^{-(\alpha - \rho) X_i}  (|\lambda| + e^{-\tilde{\alpha}X_m})( |\tilde{c}_{i-1}^+| + |\tilde{c}_{i+1}^-|) +  e^{-\tilde{\alpha}X_m}(|\lambda| + |D_i|)|d| \Big)
\end{align*}

Combining all of these, the terms involving $c_i$ are given by

\begin{align*}
e^{\nu(\lambda) X_{i-1} } &c_{i-1}^- - e^{-\nu(\lambda)X_i} c_i^- + \mathcal{O}\Big( (|\lambda| + e^{-\alpha X_m})(|\tilde{c}_{i-1}^+| + |\tilde{c}_i^-|) + e^{-(\alpha - \rho) X_i}  (|\lambda| + e^{-\tilde{\alpha}X_m})|\tilde{c}_{i+1}^-| \\
&+ e^{-\tilde{\alpha}X_m}(|\lambda| + |D_i|)|d| \Big)  
\end{align*}

\item The integral term involving the center subspace will give us the center Melnikov integral

\begin{align*}
&\langle \Psi^c(0), P_i^-(0; \lambda) \int_{-X_{i-1}}^0 \Phi^c(0, y; \lambda) P_i^-(y; \lambda) \tilde{H}_i^-(y) dy \rangle \\
&= \langle \Psi^c(0), \int_{-X_{i-1}}^0 P_i^-(0; \lambda) \Phi^c(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy \rangle \\
&= \langle \Psi^c(0), \int_{-X_{i-1}}^0 P^-(0) \Phi^c(0, y; 0) P^-(y)^{-1} \tilde{H}_i^-(y) dy \rangle + \mathcal{O}(|\lambda| + e^{-\alpha X_m}) \\
&= \int_{-X_{i-1}}^0 \langle \Psi^c(0), \Theta^c(0, y) \tilde{H}_i^-(y) \rangle dy + \mathcal{O}(|\lambda| + e^{-\alpha X_m}) \\
&= \int_{-X_{i-1}}^0 \langle \Theta^c(y, 0)^* \Psi^c(0), H(y) \rangle dy + \int_{-X_{i-1}}^0 \langle \Psi^c(0), \Theta^c(0, y) \Delta H_i^-(y) \rangle dy + \mathcal{O}(|\lambda| + e^{-\alpha X_m}) \\
&= \int_{-\infty}^0 \langle \Psi^c(y), H(y) \rangle dy + \int_{-X_{i-1}}^0 \langle \Psi^c(0), \Theta^c(0, y) \Delta H_i^-(y) \rangle dy + \mathcal{O}(e^{-\alpha X_m} + |\lambda|) \\
\end{align*}

For the integral involving $\Delta H_i^-(y)$,

\begin{align*}
\left| \int_{-X_{i-1}}^0 \langle \Psi^c(0), \Theta^c(0, y) \Delta H_i^-(y) \rangle dy \right| &\leq C \int_{-X_{i-1}}^0 e^{-\rho y} e^{-\alpha X_{i-1}} e^{-\alpha(X_{i-1} + y)} dy \\
&\leq C e^{-(\alpha - \rho)X_{i-1}} \int_{-X_{i-1}}^0 e^{-\alpha(X_{i-1} + y)} dy \\
&\leq C e^{-\tilde{\alpha}X_{i-1}}
\end{align*}

Thus we have

\begin{align*}
&\langle \Psi^c(0), P^-(0; \beta_i^\pm, \lambda) \int_{-X_{i-1}}^0 \Phi^c(0, y; \lambda) P^-(y; \beta_i^\pm, \lambda) \tilde{H}_i^-(y) dy \rangle \\
&= \int_{-\infty}^0 \langle \Psi^c(y), H(y) \rangle dy + \mathcal{O}(e^{-\tilde{\alpha} X_m} + |\lambda|) \\
\end{align*}

The ``positive'' integral is similar, and gives us the other half of the center Melnikov integral.

\end{enumerate}

The remaining terms are higher order. We will evaluate them in turn.

\begin{enumerate}

\item For the term involving $a$, we plug in $A_4$.

\begin{align*}
P_i^-(0; \lambda) \Phi^s(0, -X_{i-1}; \lambda) a_{i-1}^- &= 
P_i^-(0; \lambda) \Phi^s(0, -X_{i-1}; \lambda) P_0^s(\lambda) D_i d +
P_i^-(0; \lambda) \Phi^s(0, -X_{i-1}; \lambda) A_4(\lambda)_{i-1}^-(c^-, d) \\
&= P_i^-(0; \lambda) \Phi^s(0, -X_{i-1}; \lambda) A_4(\lambda)_{i-1}^-(c^-, d) + \mathcal{O}( e^{-\alpha X_m} |D|)
\end{align*}

Combining this with the bound for $A_4$, we have

\begin{align*}
|P_i^-(0; \lambda) &\Phi^s(0, -X_{i-1}; \lambda) a_{i-1}^-| \\
&\leq C\Big( 
e^{-2 \alpha X_{i-1}} (|\lambda| + e^{-\tilde{\alpha}X_m})(|\tilde{c}_{i-2}^+| + |\tilde{c}_i^-|) + e^{-\alpha X_m} e^{-(\alpha - \rho)X_{i-1}}|c_{i-1}^-|\\
&+ (e^{-(\alpha + \tilde{\alpha}) X_m} |\lambda|^2 + e^{-\alpha X_m}|D| + e^{-2 \alpha X_m}|\lambda|) |d| \Big)
\end{align*}

Similarly, we have

\begin{align*}
|P_i^+(0; \lambda) &\Phi^u(0, X_i; \lambda) a_i^+| \\
&\leq C\Big( 
e^{-2 \alpha X_i} (|\lambda| + e^{-\tilde{\alpha}X_m})(|\tilde{c}_{i-1}^+| + |\tilde{c}_{i+1}^-|) + e^{-\alpha X_m} e^{-(\alpha - \rho)X_i}|c_i^-| \\
&+ (e^{-(\alpha + \tilde{\alpha}) X_m} |\lambda|^2 + e^{-2 \alpha X_m}|D| + e^{-\alpha X_m}|\lambda|) |d| \Big)
\end{align*}

\item For the terms involving $b$, note that by Lemma \ref{PsiIP}, the terms $P^-(0) b_i^-$ and $P^+(0)b_i^+$ are eliminated outright when we take the inner product with $\Psi^c(0)$. For the other terms, we use the estimate for $B_1$ from Lemma \ref{inv2}.

\begin{align*}
&|(P_i^-(0; \lambda) - P^-(0))b_i^- + P_i^-(0; \lambda)(P_0^u(\lambda) - P_0^u(0))b_i^-| \\
&\leq C(|\lambda| + e^{-\alpha X_m}) |B_1(c, \tilde{c}, d)| \\
&\leq C(|\lambda| + e^{-\alpha X_m}) \Big( 
(|\lambda| + e^{-\tilde{\alpha}X_m})( |\tilde{c}_{i-1}^+| + |\tilde{c}_i^-|)
+  ( e^{-\tilde{\alpha}X_m} |D| + e^{-\tilde{\alpha}X_m}|\lambda| + |\lambda|^2)|d| \Big)
\end{align*}

\item For the non-center integral terms, we have the bound

\begin{align*}
&\left| P_i^-(0; \lambda) 
\int_{-X_{i-1}}^0 \Phi^s(0, y; \lambda) \lambda^2 d_i P^-(y; \beta_i^-, \lambda)^{-1} \tilde{H}_i^-(y) dy \right| \\
&\leq C |\lambda|^2 |d_i| \int_{-X_{i-1}}^0 e^{\alpha y} e^{\alpha y} dy \\
&\leq C |\lambda|^2 |d_i|
\end{align*}

\end{enumerate}

Putting all of this together, we obtain the center jump expressions

\begin{align*}
\xi^c_i = e^{-\nu(\lambda) X_i} c_i^- - e^{\nu(\lambda) X_{i-1}} c_{i-1}^- - \lambda^2 d_i M^c + R^c(\lambda)_i(c^-, d)
\end{align*}

where $M^c$ is the center Melnikov integral

\[
M^c = \int_{-\infty}^\infty \langle \Psi^c(y), H(y) \rangle dy 
\]

The remainder term $R^c_i(c^-, d)$ has bound

\begin{align*}
R^c&(c^-, d)_i \leq C \Big(
(|\lambda| + e^{-\alpha X_m})(|\tilde{c}_{i-1}^+| + |\tilde{c}_{i}^-|) + (|\lambda| + e^{-\alpha X _m})( e^{-(\alpha - \rho) X_{i-1}} |\tilde{c}_{i-2}^+| + e^{-(\alpha - \rho) X_i} |\tilde{c}_{i+1}^-|)  \\
&+ e^{-\alpha X_m}( e^{-(\alpha - \rho) X_i}|c_i^-| + e^{-(\alpha - \rho) X_{i-1}}|c_{i-1}^-| )
+ (|\lambda| + e^{-\tilde{\alpha} X_m})(|\lambda| + e^{-\alpha X_m})|d|
\Big)
\end{align*}

where we used the estimate $|D| = \mathcal{O}(e^{-\alpha X_m})$. We can absorb the remainders terms involving $|c_i^-|$ and $|c_{i-1}^-|$ into those involving $|\tilde{c}_{i-1}^+|$ and $|\tilde{c}_{i}^-|$ to get

\begin{align*}
R^c&(c^-, d)_i \leq C \Big(
(|\lambda| + e^{-\alpha X_m})(|\tilde{c}_{i-1}^+| + |\tilde{c}_{i}^-|) + (|\lambda| + e^{-\alpha X_m})( e^{-(\alpha - \rho) X_{i-1}} |\tilde{c}_{i-2}^+| + e^{-(\alpha - \rho) X_i} |\tilde{c}_{i+1}^-|)  \\
&+ (|\lambda| + e^{-\tilde{\alpha} X_m})(|\lambda| + e^{-\alpha X_m})|d|
\Big)
\end{align*}

We would like to write this in matrix form in a convenient way. First, let

\begin{align*}
K(\lambda) =  
\begin{pmatrix}
e^{-\nu(\lambda)X_1} & & & & & -e^{\nu(\lambda)X_0} \\
-e^{\nu(\lambda)X_1} & e^{-\nu(\lambda)X_2} \\
& -e^{\nu(\lambda)X_2} & e^{-\nu(\lambda)X_3} \\
\vdots & & \vdots & &&  \vdots \\
& & & & -e^{\nu(\lambda)X_{n-1}} & e^{-\nu(\lambda)X_0} 
\end{pmatrix}
\end{align*}

The terms in the center jump expression involving $c$ are a perturbation of $K(\lambda)c$, where $c = (c_1, \dots, c_{n-1}, c_0)^T$. Of course, we need to be more precise with this.\\

Let $T_i(\lambda)(c^-)$ be the terms involving $\tilde{c}$ in $R^c(\lambda)_i(c^-, d)$. We can write these as

\[
T_i(\lambda) = \gamma_{i,i-1} \tilde{c}_{i-1}^+ + \gamma_{i,i} \tilde{c}_{i}^- + \gamma_{i,i-2} \tilde{c}_{i-2}^+ + \gamma_{i,i+1} \tilde{c}_{i+1}^-
\] 

where

\begin{align*}
\gamma_{i,i-1}, \gamma_{i,i} &= \mathcal{O}(|\lambda| + e^{-\alpha X_m}) \\
\gamma_{i,i-2} &= \mathcal{O}(e^{-(\alpha - \rho) X_{i-1}}(|\lambda| + e^{-\alpha X_m})) \\
\gamma_{i,i+1} &= \mathcal{O}(e^{-(\alpha - \rho) X_i}(|\lambda| + e^{-\alpha X_m}))
\end{align*}

Anticipating what we want to do, we add and subtract $\tilde{c}_i^+$ and $\tilde{c}_{i-1}^-$.

\begin{align*}
T_i(\lambda) &= \gamma_{i,i-1} \tilde{c}_{i-1}^+ + \gamma_{i,i} \tilde{c}_{i}^- + \gamma_{i,i-2} ( \tilde{c}_{i-2}^+ - \tilde{c}_{i-1}^-) + \gamma_{i,i-2} \tilde{c}_{i-1}^- \\
& + \gamma_{i,i+1} (\tilde{c}_{i+1}^- - \tilde{c}_i^+) + \gamma_{i,i+1} \tilde{c}_i^+
\end{align*}

Next, we note that

\begin{align*}
\gamma_{i,i-2} \tilde{c}_{i-1}^- &= \mathcal{O}(e^{-(\alpha - \rho) X_{i-1}}(|\lambda| + e^{-\alpha X_m})|e^{-\nu(\lambda)}c_{i-1}^-|) \\
&= \mathcal{O}(e^{-(\alpha - 3 \rho) X_{i-1}}(|\lambda| + e^{-\alpha X_m})|e^{\nu(\lambda)}c_{i-1}^-|) \\
&= \mathcal{O}(e^{-\tilde{\alpha} X_{i-1}}(|\lambda| + e^{-\alpha X_m})|\tilde{c}_{i-1}^+|) \\
\end{align*}

Similarly,

\begin{align*}
\gamma_{i,i+1} \tilde{c}_i^+ &= \mathcal{O}(e^{-\tilde{\alpha} X_i}(|\lambda| + e^{-\alpha X_m})|\tilde{c}_i^-|)
\end{align*}

Both of these coefficients are higher order than $\gamma_{i,i-1}$ and $\gamma_{i,i}$. Substituting these into $T_i(\lambda)$, collecting terms, and keeping the notation $\gamma_{i,j}$ for the resulting coefficients, we have

\begin{align*}
T_i(\lambda) &= \gamma_{i,i-1} \tilde{c}_{i-1}^+ + \gamma_{i,i} \tilde{c}_{i}^- + \gamma_{i,i-2} ( \tilde{c}_{i-2}^+ - \tilde{c}_{i-1}^-) + \gamma_{i,i+1} (\tilde{c}_{i+1}^- - \tilde{c}_i^+)
\end{align*}

where we have the same bounds on the coefficients $b_{i,j}$. We have what we need to write the center jump expressions in matrix form.

\[
(K(\lambda) + C_1 K(\lambda) + K_1(\lambda)) c + D_1 d = 0
\]

where $K_1(\lambda)$ is the following ``$\gamma-$perturbation'' of $K(\lambda)$ 

\begin{align*}
K_1(\lambda) =  
\begin{pmatrix}
e^{-\nu(\lambda)X_1} \gamma_{1,1} & & & & & e^{\nu(\lambda)X_0}\gamma_{1,0} \\
e^{\nu(\lambda)X_1}\gamma_{2,1} & e^{-\nu(\lambda)X_2}\gamma_{2,2} \\
& e^{\nu(\lambda)X_2}\gamma_{3,2} & e^{-\nu(\lambda)X_3}\gamma_{3,3} \\
\vdots & & \vdots & &&  \vdots \\
& & & & e^{\nu(\lambda)X_{n-1}}\gamma_{0,n-1} & e^{-\nu(\lambda)X_0}\gamma_{0,0} 
\end{pmatrix}
\end{align*}

$C_1$ is the periodic, banded matrix

\begin{align*}
C_1 &= \begin{pmatrix}
0 & \gamma_{1,2} & 0 & 0 & \dots & 0 & -\gamma_{n-1,0} & 0 \\
0 & 0 & \gamma_{2,3} & 0 & \dots & 0 & 0 & -\gamma_{2,1} \\
-\gamma_{3,1} & 0 & 0 & \gamma_{3,4} & \dots & 0 & 0 & 0 \\
&  & & \ddots  \\
0 & 0 & 0 & 0 & \dots & 0 & 0 & \gamma_{n-1,0} \\
\gamma_{0,1} & 0 & 0 & 0 & \dots & -\gamma_{0, n-2} & 0 & 0 
\end{pmatrix}
\end{align*}

and $D_1$ is the matrix we get from combining the terms involving $d$ in $R^c(\lambda)_i(c^-, d)$ with $\lambda^2 d_i M^c$, which is the same order.\\

For $C_1$ and $D_1$, we have uniform bounds

\begin{align*}
C_1 &= \mathcal{O}(e^{-\tilde{\alpha} X_m}(|\lambda| + e^{-\alpha X_m})) \\
D_1 &= \mathcal{O}((|\lambda| + e^{-\tilde{\alpha} X_m})(|\lambda| + e^{-\alpha X_m}))
\end{align*}

\end{proof}
\end{lemma}

\subsubsection{Decaying adjoint jump}

Finally, we compute the jump in the direction of $\Psi(0)$.

\begin{lemma}\label{jumpadj}

The jumps in the direction of $\Psi(0)$ are given by

\begin{align}\label{xi}
\xi_i = \langle \Psi(X_i), Q'(-X_i) \rangle (d_{i+1} - d_i)
+ \langle \Psi(-X_i), Q'(X_i) \rangle (d_i - d_{i-1})
- \lambda_2 d_i M + R_i(\lambda)(c^-, d)
\end{align}

where $M$ is the higher order Melnikov integral

\begin{equation}\label{M}
M = \int_{-\infty}^\infty \langle \Psi(y), H(y) \rangle dy 
\end{equation}

and the remainder term has bound

\begin{align}\label{R}
|R(\lambda)&(\tilde{c}, d)| \leq C \Big(
(|\lambda| + e^{-\alpha X_m})(|\tilde{c}_{i-1}^+| + |\tilde{c}_{i}^-|) + (|\lambda| + e^{-\tilde{\alpha} X_m})(e^{-\alpha X_m} + |\lambda|) ( |\tilde{c}_{i-2}^+| + |\tilde{c}_{i+1}^-|) \\
&+ (|\lambda| + e^{-\tilde{\alpha} X_m})( e^{-\alpha X_{i-2}} |c_{i-2}^-| + e^{-\alpha X_{i-1}} |c_{i-1}^-| + e^{-\alpha X_i} |c_i^-| + e^{-\alpha X_{i+1}} |c_{i+1}^-|) \nonumber \\
&+ (|\lambda| + e^{-\tilde{\alpha} X_m})(|\lambda| + e^{-\alpha X_m})^2 |d| \nonumber \Big)
\end{align}

We can write these conditions in matrix form as

\begin{equation}
C_2 K(\lambda) + K_2(\lambda) + (A - \lambda^2 M I + D_2)d = 0
\end{equation}

where the matrix $K(\lambda)$ is defined in Lemma \ref{jumpcenteradj} and the matrix $A$ is given by

\begin{align*}
A &= \begin{pmatrix}
-a_0 + \tilde{a}_1 & a_0 - \tilde{a}_1 \\
-\tilde{a}_0 + a_1 & \tilde{a}_0 - a_1
\end{pmatrix} && n = 2 \\
A &= \begin{pmatrix}
\tilde{a}_{n-1} - a_0 & a_0 & & & \dots & -\tilde{a}_{n-1}\\
-\tilde{a}_0 & \tilde{a}_0 - a_1 &  a_1 \\
& -\tilde{a}_1 & \tilde{a}_1 - a_2 &  a_2 \\
& & \vdots & & \vdots \\
a_{n-1} & & & & -\tilde{a}_{n-2} & \tilde{a}_{n-2} - a_{n-1} \\
\end{pmatrix} && n > 2
\end{align*}

with

\begin{align*}
a_i &= \langle \Psi(X_i), Q'(-X_i) \rangle \\
\tilde{a}_i &= \langle \Psi(-X_i), Q'(X_i) \rangle
\end{align*}

and the remainder matrices have uniform bounds

\begin{align*}
C_2 &= \mathcal{O}(e^{-\tilde{\alpha}X_m})(|\lambda| + e^{-\alpha X_m})) \\
D_2 &= \mathcal{O}((|\lambda| + e^{-\tilde{\alpha} X_m})(|\lambda| + e^{-\alpha X_m})^2)
\end{align*}

$K_2(\lambda)$ is a small perturbation of $K(\lambda)$. The specific forms of $K_2(\lambda)$ and $C_2$ are given in the proof.

\begin{proof}

Recall that the terms $P_i^\pm(0; \lambda) Z_i^\pm(0)$ are given by

\begin{align*}
P_i^-(0; \lambda) Z_i^-(0) &= P^-(0)( b_i^- + P_0^c(0) e^{\nu(\lambda) X_{i-1}} c_{i-1}^- ) \\
&+ P_i^-(0; \lambda) \Phi^s(0, -X_{i-1}; \lambda) a_{i-1}^- + (P_i^-(0; \lambda) - P^-(0))b_i^- + P_i^-(0; \lambda)(P_0^u(\lambda) - P_0^u(0))b_i^- \\
&+ (P_i^-(0; \lambda) - P^-(0)) P_0^c(0) e^{\nu(\lambda) X_{i-1}} c_{i-1}^- + P_i^-(0; \lambda) (P_0^c(\lambda) - P_0^c(0)) e^{\nu(\lambda) X_{i-1}} c_{i-1}^- \\
&+ \lambda^2 d_i P_i^-(0; \lambda) \int_{-X_{i-1}}^0 \Phi^s(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy \\
&+ \lambda^2 d_i P_i^-(0; \lambda) \int_{-X_{i-1}}^0 \Phi^c(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy  \\ 
P_i^+(0; \lambda) Z_i^+(0) &=  P^+(0)( b_i^+ + P_0^c(0) e^{-\nu(\lambda)X_i} c_i^+ )\\
&+ P_i^+(0; \lambda) \Phi^u(0, X_i; \lambda) a_i^+ + (P_i^+(0; \lambda) - P^+(0)) b_i^+ + P_i^+(0; \lambda) (P_0^s(\lambda) - P_0^s(0)) b_i^+ \\
&+ (P_i^+(0; \lambda) - P^+(0))P_0^c(0) e^{-\nu(\lambda)X_i} c_i^+ + P_i^+(0; \lambda) (P_0^c(\lambda) - P_0^c(0)) e^{-\nu(\lambda)X_i} c_i^+\\
&+ \lambda^2 d_i P_i^+(0; \lambda) \int_{X_i}^0 \Phi^u(0, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
&+ \lambda^2 d_i P_i^+(0; \lambda) \int_{X_i}^0 \Phi^c(0, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
\end{align*}

As with the first jump, we will start out by computing the significant terms.

\begin{enumerate}
\item The non-center integral will give us the higher order Melnikov integral. For the ``minus'' piece, we have

\begin{align*}
&\langle \Psi(0), P_i^-(0; \lambda) \int_{-X_{i-1}}^0 \Phi^s(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy \rangle \\
&= \int_{-X_{i-1}}^0 \langle \Psi(0), P_i^-(0; \lambda), \Phi^s(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}(y) \rangle dy \\
&= \int_{-X_{i-1}}^0 \langle \Psi(0), P_i^-(0; \lambda), \Phi^s(0, y; \lambda) P_i^-(y; \lambda)^{-1} H(y) \rangle dy + \mathcal{O}({e^{-\alpha X_m}})\\
&= \int_{-X_{i-1}}^0 \langle \Psi(0), \Theta(0, y) H(y) \rangle dy + \mathcal{O}(|\lambda| + {e^{-\alpha X_m}})\\
&= \int_{-X_{i-1}}^0 \langle \Theta(y, 0)^* \Psi_i(0), H(y) \rangle dy + \mathcal{O}(|\lambda| + {e^{-\alpha X_m}})\\
&= \int_{-X_{i-1}}^0 \langle \Psi(y), H(y) \rangle dy + \mathcal{O}(|\lambda| + {e^{-\alpha X_m}})\\
&= \int_{-\infty}^0 \langle \Psi(y), H(y) \rangle dy + \mathcal{O}(|\lambda| + {e^{-\alpha X_m}})\\
\end{align*}

The ``positive'' piece is similar, and gives us the other half of the Melnikov integral.

\item For the terms involving $a_i$, we plug in $A_4$.

\begin{align*}
\langle &\Psi(0), P_i^-(0; \lambda) \Phi^s(0, -X_{i-1}; \lambda) a_{i-1}^- \rangle \\
&= \langle \Psi_i(0), P_i^-(0; \lambda) \Phi^s(0, -X_{i-1}; \lambda) (- P_i^-(-X_{i-1}; \lambda)^{-1} P_0^s(\lambda) D_{i-1} d + A_4(\lambda)_{i-1}^-(c^-, d)) \rangle \\
&= -\langle \Psi(0), \Theta^s(0, -X_{i-1}) P_0^s(0) D_{i-1} d \rangle + \mathcal{O}( |\lambda|e^{-2 \alpha X_m} + e^{-\alpha X_{i-1}} |A_4(\lambda)_{i-1}^-(c^-, d)|)\\
&= -\langle \Theta^s(-X_{i-1}, 0)^* \Psi_i(0), P_0^s(0) D_{i-1} d \rangle + \mathcal{O}( |\lambda|e^{-2 \alpha X_m} + e^{-\alpha X_{i-1}} |A_4(\lambda)_{i-1}^-(c^-, d)|)\\
&= -\langle \Psi(-X_{i-1}), P_0^s(0) D_{i-1} d \rangle + \mathcal{O}\Big( |\lambda|e^{-2 \alpha X_m} + e^{-\alpha X_{i-1}} ( 
e^{-\alpha X_{i-1}}(|\lambda| + e^{-\tilde{\alpha}X_m})(|\tilde{c}_{i-2}^+| + |\tilde{c}_i^-|) \\
&+ e^{-(\alpha - \rho) X_{i-1}} |c_{i-1}^-| + e^{-\tilde{\alpha} X_m} |\lambda|^2|d| + e^{-\alpha X_m}|D||d|) \Big) \\
&= -\langle \Psi(-X_{i-1}), P_0^s(0) D_{i-1} d \rangle 
+ \mathcal{O}\Big(  
e^{-2 \alpha X_m}(|\lambda| + e^{-\tilde{\alpha}X_{i-1}})(|\tilde{c}_{i-2}^+| + |\tilde{c}_i^-|) \\
&+ e^{-\alpha X_m}e^{-(\alpha - \rho) X_{i-1}} |c_{i-1}^-| + e^{-(\alpha + \tilde{\alpha}) X_m} |\lambda|^2|d| + e^{-2 \alpha X_m}(|D| + |\lambda|)|d|) \Big) \\ 
\end{align*}

Similarly, for the $a_i^+$ term, we have

\begin{align*}
\langle &\Psi(0), P_i^+(0; \lambda) \Phi^u(0, X_i; \lambda) a_i^+ \rangle \\
&= \langle \Psi(X_i), P_0^u(0) D_i d \rangle + \mathcal{O}\Big( e^{-2 \alpha X_i} (|\lambda| + e^{-\tilde{\alpha}X_m})(|\tilde{c}_{i-1}^+| + |\tilde{c}_{i+1}^-|) \\
&+ e^{-\alpha X_m} e^{-(\alpha - \rho) X_i} |c_i^-| e^{-(\alpha + \tilde{\alpha}) X_m} |\lambda|^2|d| + e^{-2 \alpha X_m}(|D| + |\lambda|)|d|) \Big)
\end{align*}

\end{enumerate}

The remaining terms will be higher order. Doing these in turn, we have

\begin{enumerate}
\item For the terms involving $b$, we first note that by Lemma \ref{PsiIP}, the terms $P^-(0) b_i^-$ and $P^+(0)b_i^+$ will vanish when we take the inner product with $\Psi(0)$. For the remaining terms, we substitute the estimate for $B_1$ from Lemma \ref{inv2}.

\begin{align*}
&|\langle \Psi(0), (P_i^-(0; \lambda) - P^-(0))b_i^- + P_i^-(0; \lambda)(P_0^u(\lambda) - P_0^u(0))b_i^-| \\
&\leq C (|\lambda| + e^{-\alpha X_m})\Big( 
(|\lambda| + e^{-\tilde{\alpha}X_m})( |\tilde{c}_{i-1}^+| + |\tilde{c}_i^-|)+ ( e^{-\tilde{\alpha}X_m} |D| + e^{-\tilde{\alpha}X_m}|\lambda| + |\lambda|^2)|d| \Big)
\end{align*}

\item For the terms involving $c$, we first note that by Lemma \ref{PsiIP}, the terms $P_0^c(0) e^{\nu(\lambda) X_{i-1}} c_{i-1}^-$ and $P_0^c(0) e^{-\nu(\lambda)X_i} c_i^+$ will be eliminated by taking the inner product with $\Psi(0)$. For the remaining term involving $c_{i-1}^-$, we have

\begin{align*}
|(P_i^-(0; \lambda) - P^-(0)) P_0^c(0) e^{\nu(\lambda) X_{i-1}} c_{i-1}^- + P_i^-(0; \lambda) (P_0^c(\lambda) - P_0^c(0)) e^{\nu(\lambda) X_{i-1}} c_{i-1}^-| \leq C (|\lambda| + e^{-\alpha X_m})|\tilde{c}_{i-1}^+|
\end{align*}

For the term involving $c_i^+$, we also have to use our expression from Lemma \ref{inv2} to convert $e^{-\nu(\lambda)X_i} c_i^+$ to $e^{-\nu(\lambda)X_i} c_i^-$.

\begin{align*}
&|(P_i^+(0; \lambda) - P^+(0))P_0^c(0) e^{-\nu(\lambda)X_i} c_i^+ + P_i^+(0; \lambda) (P_0^c(\lambda) - P_0^c(0)) e^{-\nu(\lambda)X_i} c_i^+| \\
&\leq C(|\lambda| + e^{-\alpha X_m})\Big( |\tilde{c}_i^-| + e^{-(\alpha - \rho)X_i} (|\lambda| + e^{-\tilde{\alpha}X_m})( |\tilde{c}_{i-1}^+| + |\tilde{c}_{i+1}^-|) 
+ e^{-(\alpha - 2 \rho) X_i}|c_i^-| + e^{-\tilde{\alpha}X_m}(|\lambda| + |D_i|)|d| \Big) \\
&\leq C(|\lambda| + e^{-\alpha X_m})\Big( |\tilde{c}_i^-| + e^{-(\alpha - \rho)X_i} (|\lambda| + e^{-\tilde{\alpha}X_m})( |\tilde{c}_{i-1}^+| + |\tilde{c}_{i+1}^-|) +  e^{-\tilde{\alpha}X_m}(|\lambda| + |D_i|)|d| \Big) 
\end{align*}

where we absorbed the $e^{-(\alpha - 2 \rho) X_i}|c_i^-|$ term into the lower order term $|\tilde{c}_i^-|$.

\item For the center integral term, we have

\begin{align*}
&\langle \Psi(0), P_i^-(0; \lambda)
\int_{-X_{i-1}}^0 \Phi^c(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy \rangle \\
&= \int_{-X_{i-1}}^0 \langle \Psi(0), P_i^-(0; \lambda) \Phi^c(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) \rangle dy \\
&= \int_{-X_{i-1}}^0 \langle \Psi(0), P^-(0) \Phi^c(0, y; 0) P^-(y)^{-1} \tilde{H}_i^-(y) \rangle dy + \mathcal{O}(|\lambda| + e^{-\alpha X_m}) \\
&= \mathcal{O}(|\lambda| + e^{-\alpha X_m})
\end{align*}

where the integral vanishes by Lemma \ref{PsiIP} since $\Phi^c(0, y; 0) P^-(y)^{-1} \tilde{H}_i^-(y) \in E^c(0)$.

\end{enumerate}

Putting this all together, we have the jump expressions

\begin{align*}
\xi_i = \langle \Psi(X_i), P_0^u(0) D_i d \rangle
+ \langle \Psi(-X_{i-1}), P_0^s(0) D_{i-1} d \rangle 
- \lambda_2 d_i M + R_i(\lambda)(c, \tilde{c}, d)
\end{align*}

where $M$ is the higher order Melnikov integral

\[
M = \int_{-\infty}^\infty \langle \Psi(y), H(y) \rangle dy 
\]

and the remainder term has piecewise bound

\begin{align*}
|R(\lambda)_i&(\tilde{c}, d)| \leq C \Big( \\
&(|\lambda| + e^{-\alpha X_m})(|\tilde{c}_{i-1}^+| + |\tilde{c}_{i}^-|) + (|\lambda| + e^{-\tilde{\alpha} X_m})( e^{-(\alpha - \rho) X_{i-1}}|\tilde{c}_{i-2}^+| + e^{-(\alpha - \rho) X_i}|\tilde{c}_{i+1}^-|)  \\
&+ e^{-\alpha X_m}( e^{-(\alpha - \rho) X_{i-1}} |c_{i-1}^-| + e^{-(\alpha - \rho) X_i} |c_i^-|) \\
&+ (|\lambda| + e^{-\tilde{\alpha} X_m})|\lambda|^2 + e^{-2 \alpha X_m}(|\lambda| + |D|)|d| \Big)
\end{align*}

Using the fact that $|D| = \mathcal{O}(e^{-\alpha X_m})$ and absorbing the higher order $|c_{i-1}^-|$ and $|c_i^|$ terms into the lower order $|\tilde{c}_{i-1}^+|$ and $|\tilde{c}_{i}^-|$ terms, this becomes

\begin{align*}
|R(\lambda)_i&(c_i^-, d)| \leq C \Big( \\
&(|\lambda| + e^{-\alpha X_m})(|\tilde{c}_{i-1}^+| + |\tilde{c}_{i}^-|) + (|\lambda| + e^{-\tilde{\alpha} X_m})( e^{-(\alpha - \rho) X_{i-1}}|\tilde{c}_{i-2}^+| + e^{-(\alpha - \rho) X_i}|\tilde{c}_{i+1}^-|) \\
&+ (|\lambda| + e^{-\tilde{\alpha} X_m})(|\lambda| + e^{-\alpha X_m})^2 |d| \Big)
\end{align*}

Before we write this in matrix form, we substitute for $D_i d$. Recalling the expression for $D_i$, we have

\begin{align*}
\langle \Psi(X_i), P_0^u(0) D_i d \rangle
&= \langle \Psi(X_i), P_0^u(0) (Q'(X_i) + Q'(-X_i)) \rangle (d_{i+1} - d_i)
+\mathcal{O}(e^{-2 \alpha X_i}(|\lambda| + e^{-\alpha X_i})) \\
&= \langle \Psi(X_i), Q'(X_i) + Q'(-X_i) \rangle (d_{i+1} - d_i)
+\mathcal{O}(e^{-2 \alpha X_i}(|\lambda| + e^{-\alpha X_i})) \\
&= \langle \Psi(X_i), Q'(-X_i) \rangle (d_{i+1} - d_i)
+\mathcal{O}(e^{-2 \alpha X_i}(|\lambda| + e^{-\alpha X_i})) 
\end{align*}

since $\langle \Psi(X_i), Q'(X_i) \rangle = 0$. Similarly, 

\begin{align*}
\langle \Psi(-X_i), P_0^s(0) D_i d \rangle
&= \langle \Psi(-X_i), Q'(X_i) \rangle (d_i - d_{i-1})
+\mathcal{O}(e^{-2 \alpha X_i}(|\lambda| + e^{-\alpha X_i})) 
\end{align*}

Since these remainder terms are already included in our remainder term, we obtain the final jump expressions

\begin{align*}
\xi_i = \langle \Psi(X_i), Q'(-X_i) \rangle (d_{i+1} - d_i)
+ \langle \Psi(-X_i), Q'(X_i) \rangle (d_i - d_{i-1})
- \lambda_2 d_i M + R_i(\lambda)(c^-, d)
\end{align*}

where $R_i(\lambda)(c^-, d)$ has the same remainder bound as above. As in Lemma \ref{jumpcenteradj}, we will write these jump expressions in matrix form. The remainder terms involving the $\tilde{c}$ work out exactly the same as in Lemma \ref{jumpcenteradj}, except the remainder coefficients are different and we do not have the $\tilde{c}$ terms by themselves. Thus, in matrix form, we have

\[
C_2 K(\lambda) + K_2(\lambda) + (A - \lambda^2 M I + D_2)d = 0
\]

where the matrix $A$ is given by

\begin{align*}
A &= \begin{pmatrix}
-a_0 + \tilde{a}_1 & a_0 - \tilde{a}_1 \\
-\tilde{a}_0 + a_1 & \tilde{a}_0 - a_1
\end{pmatrix} && n = 2 \\
A &= \begin{pmatrix}
\tilde{a}_{n-1} - a_0 & a_0 & & & \dots & -\tilde{a}_{n-1}\\
-\tilde{a}_0 & \tilde{a}_0 - a_1 &  a_1 \\
& -\tilde{a}_1 & \tilde{a}_1 - a_2 &  a_2 \\
& & \vdots & & \vdots \\
a_{n-1} & & & & -\tilde{a}_{n-2} & \tilde{a}_{n-2} - a_{n-1} \\
\end{pmatrix} && n > 2
\end{align*}

where

\begin{align*}
a_i &= \langle \Psi(X_i), Q'(-X_i) \rangle \\
\tilde{a}_i &= \langle \Psi(-X_i), Q'(X_i) \rangle
\end{align*}

$M$ is the higher order Melnikov integral

\[
M = \int_{-\infty}^\infty \langle \Psi(y), H(y) \rangle dy
\]

$K_2(\lambda)$ is the following ``$\tilde{\gamma}-$perturbation'' of $K(\lambda)$ 

\begin{align*}
K_1(\lambda) =  
\begin{pmatrix}
e^{-\nu(\lambda)X_1} \tilde{\gamma}_{1,1} & & & & & e^{\nu(\lambda)X_0}\tilde{\gamma}_{1,0} \\
e^{\nu(\lambda)X_1}\tilde{\gamma}_{2,1} & e^{-\nu(\lambda)X_2}\tilde{\gamma}_{2,2} \\
& e^{\nu(\lambda)X_2}\tilde{\gamma}_{3,2} & e^{-\nu(\lambda)X_3}\tilde{\gamma}_{3,3} \\
\vdots & & \vdots & &&  \vdots \\
& & & & e^{\nu(\lambda)X_{n-1}}\tilde{\gamma}_{0,n-1} & e^{-\nu(\lambda)X_0}\tilde{\gamma}_{0,0} 
\end{pmatrix}
\end{align*}

where 

\begin{align*}
\tilde{\gamma}_{i,i-1}, \tilde{\gamma}_{i,i} &= \mathcal{O}(|\lambda| + e^{-\alpha X_m}) \\
\end{align*}

$C_2$ is the periodic, banded matrix

\begin{align*}
C_2 &= \begin{pmatrix}
0 & \tilde{\gamma}_{1,2} & 0 & 0 & \dots & 0 & -\tilde{\gamma}_{n-1,0} & 0 \\
0 & 0 & \gamma_{2,3} & 0 & \dots & 0 & 0 & -\tilde{\gamma}_{2,1} \\
-\tilde{\gamma}_{3,1} & 0 & 0 & \tilde{\gamma}_{3,4} & \dots & 0 & 0 & 0 \\
&  & & \ddots  \\
0 & 0 & 0 & 0 & \dots & 0 & 0 & \tilde{\gamma}_{n-1,0} \\
\tilde{\gamma}_{0,1} & 0 & 0 & 0 & \dots & -\tilde{\gamma}_{0, n-2} & 0 & 0 
\end{pmatrix}
\end{align*}

where

\begin{align*}
\tilde{\gamma}_{i,i-2} &= \mathcal{O}(e^{-(\alpha - \rho) X_{i-1}}(|\lambda| + e^{-\alpha X_m})) \\
\tilde{\gamma}_{i,i+1} &= \mathcal{O}(e^{-(\alpha - \rho) X_i}(|\lambda| + e^{-\alpha X_m}))
\end{align*}

For the matrices $C_2$ and $D_2$, we have uniform bounds

\begin{align*}
C_2 &= \mathcal{O}(e^{-\tilde{\alpha}X_m})(|\lambda| + e^{-\alpha X_m})) \\
D_2 &= \mathcal{O}((|\lambda| + e^{-\tilde{\alpha} X_m})(|\lambda| + e^{-\alpha X_m})^2)
\end{align*}

\end{proof}
\end{lemma}

\subsubsection{Main Theorem}

We will now combine two jump expressions from Lemma \ref{jumpcenteradj} and Lemma \ref{jumpadj} into a single theorem. Before we do that, we will need one more result about the determinant of a particular matrix.

% bidiagonal determinant

\begin{lemma}\label{bidiag}
Let $A$ be the ``periodic'' bi-diagonal matrix
\begin{equation}
A = \begin{pmatrix}
a_1 & & & & & & b_n \\
b_1 & a_2 \\
& b_2 & a_3 \\
\vdots & & & \vdots & &&  \vdots \\
& & & & b_{n-2} & a_{n-1} \\
& & & & & b_{n-1} & a_n
\end{pmatrix}
\end{equation}

Then 

\begin{equation}
\det{A} = \prod_{k = 1}^n a_k + (-1)^n \prod_{k = 1}^{n-1} b_k
\end{equation}

\begin{proof}
Expanding by minors using the last column, we have
\begin{align*}
\det A &= a_n \det
\begin{pmatrix}
a_1 \\
b_1 & a_2 \\
& b_2 & a_3 \\
\vdots & & & & \vdots \\
& & & & b_{n-2} & a_{n-1}
\end{pmatrix}
+ (-1)^{n-1} \det
\begin{pmatrix}
b_1 & a_2 \\
& b_2 & a_3 \\
\vdots & & & & \vdots \\
& & & & & b_{n-2} & a_{n-1} \\
& & & & & & b_{n-1}
\end{pmatrix} \\
&= \prod_{k = 1}^n a_k + (-1)^{n-1} \prod_{k = 1}^n b_k
\end{align*}
since both of the matrices on the RHS are triangular.
\end{proof}
\end{lemma}

% theorem : block diagonal matrix expression

We can finally state the main theorem of this section.

\begin{theorem}\label{blockmatrixform}

Let $q_{np}(x)$ be a periodic $n-$pulse solution constructed with lengths $X_0, \dots, X_{n-1}$. Then the jump conditions can be written as the block matrix equation 

\begin{equation}\label{blockeq}
\begin{pmatrix}
K(\lambda) & D_2 \\
C_3 K(\lambda) + K(\lambda) \tilde{C}_3 & A - \lambda^2 MI + D_3
\end{pmatrix}
\begin{pmatrix}c \\ d \end{pmatrix} 
= 0
\end{equation}

where 

\begin{enumerate}

\item The remainder terms have bounds

\begin{align*}
C_3, \tilde{C}_3 &= \text{diag}(\mathcal{O}(|\lambda| + e^{-\alpha X_m})) 
+ \mathcal{O}((|\lambda| + e^{-\tilde{\alpha} X_m})( |\lambda| + e^{-\alpha X_m})) \\
D_2 &= \mathcal{O}((|\lambda| + e^{-\tilde{\alpha} X_m})(|\lambda| + e^{-\alpha X_m})) \\
D_3 &= \mathcal{O}((|\lambda| + e^{-\tilde{\alpha} X_m})(|\lambda| + e^{-\alpha X_m})^2)
\end{align*}

where $X_m = \min \{X_0, \dots, X_{n-1}\}$

\item $M$ is the Melnikov integrals

\begin{align*}
M &= \int_{-\infty}^\infty \langle \Psi(y), H(y) \rangle dy \\
\end{align*}

For KdV5, this is

\begin{align*}
M &= \int_{-\infty}^\infty q(y) q_c(y) dy \\
\end{align*}

\item The matrix $K(\lambda)$ is given by

\begin{equation}
K(\lambda) = 
\begin{pmatrix}
e^{-\nu(\lambda)X_1} & & & & & -e^{\nu(\lambda)X_0} \\
-e^{\nu(\lambda)X_1} & e^{-\nu(\lambda)X_2} \\
& -e^{\nu(\lambda)X_2} & e^{-\nu(\lambda)X_3} \\
\vdots & & \vdots & &&  \vdots \\
& & & & -e^{\nu(\lambda)X_{n-1}} & e^{-\nu(\lambda)X_0} 
\end{pmatrix}
\end{equation}

where $\nu(\lambda)$ is the small eigenvalue of the asympotic matrix $A(\lambda)$.

\item The matrix $A$ is given by

\begin{align*}
A &= \begin{pmatrix}
-a_0 + \tilde{a}_1 & a_0 - \tilde{a}_1 \\
-\tilde{a}_0 + a_1 & \tilde{a}_0 - a_1
\end{pmatrix} && n = 2 \\
A &= \begin{pmatrix}
\tilde{a}_{n-1} - a_0 & a_0 & & & \dots & -\tilde{a}_{n-1}\\
-\tilde{a}_0 & \tilde{a}_0 - a_1 &  a_1 \\
& -\tilde{a}_1 & \tilde{a}_1 - a_2 &  a_2 \\
& & \vdots & & \vdots \\
a_{n-1} & & & & -\tilde{a}_{n-2} & \tilde{a}_{n-2} - a_{n-1} \\
\end{pmatrix} && n > 2
\end{align*}

where

\begin{align*}
a_i &= \langle \Psi(X_i), Q'(-X_i) \rangle \\
\tilde{a}_i &= \langle \Psi(-X_i), Q'(X_i) \rangle
\end{align*}

For KdV5, we have $\tilde{a}_i = a_i$.

\end{enumerate}

% To leading order, equation \eqref{blockeq} is upper triangular block equation

% \begin{equation}\label{blocktri}
% \begin{pmatrix}
% K(\lambda) & D_2  \\
% 0 & A - \lambda^2 MI 
% \end{pmatrix}
% \begin{pmatrix}c \\ d \end{pmatrix} = 0
% \end{equation}

% The leading order equations have a nontrivial solution if either of the following conditions holds.

% \begin{enumerate}[(i)]
% \item $\nu(\lambda) = i \dfrac{n \pi}{X}, n \in \Z$ 
% \item $\det(A - \lambda^2 MI) = 0$
% \end{enumerate}

% where $X = X_0 + \dots + X_{n-1}$ is half the length of the domain. 

\begin{proof}
The block matrix \eqref{blockeq} combines the jump conditions from Lemma \ref{jumpcenteradj} and Lemma \ref{jumpadj}. \\

% As it is a square matrix, \eqref{blockeq} has a nontrivial solution if and only if its determinant is 0.\\

% To leading order, the block matrix \eqref{blockeq} is the upper triangular block matrix \eqref{blocktri}, which has determinant 

% \begin{align*}
% \det \begin{pmatrix}
% K(\lambda) & -\lambda^2 M(\lambda) I \\
% 0 & A - \lambda^2 MI
% \end{pmatrix} &= \det K(\lambda) \det(A - \lambda^2 MI)\\
% \end{align*}

% Using Lemma \ref{bidiag}, 

% \begin{align*}
% \det K(\lambda) &= \prod_{k = 0}^{n-1} e^{-\nu(\lambda)X_k} + (-1)^{n-1} \prod_{k = 0}^{n-1} (-e^{\nu(\lambda)X_k}) \\
% &= e^{-\nu(\lambda) X} - e^{\nu(\lambda) X} \\
% &= -2 \sinh( \nu(\lambda) X )
% \end{align*}

% where $X = X_0 + \dots + X_{n-1}$ is half the length of the domain. We have a nontrivial solution to the leading order equations if and only if 

% \[
% \nu(\lambda) = i \frac{n \pi}{X}, n \in \Z
% \]

% or 

% \[
% \det(A - \lambda^2 MI) = 0
% \]

\end{proof}
\end{theorem}

\subsection{Trying to Make This Work}

In most cases, or at least the cases we care about (such as KdV5), we will have (from symmetry of the primary pulse)

\[
\tilde{a}_i = \langle \Psi(-X_i), Q'(X_i) \rangle
= -\langle \Psi(X_i), Q'(-X_i) \rangle = a_i
\]

Thus the matrix $A$ simplifies to

\begin{align*}
A &= \begin{pmatrix}
-a_0 -a_1 & a_0 + a_1 \\
a_0 + a_1 & -a_0 - a_1
\end{pmatrix} && n = 2 \\
A &= \begin{pmatrix}
-a_{n-1} - a_0 & a_0 & & & \dots & a_{n-1}\\
a_0 & -a_0 - a_1 &  a_1 \\
& a_1 & -a_1 - a_2 &  a_2 \\
& & \vdots & & \vdots \\
a_{n-1} & & & & a_{n-2} & -a_{n-2} - a_{n-1} \\
\end{pmatrix} && n > 2
\end{align*}

Note that this is a real symmetric matrix, so its eigenvalues are all real.\\

Before we continue, we will derive a bunch of stuff which will apply for all the following cases. In all cases, $X_{n-1}$ is the ``periodic length'', and we have

\[
X_{n-1} = X_M = \max\{X_0, \dots, X_{n-1}\}
\]

First, we look at $K(\lambda)$, which we recall is given by 

\begin{equation*}
K(\lambda) = 
\begin{pmatrix}
e^{-\nu(\lambda)X_1} & & & & & -e^{\nu(\lambda)X_0} \\
-e^{\nu(\lambda)X_1} & e^{-\nu(\lambda)X_2} \\
& -e^{\nu(\lambda)X_2} & e^{-\nu(\lambda)X_3} \\
\vdots & & \vdots & &&  \vdots \\
& & & & -e^{\nu(\lambda)X_{n-1}} & e^{-\nu(\lambda)X_0} 
\end{pmatrix}
\end{equation*}

For an estimate on the operator norm of this, we have

\[
||K(\lambda)|| \leq n ||K(\lambda)||_{\text{max}} = n e^{|\text{Re }\nu(\lambda)|X_{n-1}}
\]

Next, we have an expression for the inverse of $K(\lambda)$ (when it is invertible).

\begin{lemma}\label{Kinv}

When $\det K(\lambda) \neq 0$,

\begin{align*}
&K(\lambda)^{-1} = \frac{1}{\det K(\lambda)}\\
&\begin{pmatrix}
e^{-\nu(\lambda)(X_2+\dots+X_{n-1}+X_0)} & e^{-\nu(\lambda)(-X_2-\dots-X_{n-1}-X_0)} &
e^{-\nu(\lambda)(X_2-\dots-X_{n-1}-X_0)} & \dots & e^{-\nu(\lambda)(X_2+\dots+X_{n-1}-X_0)}  \\ 
e^{-\nu(\lambda)(X_3+\dots+X_0-X_1)} & e^{-\nu(\lambda)(X_3+\dots+X_0+X_1)} &
e^{-\nu(\lambda)(-X_3-\dots-X_0-X_1)} & \dots & e^{-\nu(\lambda)(X_3+\dots-X_0-X_1)}  \\ 
& \ddots & \ddots \\
e^{-\nu(\lambda)(-X_1-X_2 -\dots-X_{n-1})} & e^{-\nu(\lambda)(X_1-X_2 -\dots-X_{n-1})} &
e^{-\nu(\lambda)(X_1+X_2 -\dots-X_{n-1})} & \dots & e^{-\nu(\lambda)(X_1+X_2+\dots+X_{n-1})} 
\end{pmatrix}
\end{align*}

where

\[
\det K(\lambda) = e^{-\nu(\lambda)X} - e^{\nu(\lambda)X} = -2 \sinh(\nu(\lambda)X)
\]

and $X = X_0 + \dots + X_{n-1}$ is half the length of the domain. 

\begin{proof}
This can be verified directly. Note that each row is a cyclic permutation of the previous row, with everything shifted one place to the right and a different index omitted. For convenience, let $S(\lambda)$ be the matrix above without the $1/\det K(\lambda)$ out front. It is straighforward to see that 
\begin{align*}
[K(\lambda)S(\lambda)]_{jj} &= e^{-\nu(\lambda)(X_0 + \dots + X_{n-1})} - e^{\nu(\lambda)(X_0 + \dots + X_{n-1})} = \det K(\lambda) \\
[K(\lambda)S(\lambda)]_{jk} &= 0 && j \neq k
\end{align*}
The same holds for $S(\lambda)K(\lambda)$.
\end{proof}

\end{lemma}

$\det K(\lambda) = 0$ if and only if $\nu(\lambda) = i n \pi/X$ for $n \in Z$. What we want is the values of $\lambda$ (not $\nu(\lambda)$) for which $K(\lambda)$ is singular. \\

Define the function $G(\lambda, r) = \nu(\lambda) - r$. Then $G(0, 0) = 0$ and $D_\lambda G(0, 0) = \nu'(0) = -1/c_0$ (which we are assuming is nonzero). Using the IFT, we can solve for $\lambda$ in terms of $r$ for $r$ near 0. In other words, we can find a function $\lambda(r)$ such that $\lambda(0) = 0$ and $G(\lambda(r), r) = 0$ for sufficiently small $r$. Thus, for sufficiently small $r$, $\nu(\lambda(r)) = r$. Thus for $n/X$ sufficiently small, we can find $\lambda(X, n)$ such that $\nu(\lambda(X, n)) = n \pi i / X$, i.e. $\det K(\lambda(X, n)) = 0$.\\

Expanding $\lambda(r)$ in a Taylor series about $r = 0$, we have

\begin{align*}
\lambda(r)
&= \lambda(0) + \lambda'(0) r + \mathcal{O}(|r|^2)
\end{align*}

Taking $r = n \pi i / X$, we have for $n/X$ small,

\begin{align*}
\lambda(X,n)
&= \frac{1}{\nu'(0)} \frac{n \pi i }{X} + \mathcal{O}(n/X)^2 \\
&= -c_0 \frac{n \pi i }{X} + \mathcal{O}(n/X)^2 \\
\end{align*} 

For $\lambda \neq \lambda(X,n)$, $K(\lambda)$ is invertible. I BELIEVE THE FOLLOWING IS TRUE, BUT WE SHOULD PROVE IT AT SOME POINT. We have the following bound.

\[
||K(\lambda)^{-1}|| \leq C \frac{e^{|\text{Re }\nu(\lambda)|X }}{\det K(\lambda)}
\]

This blows up at the points where $K(\lambda)$ is singular. Next, we obtain a lower bound for $K(\lambda)$ for $\lambda$ more than $\epsilon$ away from any of the $\lambda(X, n)$ within $\mathcal{O}(e^{-\alpha X_m}$ of the origin.\\

Expand $K(\lambda)$ in a Taylor series about each $\lambda(X, n)$ which is within $\mathcal{O}(e^{-\alpha X_0})$ of the origin. (We only have to make sure the potential interaction eigenvalues do not lie near these points).

\begin{align*}
\det K( \lambda(X, n) + \epsilon )
&= -2 \sinh \Big( \nu(\lambda(X, n) + \epsilon )X \Big) \\
\end{align*}

Substitute the Taylor series for $\nu(\lambda)$, but this time, we use the Taylor expansion about $\lambda(X, n)$, since we know that $\nu( \lambda(X, n) ) = n \pi i / X$.

\begin{align*}
\nu(\lambda(X, n) + \epsilon ) &= 
\frac{n \pi i }{X} + \mathcal{O}(\epsilon)
\end{align*}

Plugging this in above, we get

\begin{align*}
\det K( \lambda(X, n) + \epsilon )
&= -2 \sinh \Big( \nu(\lambda(X, n) + \epsilon )X \Big) \\
&= -2 \sinh \left( n \pi i + \mathcal{O}(\epsilon X) \right) \\
&= \mathcal{O}(\epsilon X )
\end{align*}

Note that, to leading order, this does not depend on $n$. As long we we are further than $\epsilon$ in distance from any of the finite number of $\lambda(X_0, n)$ within $\mathcal{O}(e^{-\alpha X_0})$ of the origin, the bound holds.\\ 

The thing we actually need a bound on is $||K(\lambda)||\:||K(\lambda)^{-1}||$ for $\lambda$ more than $\epsilon$ away from any of the $\lambda(X_0, n)$ within $\mathcal{O}(e^{-\alpha X_0})$ of the origin.




\subsubsection{2-pulse with equal distances}

We do the easiest case first. Take the 2-periodic solution, with the special case $X_0 = X_1$. 

\begin{enumerate}

\item First, we write down the matrices $A$ and $K(\lambda)$ for this specific case. For the matrix $A$, we have

\[
A = a \begin{pmatrix}
-1 & 1 \\
1 & -1
\end{pmatrix}
\]

where $a = a_0 + a_1 = 2 \langle \Psi(X_0), Q'(-X_0) \rangle$. For the matrix $K(\lambda)$ we have

\begin{equation}
K(\lambda) = 
\begin{pmatrix}
e^{-\nu(\lambda)X_0} & -e^{\nu(\lambda)X_0} \\
-e^{\nu(\lambda)X_0} & e^{-\nu(\lambda)X_0}
\end{pmatrix}
\end{equation}

\item We will look for the interaction eigenvalues first. We expect these to occur when $\det(A - \lambda^2 M I) = 0$. Since $a = \mathcal{O}(e^{-2 \alpha X_0})$, we use the following scaling for $\lambda$ and $a$.

\begin{align*}
\lambda &= e^{-\alpha X_0} \tilde{\lambda} \\
a &= e^{-2 \alpha X_0} \tilde{a}
\end{align*}

With this scaling, the matrix $A$ becomes 

\[
A = e^{-2\alpha X_0} \tilde{a} \tilde{A}
\]

where
\[
\tilde{A} = \begin{pmatrix}
-1 & 1 \\
1 & -1
\end{pmatrix}
\]

\item Next, we expand $\nu(\lambda)$ in a Taylor series about $\lambda = 0$. We can always do this, since $\nu(\lambda)$ is a root of a polynomial. Since $\nu(0) = 0$, the Taylor expansion of $\nu(\lambda)$ is 

\begin{align*}
\nu(\lambda) &= \nu'(0)\lambda + \mathcal{O}(|\lambda|^2) \\
&= \nu'(0)\tilde{\lambda}e^{-\alpha X_0} + \mathcal{O}(e^{-2 \alpha X_0})
\end{align*}

where we used our scaling for $\lambda$ in the second line. We should be able to compute $\nu'(0)$, so let's do that now. The characteristic polynomial of $A_\infty(\lambda)$ is of the form

\[
P(\nu; \lambda) = 
-\nu^{2m+1} + c_{2m - 1} \nu^{2m} + \dots + c_0 \nu + \lambda = 0
\]

where the $c_i$ are real. $P(0; 0) = 0$ and $P_\nu(0; 0) = c_0$. Thus, as long as $c_0 \neq 0$ (for KdV5, $c_0 = -c$, where $c$ is the moving frame speed; we will take $c_0 \neq 0$ as a hypothesis), we can use the IFT to solve for $\nu$ as a function of $\lambda$ near $\lambda = 0$. This is our function $\nu(\lambda)$. By the IFT again, $\nu'(\lambda) = -P_\nu(\nu(\lambda); \lambda)^{-1} P_\lambda(\nu(\lambda); \lambda)$. At $\lambda = 0$, this is

\begin{align*}
\nu'(0) &= -\frac{1}{c_0}
\end{align*}

Numerics on KdV5 supports this result. Thus we have

\begin{align*}
\nu(\lambda) &= -\frac{1}{c_0}\tilde{\lambda}e^{-\alpha X_0} + \mathcal{O}(e^{-2 \alpha X_0})
\end{align*}

\item In this special case, we can use the Taylor expansion of $\nu(\lambda)$ to show that $K(\lambda) = \mathcal{O}(1)$.

\begin{align*}
||K(\lambda)|| &\leq C e^{|\nu(\lambda)|X_0} \\
 &= \mathcal{O} (1 + |\nu(\lambda)|X_0 + \mathcal{O}(|\nu(\lambda)|^2 X_0^2) \\
&= \mathcal{O}( 1 + |1/c_0| |\tilde{\lambda} | e^{-\alpha X_0} X_0 ) \\
&= \mathcal{O}(1)
\end{align*}

as long as we take $X_0$ sufficiently large so that, say, $e^{-\alpha X_0} X_0 \leq 1$.

\item We want to invert $K(\lambda)$ (when possible). To do this, we look at the determinant of $K(\lambda)$, which is given by

\[
\det K(\lambda) = e^{-2 \nu(\lambda)X_0} - e^{2 \nu(\lambda)X_0} = -2 \sinh(2 \nu(\lambda) X_0)
\]

From this, we see that $\det K(\lambda) = 0$ if and only if $2 \nu(\lambda)X_0 = n \pi i$, i.e. $\nu(\lambda) = n \pi i / 2 X_0$ for $n \in \Z$. This tells us the values of $\nu(\lambda)$ for which $\det K(\lambda) = 0$. What we really want is the values of $\lambda$ for which $\det K(\lambda) = 0$. So let's look at that.\\

Define the function $G(\lambda, r) = \nu(\lambda) - r$. Then $G(0, 0) = 0$ and $D_\lambda G(0, 0) = \nu'(0) = -1/c_0$, which we are assuming is nonzero. Using the IFT, we can solve for $\lambda$ in terms of $r$ for $r$ near 0. In other words, we can find a function $\lambda(r)$ such that $\lambda(0) = 0$ and $G(\lambda(r), r) = 0$ for sufficiently small $r$. Thus, for sufficiently small $r$, $\nu(\lambda(r)) = r$. For suffiently large $X_0$ and sufficiently small $n \in \Z$, $|n \pi i / 2 X_0|$ is small, so we can find $\lambda(X_0, n)$ such that $\nu(\lambda(X_0, n)) = n \pi i / 2 X_0$, and so $\det K(\lambda(X_0, n)) = 0$.\\

To better understand $\lambda(r)$, we expand $\lambda(r)$ in a Taylor series about $r = 0$. For $r = n \pi i / 2 X_0 $, we have

\begin{align*}
\lambda(X_0,n)
&= \lambda(0) + \lambda'(0) \frac{n \pi i }{2 X_0} + \mathcal{O}(|n \pi i / 2 X_0|^2) \\
&= \frac{1}{\nu'(0)} \frac{n \pi i }{2 X_0} + \mathcal{O}(n/X_0)^2 \\
&= -c_0 \frac{n \pi i }{2 X_0} + \mathcal{O}(n/X_0)^2 \\
\end{align*} 

It is clear that $\det K(0) = 0$, so there will be a problem at $\lambda = 0$. Using our scaling for the interaction eigenvalues, we do not have to be concerned with the other values of $\lambda$ for which $K(\lambda)$ is singular. To see this, we want to invert $K(\lambda)$ near $\tilde{\lambda} e^{-\alpha X_0}$, which, for sufficiently large $X_0$, is significantly smaller than $\lambda(X_0,1) = \mathcal{O}(1/X_0)$. If we like, we can, say, choose $X_0$ sufficiently large so that $e^{-\alpha X_0 /2} < 1/X_0$.

\item Next, we obtain an expression and a bound for $K(\lambda)^{-1}$ for $\lambda$ near $\tilde{\lambda} e^{-\alpha X_0}$. To do this, we expand the determinant of $K(\lambda)$ in a Taylor series at $\lambda = 0$. Using the Taylor series for $\sinh x$ and our scaling, we get

\begin{align*}
\det K(\lambda) &= -4 \nu(\lambda) X_0 + \mathcal{O}(|\nu(\lambda)|^3) \\
&= -4 X_0 [ (-1/c_0)\tilde{\lambda}e^{-\alpha X_0} + \mathcal{O}(e^{-2 \alpha X_0}) ] + \mathcal{O}(e^{-3 \alpha X_0}) \\
&= \frac{4}{c_0}\tilde{\lambda}e^{-\alpha X_0}X_0 + \mathcal{O}(e^{-2 \alpha X_0}X_0) 
\end{align*}

Thus for $K(\lambda)^{-1}$ (near $\lambda = \tilde{\lambda} e^{-\alpha X_0}$) we have

\begin{equation}
K(\lambda)^{-1} = 
\frac{c_0}{4 \tilde{\lambda}e^{-\alpha X_0}X_0 + \mathcal{O}(e^{-2 \alpha X_0}X_0)}
\begin{pmatrix}
e^{-\nu(\lambda)X_0} & e^{\nu(\lambda)X_0} \\
e^{\nu(\lambda)X_0} & e^{-\nu(\lambda)X_0}
\end{pmatrix}
\end{equation}

where the matrix on the RHS is $\mathcal{O}(1)$ by the same argument we used above to show that $||K(\lambda)|| = \mathcal{O}(1)$. For a bound, we have

\[
||K(\lambda)^{-1}|| \leq C \frac{ e^{\alpha X_0} }{X_0}
\]

\item To find the interaction eigenvalues, we can now solve the first block matrix equation for $c$. Since we are using our scaling, as long as $\lambda \neq 0$, we can invert $K(\lambda)$. The inverse $K(\lambda)^{-1}$ will blow up in norm as $\lambda$ approaches 0, but we hope our bounds on the other terms will take care of that for us. Solving for $c$, we have

\begin{align*}
c = -K(\lambda)^{-1} D_2 d \\
\end{align*}

Plugging this into the second block matrix equation, we get

\begin{align*}
(C_3 K(\lambda) + K(\lambda) \tilde{C}_3) c + (A - \lambda^2 MI + D_3)d &= 0 \\
-(C_3 K(\lambda) + K(\lambda) \tilde{C}_3)K(\lambda)^{-1} D_2 d + (A - \lambda^2 MI + D_3)d &= 0 \\
(A - \lambda^2 MI )d + (D_3 - C_3 D_2 - K(\lambda) \tilde{C}_3 K(\lambda)^{-1} D_2) d &= 0 \\
\end{align*}

We need to check that remainder term on the LHS is really higher order. Using our scaling, $D_3, C_3 D_2 = \mathcal{O}(e^{-3 \alpha X_0})$, which will be good. That leaves the last term. Again, using our scaling and the bound on $||K(\lambda)||^{-1}$ we found above, this becomes

\begin{align*}
|K(\lambda) \tilde{C}_3 K(\lambda)^{-1} D_2| &\leq
C e^{-\alpha X_0} \frac{e^{\alpha X_0}}{X_0} e^{-2 \alpha X_0} \\
&= \frac{e^{-2 \alpha X_0}}{X_0}
\end{align*}

Putting all of this together, we have 

\[
(A - \lambda^2 MI )d + \mathcal{O}\left( e^{-3 \alpha X_0} + \frac{e^{-2 \alpha X_0}}{X_0} \right) d = 0
\]

Now we use our scaling again to write this using $\tilde{\lambda}$ and $\tilde{a}$.

\[
(e^{-2\alpha X_0} \tilde{a} \tilde{A} - e^{-2 \alpha X_0} \tilde{\lambda}^2 M I )d + \mathcal{O}\left( e^{-3 \alpha X_0} + \frac{e^{-2 \alpha X_0}}{X_0} \right) d = 0
\]

This simplifies to

\begin{equation}\label{tildeaEq1}
(\tilde{a} \tilde{A} - \tilde{\lambda}^2 M I )d + \mathcal{O}\left( e^{-\alpha X_0} + \frac{1}{X_0} \right) d = 0
\end{equation}

where because of our scaling, $\tilde{a} \tilde{A} - \tilde{\lambda}^2 M I  = \mathcal{O}(1)$. The equation $(\tilde{a} \tilde{A} - \tilde{\lambda}^2 M I )d = 0$ has a nontrivial solution if and only if its determinant is 0, which is true if and only if $\tilde{\lambda} = 0$ (which we already know) or $\tilde{\lambda} = \pm \sqrt{-2 \tilde{a}/M}$ (which does not depend on $X_0$). Let

\[
\tilde{\mu}_0 = \sqrt{-2 \tilde{a}/M}
\]

$\tilde{\mu}_0$ is either real or pure imaginary, depending on the signs of $\tilde{a}$ and $M$. We write the matrix equation \label{tildeaEq1} as

\[
G(\tilde{\lambda}, r_0)d = 0
\]

where $r_0 = 1/X_0$. Since $\det G(\pm \tilde{\mu}_0, r_0) = 0$, and $G(\tilde{\lambda}, r_0)$ is smooth in $\tilde{\lambda}$ (it is in fact a polynomial in $\tilde{\lambda}$), for sufficiently small $r$ (i.e. sufficiently large $X_0$) we can use the IFT to find unique $\tilde{\lambda}^\pm(X_0)$ near $\pm \tilde{\mu}_0$ such that $\det( \tilde{\lambda}^\pm(X_0), X_0) = 0$. \\

Finally, we undo our scaling. Let

\[
\lambda^\pm(X_0) = e^{-\alpha X_0} \tilde{\lambda}^\pm(X_0)
\]

These are the interaction eigenvalues we seek. By Hamiltonian symmetry of the underlying system (we will have already mentioned this), eigenvalues must come in quartets, i.e. if $\alpha + \beta i$ is an eigenvalue, so are $\pm \alpha \pm \beta i$. Since there are only two eigenvalues of this magnitude, we conclude that we have a pair of interaction eigenvalues at $\pm \lambda(X_0)$. This pair must be real or purely imaginary, and $\lambda(X_0)$ is close to $\pm e^{-\alpha X_0} \tilde{\mu}_0 = \sqrt{-2a/M}$.

\item Now let's look at the eigenvalues which arise for those $\lambda$ near where $K(\lambda)$ is singular. Because we chose the two distances to be equal, the only value of $\lambda$ for which both $A - \lambda^2 M I$ and $K(\lambda)$ are singular is $\lambda = 0$.\\

Choose $\lambda$ for which $A - \lambda^2 M I$ is nonsingular, i.e. $\lambda \neq 0$ and $\lambda \neq \pm \lambda(X_0)$. Then for our chosen $\lambda$, we can invert $A - \lambda^2 M I$. Since $\det(A - \lambda^2 M I) = \lambda^2 M(2 a + \lambda^2 M)$, the inverse of $A - \lambda^2 M I$ is

\begin{align*}
(A - \lambda^2 M I)^{-1} &=
\frac{1}{\lambda^2 M(2a + \lambda^2 M)}
\begin{pmatrix}
-a - \lambda^2 M & -a \\
-a & -a - \lambda^2 M
\end{pmatrix} \\
\end{align*}

To go further, we need to bound this. Since we expect to find eigenvalues near $\lambda(X_0, n) = -c_0 \frac{n \pi i }{2 X_0} + \mathcal{O}(n/X_0)^2$, we take the scaling $\lambda = \tilde{\lambda}(n/X_0)$. This gives us the bound

\[
||(A - \lambda^2 M I)^{-1}||\leq C \left( \frac{n}{X_0} \right)^{-2}
\]

\item We then use this to solve for $d$ in the second block matrix equation.

\begin{align*}
(C_3 K(\lambda) + K(\lambda) \tilde{C}_3) c + (A - \lambda^2 MI + D_3)d &= 0 \\
(C_3 K(\lambda) + K(\lambda) \tilde{C}_3) c + (A - \lambda^2 MI)(I + (A - \lambda^2 MI)^{-1} D_3)d &= 0
\end{align*}

With our scaling, $D_3 = \mathcal{O}(n/X_0)^3$, thus $(A - \lambda^2 MI)^{-1} D_3 = \mathcal{O}(n/X_0)$. Thus for sufficiently large $X_0$ and sufficiently small $n$, $I + (A - \lambda^2 MI)^{-1} D_3$ is invertible. We can now solve for $d$.

\begin{align*}
 d &= 
 -(I + (A - \lambda^2 MI)^{-1} D_3)^{-1}(A - \lambda^2 MI)^{-1}(C_3 K(\lambda) + K(\lambda) \tilde{C}_3) c \\
 &= -C_4 (C_3 K(\lambda) + K(\lambda) \tilde{C}_3) c
\end{align*}

where $C_4 = (I + (A - \lambda^2 MI)^{-1} D_3)^{-1}(A - \lambda^2 MI)^{-1} = \mathcal{O}(n/X_0)^{-2}$.\\

Finally, we substitute this for $d$ into the first line of the block matrix equation to get

\begin{align*}
K(\lambda)c + D_2 d &= 0 \\
K(\lambda)c - D_2 C_4 (C_3 K(\lambda) + K(\lambda) \tilde{C}_3) c &= 0 \\
(I - D_2 C_4 C_3 ) K(\lambda)c - K(\lambda) \tilde{C}_3) c &= 0
\end{align*}

Since $D_2 C_4 C_3 = \mathcal{O}(n/X_0)$, for sufficiently large $X_0$ and sufficiently small $n$, $I + D_2 C_4 C_3$ is invertible, and this reduces to

\[
K(\lambda)c + C_5 K(\lambda) \tilde{C}_3 c = 0
\]

where $C_5 = (I + D_2 C_4 C_3)^{-1} = \mathcal{O}(1)$ and $\tilde{C}_3 = \mathcal{O}(n/X_0)$.

\item Finally, we can solve $K(\lambda)c + C_5 K(\lambda) \tilde{C}_3 c = 0$. We expect this will be possible for $\lambda$ close to $\lambda(X_0, n) = -c_0 \frac{n \pi i }{2 X_0} + \mathcal{O}(n/X_0)^2$. Recall that we showed above that $K(\lambda) = \mathcal{O}(1)$. Thus we can write our equation as

\[
(K(\lambda)c + C_6 ) c = 0
\]

where $C_6 = \mathcal{O}(n/X_0)$. Let

\[
G(\lambda, r) = \det( K(\lambda) + C_6 )
\]

where $r = \mathcal{O}(n/X_0)$ is small. Then since $G( \lambda(X_0, n), 0) = 0$, it follows from the IFT that for sufficiently small $r$, i.e. sufficiently small $n/X_0$, we can find $\lambda^c(X_0, n)$ close to $\lambda(X_0, n)$ such that $\det ( K(\lambda^c(X_0, n)) + C_6 ) = 0$. By Hamiltonian symmetry, we conclude that $\lambda^c(X_0, n)$ is pure imaginary, and $\lambda^c(X_0, -n) = \lambda^c(X_0, n)$.

\item The only thing we have not dealt with is what happens at $\lambda = 0$. We will do this in the following way. We have a pair of interaction eigenvalue which are $\mathcal{O}(e^{-\alpha X_0})$ and the smallest nonzero ``essential spectrum'' eigenvalues are at $i C/X_0$, so have magnitude of $C/X_0$. Choose a radius $\delta > 0$ between these so that $\delta = C_0 e^{-\alpha X_0}$, where $C_0$ is sufficiently large so that

\[
\delta - |\sqrt{-2a/M}| \geq e^{-\alpha X_0}
\]

This implies that for $|\lambda| = \delta$,

\begin{align*}
|2a + \lambda^2 M| &= |M||\lambda^2 - (-2a/M)| \\
&= |M|\:|\lambda + \sqrt{-2a/M}||\lambda - \sqrt{-2a/M}| \\
&\geq C e^{-2 \alpha X_0}
\end{align*}

Let

\begin{equation}
E(\lambda) = \det
\begin{pmatrix}
K(\lambda) & D_2 \\
C_3 K(\lambda) + K(\lambda) \tilde{C}_3 & A - \lambda^2 MI + D_3
\end{pmatrix}
\end{equation}

Since $K(\lambda)$ is invertible for our choice of $\delta$, we can use a standard determinant identity (that is not hard to show) to get

\begin{align*}
E(\lambda) &= \det K(\lambda) \det(A - \lambda^2 MI + D_3 - (C_3 K(\lambda) + K(\lambda) \tilde{C}_3) K(\lambda)^{-1} D_2 ) \\
&= \det K(\lambda) \det(A - \lambda^2 MI + D_3 - (C_3 + K(\lambda) \tilde{C}_3 K(\lambda)^{-1} ) D_2 ) \\
&= \det K(\lambda) \det(A - \lambda^2 MI + R(\lambda) )
\end{align*}

where

\[
R(\lambda) = D_3 - (C_3 + K(\lambda) \tilde{C}_3 K(\lambda)^{-1} ) D_2
\]

Since $A - \lambda^2 MI$ is also invertible for our choice of $\delta$,

\begin{align*}
E(\lambda) &= \det K(\lambda) \det((A - \lambda^2 MI)(I + (A - \lambda^2 MI)^{-1} R(\lambda) ))\\
&= \det K(\lambda) \det(A - \lambda^2 MI) \det (I + (A - \lambda^2 MI)^{-1} R(\lambda) )
\end{align*}

To go any further, we need to bound the remainder term. Since we are taking $\lambda = \mathcal{O}(e^{-\alpha X_0})$, we can use the bounds from above to get

\begin{align*}
|R(\lambda)| \leq C e^{-2 \alpha X_0}\left( \frac{1}{X_0} + e^{-\alpha X_0} \right)
\end{align*}

To bound $(A - \lambda^2 MI)^{-1}$, recall that we have

\begin{align*}
(A - \lambda^2 M I)^{-1} &=
\frac{1}{\lambda^2 M(2a + \lambda^2 M)}
\begin{pmatrix}
-a - \lambda^2 M & -a \\
-a & -a - \lambda^2 M
\end{pmatrix} \\
\end{align*}

From how we defined $\delta$, we have

\[
||(A - \lambda^2 M I)^{-1}|| \leq C e^{2 \alpha X_0}
\]

Thus we conclude

\[
||(A - \lambda^2 MI)^{-1} R(\lambda)|| \leq C \left( \frac{1}{X_0} + e^{-\alpha X_0} \right)
\]

For sufficiently large $X_0$, this is much less than 1, from the expression $\det(I + \epsilon A) = 1 + \epsilon \text{Tr }A + \mathcal{O}(\epsilon^2)$, we have

\[
\det (I + (A - \lambda^2 MI)^{-1} R(\lambda) ) 
= 1 + \mathcal{O}\left( \frac{1}{X_0} + e^{-\alpha X_0} \right)
\]

Thus we have for $|\lambda| = \delta$,

\begin{align*}
E(\lambda) &= \det K(\lambda) \det(A - \lambda^2 MI) \left( 1 + \mathcal{O}\left( \frac{1}{X_0} + e^{-\alpha X_0} \right) \right)
\end{align*}

We are interested in the location of the zeros of $E(\lambda)$ inside $B(0, \delta) \subset \C$. From Rouche's Theorem, since the remainder portion of this is less than the rest of it on the circle of radius $\delta$ in the complex plane, $E(\lambda)$ and $\det K(\lambda) \det(A - \lambda^2 MI)$ have the same number of zeros (counting multiplicity) inside $B(0, \delta)$. In this special case, $\det K(\lambda)$ has one zero inside $B(0, \delta)$ and $\det(A - \lambda^2 MI)$ has 4 (two at 0 and two near the interaction eigenvalues), thus $E(\lambda)$ has 5 zeros inside $B(0, \delta)$. We know that these zeros include the two interaction eigenvalues and a kernel eigenvalue with algebraic multiplicity 2 (we know what their corresponding eigenfunctions are), which leaves us with exactly one more zero of $E(\lambda)$ inside $B(0, \delta)$. By the Hamiltonian symmetry, any eigenvalues must come in quartets. Thus the remaining zero of $E(\lambda)$ inside $B(0, \delta)$ corresponds to a lone eigenvalue, which must be at 0 and which corresponds to when $K(0)$ is singular.

\item Let us summarize the results that we have obtained. For the 2-periodic solution with lengths $X_0 = X_1$,

\begin{itemize}
	\item For $X_0$ sufficiently large, there is a pair of interaction eigenvalues at $\lambda = \pm \lambda(X_0)$, where $\lambda(X_0) = \mathcal{O}(e^{-\alpha X_0})$ is close to $\pm \sqrt{-2a/M}$ and is either real or purely imaginary.
	\item For $n/X_0$ sufficiently small, there is a pair of purely imaginary ``essential spectrum'' eigenvalues located at $\lambda = \pm \lambda^c(X_0, n)$, where $\lambda^c(X_0, n)$ is close to $-c_0 \frac{n \pi i }{2 X_0}$.
	\item There is an eigenvalue at 0 (from translation invariance) with algebraic multiplicity 2 (and for which the eigenfunction and generalized eigenfunction are known). There is an additional eigenvalue at 0 which comes from the fact that $K(0)$ is singular.
	\item Within a ball of radius $\delta = C_0 e^{-\alpha X_0)}$ which is sufficiently large to enclose all of these, there are no other eigenvalues.
\end{itemize}

\end{enumerate}

\subsubsection{2-pulse with Unequal Distances (with one restriction)}

The next easiest case is the 2-pulse $X_0 < X_1$. (It doesn't matter which way we do it, but by the convention from the existence problem, $X_1$ is the ``periodic distance''). Much of this is similar to the case with equal distances. We will also need to make a small restriction on the distances, which we will get to when we need it.

\begin{enumerate}

\item First, we write down the matrices $A$ and $K(\lambda)$. For the matrix $A$ we have

\[
A = a \begin{pmatrix}
-1 & 1 \\
1 & -1
\end{pmatrix}
\]

where $a = a_0 + a_1 = \langle \Psi(X_0), Q'(-X_0) \rangle + \langle \Psi(X_1), Q'(-X_1) \rangle$. For the matrix $K(\lambda)$ we have

\begin{equation}
K(\lambda) = 
\begin{pmatrix}
e^{-\nu(\lambda)X_1} & -e^{\nu(\lambda)X_0} \\
-e^{\nu(\lambda)X_1} & e^{-\nu(\lambda)X_0}
\end{pmatrix}
\end{equation}

\item We will look for the interaction eigenvalues first. We expect these to occur when $\det(A - \lambda^2 M I) = 0$. Since $e^{-\alpha X_1}$ is small compared to $e^{-\alpha X_0}$, we take the scaling

\begin{align*}
\lambda &= e^{-\alpha X_0} \tilde{\lambda} \\
a &= e^{-2 \alpha X_0} \tilde{a}
\end{align*}

Then we can write the matrix $A$ as 

\[
A = e^{-2 \alpha X_0} \tilde{a} \tilde{A}
\]

where
\[
\tilde{A} = \begin{pmatrix}
-1 & 1 \\
1 & -1
\end{pmatrix}
\]

So far, this is the same as the previous case.

\item Next, we expand $\nu(\lambda)$ in a Taylor series about $\lambda = 0$. This is exactly the same as in the previous section.

\begin{align*}
\nu(\lambda) &= -\frac{1}{c_0}\tilde{\lambda}e^{-\alpha X_0} + \mathcal{O}(e^{-2 \alpha X_0})
\end{align*}

\item We can use this to get a bound on $K(\lambda)$ itself.

\begin{align*}
||K(\lambda)|| &= \mathcal{O}( e^{|\nu(\lambda)|X_0} + e^{|\nu(\lambda)|X_1} ) \\
 &= \mathcal{O} (1 + |\nu(\lambda)X_0| + |\nu(\lambda)X_1 |+ (|\nu(\lambda)X_0|^2 + (|\nu(\lambda)X_1|^2)) \\
&= \mathcal{O}( 1 + |1/c_0| |\tilde{\lambda} | e^{-\alpha X_0} X_0 + |1/c_0| |\tilde{\lambda} | e^{-\alpha X_0} X_1) \\ 
&= \mathcal{O}( 1 + e^{-\alpha X_0}(X_0 + X_1) ) \\
&= \mathcal{O}\left( 1 + e^{-\alpha X_0}X_0\left( 1 + \frac{X_1}{X_0} \right) \right)  
\end{align*}

For now, we take the following hypothesis regarding the lengths $X_0$ and $X_1$.

\begin{hypothesis}\label{X0X1} $X_0$ is sufficiently large so that
\[
e^{-\alpha X_0}X_0\left( 1 + \frac{X_1}{X_0} \right) = \mathcal{O}(1)
\]
\end{hypothesis}

With this hypothesis, $||K(\lambda)|| = \mathcal{O}(1)$.

\item We would like to invert $K(\lambda)$, when possible. The determinant of $K(\lambda)$ is given by

\[
\det K(\lambda) = e^{-\nu(\lambda)(X_0+X_1)} - e^{\nu(\lambda)(X_0+X_1)} = -2 \sinh(\nu(\lambda)(X_0+X_1)) = -2 \sinh(\nu(\lambda)X)
\]

This is 0 if and only if $\nu(\lambda)(X_0+X_1) = n \pi i$, i.e. $\nu(\lambda) = n \pi i / X$ for $n \in \Z$, where $X = X_0 + X_1$ is half the length of the domain. This includes $\lambda = 0$.\\

Inverting this is more complicated than in the previous case since. There will potentially be many places on the imaginary axis where $K(\lambda)$ is singular, and these will ``pile up'' as $X_1$ gets larger. We need to avoid all of them. \\

The idea is to put small open balls around each point where $K(\lambda)$ is singular, and then we will have a bound on $K(\lambda)^{-1}$ which only holds outside these open balls. The hope is that we can do this, i.e. we can make the balls large enough to get a good bound, but small enough that they do not overlap.\\

To do this, first we need to know the values of $\lambda$ (not $\nu(\lambda))$ where $K(\lambda)$ is singular. These values will only depend on $X = X_0 + X_1$. Adapting what we did above, for sufficiently large $X$ and sufficiently small $n$, $K(\lambda)$ is singular for $\lambda = \lambda(X, n)$, which is given by

\begin{align*}
\lambda(X, n)
&= -c_0 \frac{n \pi i }{X} + \mathcal{O}\left( \frac{n}{X}\right)^2 \\
\end{align*}

Note that $\nu(\lambda(X, n)) = n \pi i/X$. In addition, near the origin, the points $\lambda(X, n)$ are, to leading order, equally spaced on the imaginary axis.

\item Now we Taylor expand $K(\lambda)$ about each $\lambda(X, n)$ which is within $\mathcal{O}(e^{-\alpha X_0})$ of the origin. (We only have to worry about stuff near the interaction eigenvalues we are looking for.)

\begin{align*}
\det K( \lambda(X, n) + \epsilon )
&= -2 \sinh \Big( \nu(\lambda(X, n) + \epsilon )X \Big) \\
\end{align*}

To keep going, we need to substitute the Taylor series for $\nu(\lambda)$. This time, we use the Taylor expansion about $\lambda(X, n)$, since we know that $\nu( \lambda(X, n) ) = n \pi i / X$.

\begin{align*}
\nu(\lambda(X, n) + \epsilon ) &= 
\frac{n \pi i }{X} + \mathcal{O}(\epsilon)
\end{align*}

Plugging this in above, we get

\begin{align*}
\det K( \lambda(X, n) + \epsilon )
&= -2 \sinh \Big( \nu(\lambda(X, n) + \epsilon )X \Big) \\
&= -2 \sinh \left( n \pi i + \mathcal{O}(\epsilon X) \right) \\
&= \mathcal{O}(\epsilon X )
\end{align*}

Note that, to leading order, this does not depend on $n$. As long we we are further than $\epsilon$ in distance from any of the $\lambda(X_0, n)$, the bound holds.\\

For $K(\lambda)^{-1}$, we have the expression

\begin{equation}
K(\lambda)^{-1} = \frac{1}{\det K(\lambda) }
\begin{pmatrix}
e^{-\nu(\lambda)X_0} & e^{\nu(\lambda)X_0} \\
e^{\nu(\lambda)X_1} & e^{-\nu(\lambda)X_1}
\end{pmatrix}
\end{equation}

Using the determinant bound and Hypothesis \ref{X0X1} (in which we took $e^{-\alpha X_0}X_0\left( 1 + \frac{X_1}{X_0} \right) = \mathcal{O}(1)$), we have a bound on $K( \lambda )^{-1}$.

\begin{align*}
|| K( \lambda )^{-1}|| 
&\leq C \left( \frac{1}{\epsilon X} \right)
\end{align*}

which holds as long as we are outside an $\epsilon-$ball around the points $\lambda(X,n)$ where $K(\lambda)$ is singular.\\

All that remains now is to choose an appropriate $\epsilon$. Recall that the spacing between the points $\lambda(X, n)$ is order $\mathcal{O}(1/X)$, so we need $\epsilon$ to be some fraction of that. Since $X_0$ is large, $e^{-\alpha X_0}$ is small (way less than 1), so choose

\[
\epsilon = \frac{e^{-\alpha X_0/2}}{X}
\]

Substituting this in, we attain the bound

\begin{align*}
|| K( \lambda(X_1, n))^{-1}|| 
&\leq C e^{\alpha X_0/2}
\end{align*}

which holds outside the chosen $\epsilon-$balls.

\item As long as $\lambda$ is outside of the $\epsilon-$balls, we can solve for $c$ in terms of $d$ using the first line of the block matrix equation. Solving for $c$, we have

\begin{align*}
c = -K(\lambda)^{-1} D_2 d \\
\end{align*}

Plugging this into the second block matrix equation, we get

\begin{align*}
(C_3 K(\lambda) + K(\lambda) \tilde{C}_3) c + (A - \lambda^2 MI + D_3)d &= 0 \\
-(C_3 K(\lambda) + K(\lambda) \tilde{C}_3)K(\lambda)^{-1} D_2 d + (A - \lambda^2 MI + D_3)d &= 0 \\
(A - \lambda^2 MI )d + (D_3 - C_3 D_2 - K(\lambda) \tilde{C}_3 K(\lambda)^{-1} D_2) d &= 0 \\
\end{align*}

We need to check that remainder term on the LHS is really higher order. Using our scaling, $D_3, C_3 D_2 = \mathcal{O}(e^{-3 \alpha X_0})$, which will be good. That leaves the last term. For that, we have

\begin{align*}
|K(\lambda) \tilde{C}_3 K(\lambda)^{-1} D_2| &\leq
C e^{-\alpha X_0} e^{\alpha X_0/2} e^{-2 \alpha X_0} \\
&= e^{-(5/2) \alpha X_0}
\end{align*}

Putting all of this together, we have 

\[
(A - \lambda^2 MI )d + \mathcal{O}\left( e^{-(5/2) \alpha X_0} \right) d = 0
\]

Recall that in the equal length case, we had a term which looked like $1/X_0$ and then had to take $X_0$ sufficiently large. We already took $X_0$ large enough above, so that should be taken care of.

\item The rest of this works out as in the previous case where both lengths were equal. We use our scaling again to write this using $\tilde{\lambda}$ and $\tilde{a}$. After simplifying, this becomes

\[
(\tilde{a}\tilde{A} - \tilde{\lambda}^2 MI )d + \mathcal{O}\left( e^{-\alpha X_0/2} \right) d = 0
\]

Because of our scaling, $\tilde{a} \tilde{A} - \tilde{\lambda}^2 M I  = \mathcal{O}(1)$. The equation $(\tilde{a} \tilde{A} - \tilde{\lambda}^2 M I )d = 0$ has a nontrivial solution if and only if its determinant is 0, which is true if and only if $\tilde{\lambda} = 0$ (which we already know, since we know what these eigenfunctions are) or $\tilde{\lambda} = \pm \sqrt{-2 \tilde{a}/M}$ (which does not depend on $X_0$). Let

\[
\tilde{\mu}_0 = \sqrt{-2 \tilde{a}/M}
\]

$\tilde{\mu}_0$ is either real or pure imaginary, depending on the signs of $\tilde{a}$ and $M$. We write this matrix equation as

\[
G(\tilde{\lambda}, r_0)d = 0
\]

where $r_0 = \mathcal{O}(e^{-\alpha X_0/2})$. Since $\det G(\pm \tilde{\mu}_0, r_0) = 0$, and $G(\tilde{\lambda}, r)$ is smooth in $\tilde{\lambda}$ (it is in fact a polynomial in $\tilde{\lambda}$), for sufficiently small $r_0$ (i.e. sufficiently large $X_0$) we can use the IFT to find unique $\tilde{\lambda}^\pm(X_0)$ near $\pm \tilde{\mu}_0$ such that $\det( \tilde{\lambda}^\pm(X_0), X_0) = 0$. \\

Finally, we undo our scaling. Let

\[
\lambda^\pm(X_0) = e^{-\alpha X_0} \tilde{\lambda}^\pm(X_0)
\]

These are the interaction eigenvalues we seek. By Hamiltonian symmetry of the underlying system (we will have already mentioned this somewhere), eigenvalues must come in quartets, i.e. if $\alpha + \beta i$ is an eigenvalue, so are $\pm \alpha \pm \beta i$. Since there are only two eigenvalues of this magnitude, we conclude that we have a pair of interaction eigenvalues at $\pm \lambda(X_0)$. This pair must be real or purely imaginary, and $\lambda(X_0)$ is close to $e^{-\alpha X_0} \tilde{\mu}_0 = \sqrt{-2a/M}$.

\item From here, we do the same thing for the points where $K(\lambda)$ is singular. This is basically the same idea, except here we need to make an $\epsilon-$ball around 0 and the interaction eigenvalues so we can invert $A - \lambda^2 M I$. For $\lambda \neq 0, \pm \sqrt{-2a/M}$, we have

\begin{align*}
(A - \lambda^2 M I)^{-1} &=
\frac{1}{\lambda^2 M(2a + \lambda^2 M)}
\begin{pmatrix}
-a - \lambda^2 M & -a \\
-a & -a - \lambda^2 M
\end{pmatrix} \\
\end{align*}

Outside these $\epsilon-$balls, we have $|\lambda|^2 \geq \epsilon^2$ (for $\lambda$ near 0) and for $\lambda$ near the interaction eigenvalues,

\begin{align*}
|2a + \lambda^2 M| &= |M||\lambda^2 - (-2a/M)| \\
&= |M||\lambda - \sqrt{-2a/M}||\lambda + \sqrt{-2a/M}| \\
&\geq |M| \epsilon^2
\end{align*}

Thus, outside the $\epsilon-$balls, we have

\[
||(A - \lambda^2 M I)^{-1}|| \leq C \frac{1}{\epsilon^2} 
\]

We choose a similar $\epsilon$ as we did before, i.e. 

\[
\epsilon = \frac{e^{-\alpha X_0/4}}{X}
\]

(the only difference is the fraction 1/4 instead of 1/2, which we want here since we are squaring $\epsilon$), which gives us


\[
||(A - \lambda^2 M I)^{-1}|| \leq C e^{\alpha X_0/2} X^2
\]


We expect to find eigenvalues near $\lambda(X, n) = -c_0 \frac{n \pi i }{X} + \mathcal{O}(n/X)^2$. If we are outside of the $\epsilon$-balls, we can solve for $d$ in the second block matrix equation. As above, we have

\begin{align*}
(C_3 K(\lambda) + K(\lambda) \tilde{C}_3) c + (A - \lambda^2 MI)(I + (A - \lambda^2 MI)^{-1} D_3)d &= 0
\end{align*}

Using our bound for $||(A - \lambda^2 M I)^{-1}||$, we have

\begin{align*}
||(A - \lambda^2 MI)^{-1} D_3)|| &= \mathcal{O}(e^{\alpha X_0/2} X^2(e^{-3 \alpha X_0} + |\lambda|^3)) \\
&= \mathcal{O}(e^{-\alpha X_0/2} e^{-2 \alpha X_0} X^2 + e^{\alpha X_0/2} X^2 |\lambda|^3)
\end{align*}

We want to find eigenvalues within a ball of radius $C_0 e^{-\alpha X_0}$ of the origin, so, with that in mind, this bound becomes

\begin{align*}
||(A - \lambda^2 MI)^{-1} D_3)|| 
&= \mathcal{O}(e^{-\alpha X_0/2} e^{-2 \alpha X_0} X^2 )
\end{align*}

Unfortunately, this could blow up. But we have Hypothesis \ref{X0X1} to fall back on, which tells us that

\begin{align*}
e^{-\alpha X_0} X = \mathcal{O}(1)
\end{align*}

Thus we have

\begin{align*}
||(A - \lambda^2 MI)^{-1} D_3)|| 
&= \mathcal{O}(e^{-\alpha X_0/2})
\end{align*}

For sufficiently large $X_0$, $(I + (A - \lambda^2 MI)^{-1} D_3)$ is invertible, so we can solve for $D$ to get

\begin{align*}
 d &= 
 -(I + (A - \lambda^2 MI)^{-1} D_3)^{-1}(A - \lambda^2 MI)^{-1}(C_3 K(\lambda) + K(\lambda) \tilde{C}_3) c \\
 &= -C_4 (C_3 K(\lambda) + K(\lambda) \tilde{C}_3) c
\end{align*}

where $C_4 = (I + (A - \lambda^2 MI)^{-1} D_3)^{-1}(A - \lambda^2 MI)^{-1} = \mathcal{O}(e^{\alpha X_0/2} X^2)$.\\

Finally, we substitute this for $d$ into the first line of the block matrix equation to get (as above)

\begin{align*}
(I - D_2 C_4 C_3 ) K(\lambda)c - K(\lambda) \tilde{C}_3 c &= 0
\end{align*}

We would like to invert $I - D_2 C_4 C_3$. For a bound on $D_2 C_4 C_3$, we have

\begin{align*}
D_2 C_4 C_3 &= \mathcal{O}\Big( (e^{-2 \alpha X_0} + |\lambda|^2)( e^{\alpha X_0/2} X^2)(e^{-\alpha X_0} + |\lambda|) \Big) \\
&= \mathcal{O}\Big( e^{-\alpha X_0/2} e^{- 2 \alpha X_0} X^2 \Big) \\
&= \mathcal{O}( e^{-\alpha X_0/2})
\end{align*}

where we used Hypothesis \ref{X0X1} again. Thus, for sufficiently large $X_0$, we can invert $I - D_2 C_4 C_3$ to get

\[
K(\lambda)c + C_5 K(\lambda) \tilde{C}_3 c = 0
\]

where $C_5 = (I + D_2 C_4 C_3)^{-1} = \mathcal{O}(1)$ and $\tilde{C}_3 = \mathcal{O}(e^{-\alpha X_0})$.

\item Finally, we can solve $K(\lambda)c + C_5 K(\lambda) \tilde{C}_3 c = 0$. We expect this will be possible for $\lambda$ close to $\lambda(X, n) = -c_0 \frac{n \pi i }{X} + \mathcal{O}(n/X)^2$. Using Hypothesis \ref{X0X1}, we have $K(\lambda) = \mathcal{O}(1)$. Thus we can write our equation as

\[
(K(\lambda)c + C_6 ) c = 0
\]

where $C_6 = \mathcal{O}(e^{-\alpha X_0})$. Let

\[
G(\lambda, r) = \det( K(\lambda) + C_6 )
\]

where $r = \mathcal{O}(e^{-\alpha X_0})$ is small. Choose $n$ such that $|\lambda(X,n)| \leq C_0 e^{-\alpha X_0}$. Then since $G( \lambda(X, n), 0) = 0$, it follows from the IFT that for sufficiently small $r$, i.e. sufficiently large $X_0$, we can find $\lambda^c(X, n)$ close to $\lambda(X, n)$ such that $\det ( K(\lambda^c(X, n)) + C_6 ) = 0$. By Hamiltonian symmetry, we conclude that $\lambda^c(X, n)$ is pure imaginary, and $\lambda^c(X, -n) = \lambda^c(X, n)$.

\item Finally, we deal with what happens at $\lambda = 0$. This will be similar to what we did above. We have a pair of interaction eigenvalue which are $\mathcal{O}(e^{-\alpha X_0})$ and ``essential spectrum'' eigenvalues at $i C n /X$, but for large $X$, the ``essential spectrum'' eigenvalues may be smaller than the interaction eigenvalues.\\

 Choose a radius $\delta > 0$ so that $\delta = C_0 e^{-\alpha X_0}$, where $C_0$ is sufficiently large so that

\[
\delta - |\sqrt{-2a/M}| \geq e^{-\alpha X_0}
\]

and for all $n$ with $|\lambda^c(X, n)| \leq C_0 e^{-\alpha X_0}$,

\[
|\delta - |\lambda^c(X, n)| | \geq C \frac{1}{3X}
\]

For this second condition, since the $\lambda^c(X, n)$ are, to leading order, equally spaced, this is just saying that we choose $\delta$ so that the $\delta-$circle cuts half way between two of these. Based on what we have already done, this implies the following bounds

\begin{align*}
||(A - \lambda^2 M I)^{-1}|| &\leq C e^{2 \alpha X_0} \\
||K(\lambda)^{-1}|| &\leq C 
\end{align*}

Let

\begin{equation}
E(\lambda) = \det
\begin{pmatrix}
K(\lambda) & D_2 \\
C_3 K(\lambda) + K(\lambda) \tilde{C}_3 & A - \lambda^2 MI + D_3
\end{pmatrix}
\end{equation}

Since $K(\lambda)$ is invertible for our choice of $\delta$, as we did above, we have

\begin{align*}
E(\lambda) &= \det K(\lambda) \det(A - \lambda^2 MI + R(\lambda) )
\end{align*}

where, using Hypothesis \ref{X0X1} to bound $K(\lambda) \tilde{C}_3 K(\lambda)^{-1}$, we have

\begin{align*}
R(\lambda) &= D_3 - (C_3 + K(\lambda) \tilde{C}_3 K(\lambda)^{-1} ) D_2 \\
&= \mathcal{O}(e^{-3 \alpha X_0})
\end{align*}

Since $A - \lambda^2 MI$ is also invertible for our choice of $\delta$, we have

\begin{align*}
E(\lambda) &= \det K(\lambda) \det(A - \lambda^2 MI) \det (I + (A - \lambda^2 MI)^{-1} R(\lambda) )
\end{align*}

For a bound on $(A - \lambda^2 MI)^{-1} R(\lambda)$, we have

\[
||(A - \lambda^2 MI)^{-1} R(\lambda)|| \leq C e^{2 \alpha X_0} e^{-3 \alpha X_0} = C e^{-\alpha X_0}
\]

For sufficiently large $X_0$, this is much less than 1. Thus from the expression $\det(I + \epsilon A) = 1 + \epsilon \text{Tr }A + \mathcal{O}(\epsilon^2)$, we have

\[
\det (I + (A - \lambda^2 MI)^{-1} R(\lambda) ) 
= 1 + \mathcal{O}\left( e^{-\alpha X_0} \right)
\]

We conclude that for $|\lambda| = \delta$,

\begin{align*}
E(\lambda) &= \det K(\lambda) \det(A - \lambda^2 MI) \left( 1 + \mathcal{O}(e^{-\alpha X_0}) \right)
\end{align*}

From Rouche's Theorem, since the remainder portion of this is less than the rest of it on the circle of radius $\delta$ in the complex plane, $E(\lambda)$ and $\det K(\lambda) \det(A - \lambda^2 MI)$ have the same number of zeros (counting multiplicity) inside $B(0, \delta) \subset \C$. We have the following
\begin{enumerate}
\item $\det K(\lambda)$ has $2k_0 + 1$ zeros inside $B(0, \delta)$ for some integer $k_0$. These come from $\lambda = 0$ and $\lambda = \pm \lambda^c(X, k)$ for $k = 1, \dots, k_0$.
\item $\det(A - \lambda^2 MI)$ has 4 zeros inside $B(0, \delta)$ (two at 0 and two near the interaction eigenvalues).
\end{enumerate}

Thus $E(\lambda)$ has $2k_0 + 5$ zeros inside $B(0, \delta)$. We know that these zeros include the two interaction eigenvalues, a kernel eigenvalue with algebraic multiplicity 2; and $k_0$ pairs of interaction eigenvalues at $\pm \lambda^c(X, k)$ for $k = 1, \dots, k_0$. This leaves us with exactly one more zero of $E(\lambda)$ inside $B(0, \delta)$. By the Hamiltonian symmetry, any eigenvalues must come in quartets. Thus the remaining zero of $E(\lambda)$ inside $B(0, \delta)$ corresponds to a lone eigenvalue, which must be at 0. This corresponds to the case when $K(0)$ is singular.

\item Let us summarize the results that we have obtained. For the 2-periodic solution with lengths $X_0 < X_1$, assume that

\[
e^{-\alpha X_0}X_0\left( 1 + \frac{X_1}{X_0} \right) = \mathcal{O}(1)
\]

Recall that $K(\lambda)$ is singular at

\[
\lambda(X, n) = -c_0 \frac{n \pi i}{X} + \mathcal{O}(n/X)^2
\]

and $A - \lambda^M I$ is singular at $\lambda = 0$ and $\pm \mu_0 = \pm \sqrt{-2a/M}$. Let 

\[
\epsilon = \frac{e^{-\alpha X_0/4}}{X}
\]

and assume that $| \pm\mu_0 - \lambda(X, n)| \geq \epsilon$ for all $n$. Then the following is true.

\begin{itemize}
	\item For $X_0$ sufficiently large, there is a pair of interaction eigenvalues at $\lambda = \pm \lambda(X_0)$, where $\lambda(X_0) = \mathcal{O}(e^{-\alpha X_0})$ is close to $\pm \sqrt{-2a/M}$ and is either real or purely imaginary.
	\item For each $n/X_0$ sufficiently small, there is a pair of purely imaginary ``essential spectrum'' eigenvalues located at $\lambda = \pm \lambda^c(X_0, n)$, where $\lambda^c(X_0, n)$ is close to $\lambda(X, n)$.
	\item There is an eigenvalue at 0 (from translation invariance) with algebraic multiplicity 2 (and for which the eigenfunction and generalized eigenfunction are known). There is an additional eigenvalue at 0 which comes from the fact that $K(0)$ is singular.
	\item Within a ball of radius $\delta = C_0 e^{-\alpha X_0)}$ which is sufficiently large to enclose all of these, there are no other eigenvalues.
\end{itemize}


\end{enumerate}

\subsubsection{2-pulse with Unequal Distances}

Now we try to get this to work without Hypothesis \ref{X0X1}. It helps to recall what we are trying to accomplish. To find the interaction eigenvalues, we solve for $c$ near where we expect the interaction eigenvalues to be, then we plug that in to get an equation for $d$.\\

The interaction eigenvalues are near $\pm \sqrt{-a/2M} = \mathcal{O}(e^{-\alpha X_0})$, which is either real or pure imaginary. We consider the two cases separately.

\begin{enumerate}

\item Suppose $\sqrt{-a/2M}$ is purely imaginary, and recall that $\nu(\lambda) = -\frac{1}{c_0}\lambda + \mathcal{O}(|\lambda|^2)$. Choose $\lambda$ such that 


\[
|\text{Re }\lambda| \leq 1/X
\]

Then for these $\lambda$ we have $||K(\lambda)|| = \mathcal{O}(1)$. As long as we choose $\lambda$ so that $K(\lambda)$ is invertible, we have $||K(\lambda)^{-1}|| = \mathcal{O}(1/\det K(\lambda))$. As before, recall that $K(\lambda)$ is singular for $\lambda = \lambda(X, n)$, $n \in \Z$, given by

\begin{align*}
\lambda(X, n)
&= -c_0 \frac{n \pi i }{X} + \mathcal{O}\left( \frac{n}{X}\right)^2 \\
\end{align*}

As long as we are at least $\epsilon$ away from these points, then $\det K(\lambda) = \mathcal{O}(\epsilon X)$. Choosing $\epsilon = e^{-\alpha X_0/2}/X$ and combining what we have so far, we conclude that, outside the $\epsilon$ balls and within $1/X$ of the imaginary axis, we have

\[
||K(\lambda)^{-1}|| = \mathcal{O}(e^{\alpha X_0/2})
\]

We solve for $c$ using the first line of the block matrix equation to get

\begin{align*}
c = -K(\lambda)^{-1} D_2 d
\end{align*}

Plugging this into the second block matrix equation, we get

\begin{align*}
(A - \lambda^2 MI )d + (D_3 - C_3 D_2 - K(\lambda) \tilde{C}_3 K(\lambda)^{-1} D_2) d &= 0 \\
\end{align*}

We need to check the order of the remainder term on the RHS. The terms $D_3, C_3 D_2 = \mathcal{O}(e^{-3 \alpha X_0})$. For the final term, we have

\begin{align*}
|K(\lambda) \tilde{C}_3 K(\lambda)^{-1} D_2| &\leq
C e^{-\alpha X_0} e^{\alpha X_0/2} e^{-2 \alpha X_0} \\
&= e^{-(5/2) \alpha X_0}
\end{align*}

Putting all of this together, we have 

\[
(A - \lambda^2 MI )d + \mathcal{O}\left( e^{-(5/2) \alpha X_0} \right) d = 0
\]

Using our scaling and simplifying, this becomes

\[
(\tilde{a}\tilde{A} - \tilde{\lambda}^2 MI )d + \mathcal{O}\left( e^{-\alpha X_0/2} \right) d = 0
\]

Because of our scaling, $\tilde{a} \tilde{A} - \tilde{\lambda}^2 M I  = \mathcal{O}(1)$. The equation $(\tilde{a} \tilde{A} - \tilde{\lambda}^2 M I )d = 0$ has a nontrivial solution if and only if its determinant is 0, which is true if and only if $\tilde{\lambda} = 0$ (which we already know, since we know what these eigenfunctions are) or $\tilde{\lambda} = \pm \sqrt{-2 \tilde{a}/M}$ (which does not depend on $X_0$). Let

\[
\tilde{\mu}_0 = \sqrt{-2 \tilde{a}/M}
\]

$\tilde{\mu}_0$ is either real or pure imaginary, depending on the signs of $\tilde{a}$ and $M$. We write this matrix equation as

\[
G(\tilde{\lambda}, r_0)d = 0
\]

where $r_0 = \mathcal{O}(e^{-\alpha X_0/2})$. Since $\det G(\pm \tilde{\mu}_0, r_0) = 0$, and $G(\tilde{\lambda}, r)$ is smooth in $\tilde{\lambda}$ (it is in fact a polynomial in $\tilde{\lambda}$), for sufficiently small $r_0$ (i.e. sufficiently large $X_0$) we can use the IFT to find unique $\tilde{\lambda}^\pm(X_0)$ near $\pm \tilde{\mu}_0$ such that $\det( \tilde{\lambda}^\pm(X_0), X_0) = 0$. \\

Finally, we undo our scaling. Let

\[
\lambda^\pm(X_0) = e^{-\alpha X_0} \tilde{\lambda}^\pm(X_0)
\]

These are the interaction eigenvalues we seek. By Hamiltonian symmetry of the underlying system (we will have already mentioned this somewhere), eigenvalues must come in quartets, i.e. if $\alpha + \beta i$ is an eigenvalue, so are $\pm \alpha \pm \beta i$. Since there are only two eigenvalues of this magnitude, we conclude that we have a pair of interaction eigenvalues at $\pm \lambda(X_0)$. This pair must be real or purely imaginary, and $\lambda(X_0)$ is close to $e^{-\alpha X_0} \tilde{\mu}_0 = \sqrt{-2a/M}$.





\end{enumerate}



\end{document}